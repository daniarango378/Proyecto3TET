24/11/24 08:24:00 WARN DependencyUtils: Local jar /home/hadoop/mysql-connector-java-8.0.xx.jar does not exist, skipping.
24/11/24 08:24:01 INFO SparkContext: Running Spark version 3.4.1-amzn-1
24/11/24 08:24:01 INFO ResourceUtils: ==============================================================
24/11/24 08:24:01 INFO ResourceUtils: No custom resources configured for spark.driver.
24/11/24 08:24:01 INFO ResourceUtils: ==============================================================
24/11/24 08:24:01 INFO SparkContext: Submitted application: Extract_DB_to_S3
24/11/24 08:24:01 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 4, script: , vendor: , memory -> name: memory, amount: 9486, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/11/24 08:24:01 INFO ResourceProfile: Limiting resource is cpus at 4 tasks per executor
24/11/24 08:24:01 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/11/24 08:24:01 INFO SecurityManager: Changing view acls to: hadoop
24/11/24 08:24:01 INFO SecurityManager: Changing modify acls to: hadoop
24/11/24 08:24:01 INFO SecurityManager: Changing view acls groups to: 
24/11/24 08:24:01 INFO SecurityManager: Changing modify acls groups to: 
24/11/24 08:24:01 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: hadoop; groups with view permissions: EMPTY; users with modify permissions: hadoop; groups with modify permissions: EMPTY
24/11/24 08:24:01 INFO Utils: Successfully started service 'sparkDriver' on port 44879.
24/11/24 08:24:01 INFO SparkEnv: Registering MapOutputTracker
24/11/24 08:24:01 INFO SparkEnv: Registering BlockManagerMaster
24/11/24 08:24:01 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/11/24 08:24:01 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/11/24 08:24:01 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/11/24 08:24:01 INFO DiskBlockManager: Created local directory at /mnt/tmp/blockmgr-2d392756-5ab9-4bc6-9f96-0ea963d6667b
24/11/24 08:24:01 INFO MemoryStore: MemoryStore started with capacity 912.3 MiB
24/11/24 08:24:01 INFO SparkEnv: Registering OutputCommitCoordinator
24/11/24 08:24:01 INFO SubResultCacheManager: Sub-result caches are disabled.
24/11/24 08:24:01 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/11/24 08:24:02 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/11/24 08:24:02 INFO SparkContext: Added JAR /home/hadoop/mysql-connector-j-9.1.0.jar at spark://ip-172-31-84-208.ec2.internal:44879/jars/mysql-connector-j-9.1.0.jar with timestamp 1732436641076
24/11/24 08:24:02 INFO Utils: Using 50 preallocated executors (minExecutors: 0). Set spark.dynamicAllocation.preallocateExecutors to `false` disable executor preallocation.
24/11/24 08:24:02 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at ip-172-31-84-208.ec2.internal/172.31.84.208:8032
24/11/24 08:24:03 INFO Configuration: resource-types.xml not found
24/11/24 08:24:03 INFO ResourceUtils: Unable to find 'resource-types.xml'.
24/11/24 08:24:03 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (12288 MB per container)
24/11/24 08:24:03 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
24/11/24 08:24:03 INFO Client: Setting up container launch context for our AM
24/11/24 08:24:03 INFO Client: Setting up the launch environment for our AM container
24/11/24 08:24:03 INFO Client: Preparing resources for our AM container
24/11/24 08:24:03 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
24/11/24 08:24:05 INFO Client: Uploading resource file:/mnt/tmp/spark-bad75001-ac57-492d-80ed-ff6f5e5bc282/__spark_libs__1805698188553639843.zip -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0010/__spark_libs__1805698188553639843.zip
24/11/24 08:24:06 INFO Client: Uploading resource file:/home/hadoop/mysql-connector-java-8.0.xx.jar -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0010/mysql-connector-java-8.0.xx.jar
24/11/24 08:24:06 INFO Client: Deleted staging directory hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0010
24/11/24 08:24:06 ERROR SparkContext: Error initializing SparkContext.
java.io.FileNotFoundException: File file:/home/hadoop/mysql-connector-java-8.0.xx.jar does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:832) ~[hadoop-client-api-3.3.3-amzn-6.jar:?]
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1153) ~[hadoop-client-api-3.3.3-amzn-6.jar:?]
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:822) ~[hadoop-client-api-3.3.3-amzn-6.jar:?]
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:472) ~[hadoop-client-api-3.3.3-amzn-6.jar:?]
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:390) ~[hadoop-client-api-3.3.3-amzn-6.jar:?]
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:341) ~[hadoop-client-api-3.3.3-amzn-6.jar:?]
	at org.apache.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:461) ~[spark-yarn_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
	at org.apache.spark.deploy.yarn.Client.distribute$1(Client.scala:557) ~[spark-yarn_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
	at org.apache.spark.deploy.yarn.Client.$anonfun$prepareLocalResources$26(Client.scala:709) ~[spark-yarn_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62) ~[scala-library-2.12.15.jar:?]
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55) ~[scala-library-2.12.15.jar:?]
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49) ~[scala-library-2.12.15.jar:?]
	at org.apache.spark.deploy.yarn.Client.$anonfun$prepareLocalResources$25(Client.scala:708) ~[spark-yarn_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
	at org.apache.spark.deploy.yarn.Client.$anonfun$prepareLocalResources$25$adapted(Client.scala:707) ~[spark-yarn_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
	at scala.collection.immutable.List.foreach(List.scala:431) ~[scala-library-2.12.15.jar:?]
	at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:707) ~[spark-yarn_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
	at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:984) ~[spark-yarn_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
	at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:221) ~[spark-yarn_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
	at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:62) ~[spark-yarn_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
	at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:235) ~[spark-core_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:591) ~[spark-core_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58) ~[spark-core_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_432]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_432]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_432]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_432]
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247) ~[py4j-0.10.9.7.jar:?]
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374) ~[py4j-0.10.9.7.jar:?]
	at py4j.Gateway.invoke(Gateway.java:238) ~[py4j-0.10.9.7.jar:?]
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80) ~[py4j-0.10.9.7.jar:?]
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69) ~[py4j-0.10.9.7.jar:?]
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182) ~[py4j-0.10.9.7.jar:?]
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106) ~[py4j-0.10.9.7.jar:?]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_432]
24/11/24 08:24:06 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/11/24 08:24:06 INFO SparkUI: Stopped Spark web UI at http://ip-172-31-84-208.ec2.internal:4040
24/11/24 08:24:06 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to send shutdown message before the AM has registered!
24/11/24 08:24:06 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to request executors before the AM has registered!
24/11/24 08:24:06 INFO YarnClientSchedulerBackend: Shutting down all executors
24/11/24 08:24:06 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
24/11/24 08:24:06 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
24/11/24 08:24:06 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/11/24 08:24:06 INFO MemoryStore: MemoryStore cleared
24/11/24 08:24:06 INFO BlockManager: BlockManager stopped
24/11/24 08:24:06 INFO BlockManagerMaster: BlockManagerMaster stopped
24/11/24 08:24:06 WARN MetricsSystem: Stopping a MetricsSystem that is not running
24/11/24 08:24:07 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/11/24 08:24:07 INFO SparkContext: Successfully stopped SparkContext
Traceback (most recent call last):
  File "/home/hadoop/extract_to_raw.py", line 6, in <module>
    .config("spark.jars", "/home/hadoop/mysql-connector-j-9.1.0.jar") \
  File "/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/session.py", line 477, in getOrCreate
  File "/usr/lib/spark/python/lib/pyspark.zip/pyspark/context.py", line 515, in getOrCreate
  File "/usr/lib/spark/python/lib/pyspark.zip/pyspark/context.py", line 212, in __init__
  File "/usr/lib/spark/python/lib/pyspark.zip/pyspark/context.py", line 287, in _do_init
  File "/usr/lib/spark/python/lib/pyspark.zip/pyspark/context.py", line 420, in _initialize_context
  File "/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1588, in __call__
  File "/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 328, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.
: java.io.FileNotFoundException: File file:/home/hadoop/mysql-connector-java-8.0.xx.jar does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:832)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1153)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:822)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:472)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:390)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:341)
	at org.apache.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:461)
	at org.apache.spark.deploy.yarn.Client.distribute$1(Client.scala:557)
	at org.apache.spark.deploy.yarn.Client.$anonfun$prepareLocalResources$26(Client.scala:709)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.deploy.yarn.Client.$anonfun$prepareLocalResources$25(Client.scala:708)
	at org.apache.spark.deploy.yarn.Client.$anonfun$prepareLocalResources$25$adapted(Client.scala:707)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:707)
	at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:984)
	at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:221)
	at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:62)
	at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:235)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:591)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.lang.Thread.run(Thread.java:750)

24/11/24 08:24:07 INFO ShutdownHookManager: Shutdown hook called
24/11/24 08:24:07 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-bad75001-ac57-492d-80ed-ff6f5e5bc282
24/11/24 08:24:07 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-a6c2ee29-f181-4a11-9195-41caaaf6f9d7
24/11/24 08:27:32 WARN DependencyUtils: Local jar /home/hadoop/mysql-connector-java-8.0.xx.jar does not exist, skipping.
24/11/24 08:27:33 INFO SparkContext: Running Spark version 3.4.1-amzn-1
24/11/24 08:27:33 INFO ResourceUtils: ==============================================================
24/11/24 08:27:33 INFO ResourceUtils: No custom resources configured for spark.driver.
24/11/24 08:27:33 INFO ResourceUtils: ==============================================================
24/11/24 08:27:33 INFO SparkContext: Submitted application: Extract_DB_to_S3
24/11/24 08:27:33 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 4, script: , vendor: , memory -> name: memory, amount: 9486, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/11/24 08:27:33 INFO ResourceProfile: Limiting resource is cpus at 4 tasks per executor
24/11/24 08:27:33 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/11/24 08:27:33 INFO SecurityManager: Changing view acls to: hadoop
24/11/24 08:27:33 INFO SecurityManager: Changing modify acls to: hadoop
24/11/24 08:27:33 INFO SecurityManager: Changing view acls groups to: 
24/11/24 08:27:33 INFO SecurityManager: Changing modify acls groups to: 
24/11/24 08:27:33 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: hadoop; groups with view permissions: EMPTY; users with modify permissions: hadoop; groups with modify permissions: EMPTY
24/11/24 08:27:34 INFO Utils: Successfully started service 'sparkDriver' on port 37497.
24/11/24 08:27:34 INFO SparkEnv: Registering MapOutputTracker
24/11/24 08:27:34 INFO SparkEnv: Registering BlockManagerMaster
24/11/24 08:27:34 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/11/24 08:27:34 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/11/24 08:27:34 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/11/24 08:27:34 INFO DiskBlockManager: Created local directory at /mnt/tmp/blockmgr-76ffdb7e-8fcb-429c-834f-29c84e0cca90
24/11/24 08:27:34 INFO MemoryStore: MemoryStore started with capacity 912.3 MiB
24/11/24 08:27:34 INFO SparkEnv: Registering OutputCommitCoordinator
24/11/24 08:27:34 INFO SubResultCacheManager: Sub-result caches are disabled.
24/11/24 08:27:34 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/11/24 08:27:34 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/11/24 08:27:34 INFO SparkContext: Added JAR /home/hadoop/mysql-connector-j-9.1.0.jar at spark://ip-172-31-84-208.ec2.internal:37497/jars/mysql-connector-j-9.1.0.jar with timestamp 1732436853585
24/11/24 08:27:34 INFO Utils: Using 50 preallocated executors (minExecutors: 0). Set spark.dynamicAllocation.preallocateExecutors to `false` disable executor preallocation.
24/11/24 08:27:34 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at ip-172-31-84-208.ec2.internal/172.31.84.208:8032
24/11/24 08:27:35 INFO Configuration: resource-types.xml not found
24/11/24 08:27:35 INFO ResourceUtils: Unable to find 'resource-types.xml'.
24/11/24 08:27:35 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (12288 MB per container)
24/11/24 08:27:35 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
24/11/24 08:27:35 INFO Client: Setting up container launch context for our AM
24/11/24 08:27:35 INFO Client: Setting up the launch environment for our AM container
24/11/24 08:27:35 INFO Client: Preparing resources for our AM container
24/11/24 08:27:35 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
24/11/24 08:27:38 INFO Client: Uploading resource file:/mnt/tmp/spark-04666ea5-b1e0-407b-a6ed-39f2fd938664/__spark_libs__2486544150378388107.zip -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0012/__spark_libs__2486544150378388107.zip
24/11/24 08:27:39 INFO Client: Uploading resource file:/home/hadoop/mysql-connector-java-8.0.xx.jar -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0012/mysql-connector-java-8.0.xx.jar
24/11/24 08:27:39 INFO Client: Deleted staging directory hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0012
24/11/24 08:27:39 ERROR SparkContext: Error initializing SparkContext.
java.io.FileNotFoundException: File file:/home/hadoop/mysql-connector-java-8.0.xx.jar does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:832) ~[hadoop-client-api-3.3.3-amzn-6.jar:?]
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1153) ~[hadoop-client-api-3.3.3-amzn-6.jar:?]
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:822) ~[hadoop-client-api-3.3.3-amzn-6.jar:?]
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:472) ~[hadoop-client-api-3.3.3-amzn-6.jar:?]
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:390) ~[hadoop-client-api-3.3.3-amzn-6.jar:?]
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:341) ~[hadoop-client-api-3.3.3-amzn-6.jar:?]
	at org.apache.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:461) ~[spark-yarn_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
	at org.apache.spark.deploy.yarn.Client.distribute$1(Client.scala:557) ~[spark-yarn_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
	at org.apache.spark.deploy.yarn.Client.$anonfun$prepareLocalResources$26(Client.scala:709) ~[spark-yarn_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62) ~[scala-library-2.12.15.jar:?]
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55) ~[scala-library-2.12.15.jar:?]
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49) ~[scala-library-2.12.15.jar:?]
	at org.apache.spark.deploy.yarn.Client.$anonfun$prepareLocalResources$25(Client.scala:708) ~[spark-yarn_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
	at org.apache.spark.deploy.yarn.Client.$anonfun$prepareLocalResources$25$adapted(Client.scala:707) ~[spark-yarn_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
	at scala.collection.immutable.List.foreach(List.scala:431) ~[scala-library-2.12.15.jar:?]
	at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:707) ~[spark-yarn_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
	at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:984) ~[spark-yarn_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
	at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:221) ~[spark-yarn_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
	at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:62) ~[spark-yarn_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
	at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:235) ~[spark-core_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:591) ~[spark-core_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58) ~[spark-core_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_432]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_432]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_432]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_432]
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247) ~[py4j-0.10.9.7.jar:?]
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374) ~[py4j-0.10.9.7.jar:?]
	at py4j.Gateway.invoke(Gateway.java:238) ~[py4j-0.10.9.7.jar:?]
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80) ~[py4j-0.10.9.7.jar:?]
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69) ~[py4j-0.10.9.7.jar:?]
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182) ~[py4j-0.10.9.7.jar:?]
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106) ~[py4j-0.10.9.7.jar:?]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_432]
24/11/24 08:27:39 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/11/24 08:27:39 INFO SparkUI: Stopped Spark web UI at http://ip-172-31-84-208.ec2.internal:4040
24/11/24 08:27:39 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to send shutdown message before the AM has registered!
24/11/24 08:27:39 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to request executors before the AM has registered!
24/11/24 08:27:39 INFO YarnClientSchedulerBackend: Shutting down all executors
24/11/24 08:27:39 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
24/11/24 08:27:39 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
24/11/24 08:27:39 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/11/24 08:27:39 INFO MemoryStore: MemoryStore cleared
24/11/24 08:27:39 INFO BlockManager: BlockManager stopped
24/11/24 08:27:39 INFO BlockManagerMaster: BlockManagerMaster stopped
24/11/24 08:27:39 WARN MetricsSystem: Stopping a MetricsSystem that is not running
24/11/24 08:27:39 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/11/24 08:27:39 INFO SparkContext: Successfully stopped SparkContext
Traceback (most recent call last):
  File "/home/hadoop/extract_to_raw.py", line 6, in <module>
    .config("spark.jars", "/home/hadoop/mysql-connector-j-9.1.0.jar") \
  File "/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/session.py", line 477, in getOrCreate
  File "/usr/lib/spark/python/lib/pyspark.zip/pyspark/context.py", line 515, in getOrCreate
  File "/usr/lib/spark/python/lib/pyspark.zip/pyspark/context.py", line 212, in __init__
  File "/usr/lib/spark/python/lib/pyspark.zip/pyspark/context.py", line 287, in _do_init
  File "/usr/lib/spark/python/lib/pyspark.zip/pyspark/context.py", line 420, in _initialize_context
  File "/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1588, in __call__
  File "/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 328, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.
: java.io.FileNotFoundException: File file:/home/hadoop/mysql-connector-java-8.0.xx.jar does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:832)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1153)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:822)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:472)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:390)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:341)
	at org.apache.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:461)
	at org.apache.spark.deploy.yarn.Client.distribute$1(Client.scala:557)
	at org.apache.spark.deploy.yarn.Client.$anonfun$prepareLocalResources$26(Client.scala:709)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.deploy.yarn.Client.$anonfun$prepareLocalResources$25(Client.scala:708)
	at org.apache.spark.deploy.yarn.Client.$anonfun$prepareLocalResources$25$adapted(Client.scala:707)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:707)
	at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:984)
	at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:221)
	at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:62)
	at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:235)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:591)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.lang.Thread.run(Thread.java:750)

24/11/24 08:27:39 INFO ShutdownHookManager: Shutdown hook called
24/11/24 08:27:39 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-04666ea5-b1e0-407b-a6ed-39f2fd938664
24/11/24 08:27:39 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-1863c08c-efe5-4c43-b847-bea8cee09a8c
24/11/24 08:34:22 WARN DependencyUtils: Local jar /home/hadoop/mysql-connector-java-8.0.xx.jar does not exist, skipping.
24/11/24 08:34:23 INFO SparkContext: Running Spark version 3.4.1-amzn-1
24/11/24 08:34:23 INFO ResourceUtils: ==============================================================
24/11/24 08:34:23 INFO ResourceUtils: No custom resources configured for spark.driver.
24/11/24 08:34:23 INFO ResourceUtils: ==============================================================
24/11/24 08:34:23 INFO SparkContext: Submitted application: Extract_DB_to_S3
24/11/24 08:34:23 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 4, script: , vendor: , memory -> name: memory, amount: 9486, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/11/24 08:34:23 INFO ResourceProfile: Limiting resource is cpus at 4 tasks per executor
24/11/24 08:34:23 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/11/24 08:34:23 INFO SecurityManager: Changing view acls to: hadoop
24/11/24 08:34:23 INFO SecurityManager: Changing modify acls to: hadoop
24/11/24 08:34:23 INFO SecurityManager: Changing view acls groups to: 
24/11/24 08:34:23 INFO SecurityManager: Changing modify acls groups to: 
24/11/24 08:34:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: hadoop; groups with view permissions: EMPTY; users with modify permissions: hadoop; groups with modify permissions: EMPTY
24/11/24 08:34:23 INFO Utils: Successfully started service 'sparkDriver' on port 44111.
24/11/24 08:34:23 INFO SparkEnv: Registering MapOutputTracker
24/11/24 08:34:23 INFO SparkEnv: Registering BlockManagerMaster
24/11/24 08:34:23 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/11/24 08:34:23 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/11/24 08:34:23 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/11/24 08:34:23 INFO DiskBlockManager: Created local directory at /mnt/tmp/blockmgr-e1a6a98b-232f-4323-ac5e-67778d09d6d7
24/11/24 08:34:23 INFO MemoryStore: MemoryStore started with capacity 912.3 MiB
24/11/24 08:34:23 INFO SparkEnv: Registering OutputCommitCoordinator
24/11/24 08:34:23 INFO SubResultCacheManager: Sub-result caches are disabled.
24/11/24 08:34:24 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/11/24 08:34:24 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/11/24 08:34:24 INFO SparkContext: Added JAR /home/hadoop/mysql-connector-j-9.1.0.jar at spark://ip-172-31-84-208.ec2.internal:44111/jars/mysql-connector-j-9.1.0.jar with timestamp 1732437263121
24/11/24 08:34:24 INFO Utils: Using 50 preallocated executors (minExecutors: 0). Set spark.dynamicAllocation.preallocateExecutors to `false` disable executor preallocation.
24/11/24 08:34:24 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at ip-172-31-84-208.ec2.internal/172.31.84.208:8032
24/11/24 08:34:24 INFO Configuration: resource-types.xml not found
24/11/24 08:34:24 INFO ResourceUtils: Unable to find 'resource-types.xml'.
24/11/24 08:34:24 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (12288 MB per container)
24/11/24 08:34:24 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
24/11/24 08:34:24 INFO Client: Setting up container launch context for our AM
24/11/24 08:34:24 INFO Client: Setting up the launch environment for our AM container
24/11/24 08:34:24 INFO Client: Preparing resources for our AM container
24/11/24 08:34:25 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
24/11/24 08:34:27 INFO Client: Uploading resource file:/mnt/tmp/spark-99d08c61-9351-4819-81de-4e3a78b0cb81/__spark_libs__3438083705112582759.zip -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0013/__spark_libs__3438083705112582759.zip
24/11/24 08:34:28 INFO Client: Uploading resource file:/home/hadoop/mysql-connector-java-8.0.xx.jar -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0013/mysql-connector-java-8.0.xx.jar
24/11/24 08:34:28 INFO Client: Deleted staging directory hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0013
24/11/24 08:34:28 ERROR SparkContext: Error initializing SparkContext.
java.io.FileNotFoundException: File file:/home/hadoop/mysql-connector-java-8.0.xx.jar does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:832) ~[hadoop-client-api-3.3.3-amzn-6.jar:?]
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1153) ~[hadoop-client-api-3.3.3-amzn-6.jar:?]
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:822) ~[hadoop-client-api-3.3.3-amzn-6.jar:?]
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:472) ~[hadoop-client-api-3.3.3-amzn-6.jar:?]
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:390) ~[hadoop-client-api-3.3.3-amzn-6.jar:?]
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:341) ~[hadoop-client-api-3.3.3-amzn-6.jar:?]
	at org.apache.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:461) ~[spark-yarn_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
	at org.apache.spark.deploy.yarn.Client.distribute$1(Client.scala:557) ~[spark-yarn_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
	at org.apache.spark.deploy.yarn.Client.$anonfun$prepareLocalResources$26(Client.scala:709) ~[spark-yarn_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62) ~[scala-library-2.12.15.jar:?]
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55) ~[scala-library-2.12.15.jar:?]
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49) ~[scala-library-2.12.15.jar:?]
	at org.apache.spark.deploy.yarn.Client.$anonfun$prepareLocalResources$25(Client.scala:708) ~[spark-yarn_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
	at org.apache.spark.deploy.yarn.Client.$anonfun$prepareLocalResources$25$adapted(Client.scala:707) ~[spark-yarn_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
	at scala.collection.immutable.List.foreach(List.scala:431) ~[scala-library-2.12.15.jar:?]
	at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:707) ~[spark-yarn_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
	at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:984) ~[spark-yarn_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
	at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:221) ~[spark-yarn_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
	at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:62) ~[spark-yarn_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
	at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:235) ~[spark-core_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:591) ~[spark-core_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58) ~[spark-core_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_432]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_432]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_432]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_432]
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247) ~[py4j-0.10.9.7.jar:?]
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374) ~[py4j-0.10.9.7.jar:?]
	at py4j.Gateway.invoke(Gateway.java:238) ~[py4j-0.10.9.7.jar:?]
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80) ~[py4j-0.10.9.7.jar:?]
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69) ~[py4j-0.10.9.7.jar:?]
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182) ~[py4j-0.10.9.7.jar:?]
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106) ~[py4j-0.10.9.7.jar:?]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_432]
24/11/24 08:34:28 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/11/24 08:34:28 INFO SparkUI: Stopped Spark web UI at http://ip-172-31-84-208.ec2.internal:4040
24/11/24 08:34:28 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to send shutdown message before the AM has registered!
24/11/24 08:34:28 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to request executors before the AM has registered!
24/11/24 08:34:28 INFO YarnClientSchedulerBackend: Shutting down all executors
24/11/24 08:34:28 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
24/11/24 08:34:28 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
24/11/24 08:34:28 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/11/24 08:34:28 INFO MemoryStore: MemoryStore cleared
24/11/24 08:34:28 INFO BlockManager: BlockManager stopped
24/11/24 08:34:28 INFO BlockManagerMaster: BlockManagerMaster stopped
24/11/24 08:34:28 WARN MetricsSystem: Stopping a MetricsSystem that is not running
24/11/24 08:34:28 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/11/24 08:34:28 INFO SparkContext: Successfully stopped SparkContext
Traceback (most recent call last):
  File "/home/hadoop/extract_to_raw.py", line 6, in <module>
    .config("spark.jars", "/home/hadoop/mysql-connector-j-9.1.0.jar") \
  File "/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/session.py", line 477, in getOrCreate
  File "/usr/lib/spark/python/lib/pyspark.zip/pyspark/context.py", line 515, in getOrCreate
  File "/usr/lib/spark/python/lib/pyspark.zip/pyspark/context.py", line 212, in __init__
  File "/usr/lib/spark/python/lib/pyspark.zip/pyspark/context.py", line 287, in _do_init
  File "/usr/lib/spark/python/lib/pyspark.zip/pyspark/context.py", line 420, in _initialize_context
  File "/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1588, in __call__
  File "/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 328, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.
: java.io.FileNotFoundException: File file:/home/hadoop/mysql-connector-java-8.0.xx.jar does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:832)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1153)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:822)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:472)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:390)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:341)
	at org.apache.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:461)
	at org.apache.spark.deploy.yarn.Client.distribute$1(Client.scala:557)
	at org.apache.spark.deploy.yarn.Client.$anonfun$prepareLocalResources$26(Client.scala:709)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.deploy.yarn.Client.$anonfun$prepareLocalResources$25(Client.scala:708)
	at org.apache.spark.deploy.yarn.Client.$anonfun$prepareLocalResources$25$adapted(Client.scala:707)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:707)
	at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:984)
	at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:221)
	at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:62)
	at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:235)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:591)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.lang.Thread.run(Thread.java:750)

24/11/24 08:34:28 INFO ShutdownHookManager: Shutdown hook called
24/11/24 08:34:28 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-99d08c61-9351-4819-81de-4e3a78b0cb81
24/11/24 08:34:28 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-f222eaf1-ee41-4e66-9d2e-6d861f878b44
24/11/24 08:38:00 WARN DependencyUtils: Local jar /home/hadoop/mysql-connector-java-8.0.xx.jar does not exist, skipping.
24/11/24 08:38:01 INFO SparkContext: Running Spark version 3.4.1-amzn-1
24/11/24 08:38:01 INFO ResourceUtils: ==============================================================
24/11/24 08:38:01 INFO ResourceUtils: No custom resources configured for spark.driver.
24/11/24 08:38:01 INFO ResourceUtils: ==============================================================
24/11/24 08:38:01 INFO SparkContext: Submitted application: Extract_DB_to_S3
24/11/24 08:38:01 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 4, script: , vendor: , memory -> name: memory, amount: 9486, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/11/24 08:38:01 INFO ResourceProfile: Limiting resource is cpus at 4 tasks per executor
24/11/24 08:38:01 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/11/24 08:38:01 INFO SecurityManager: Changing view acls to: hadoop
24/11/24 08:38:01 INFO SecurityManager: Changing modify acls to: hadoop
24/11/24 08:38:01 INFO SecurityManager: Changing view acls groups to: 
24/11/24 08:38:01 INFO SecurityManager: Changing modify acls groups to: 
24/11/24 08:38:01 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: hadoop; groups with view permissions: EMPTY; users with modify permissions: hadoop; groups with modify permissions: EMPTY
24/11/24 08:38:01 INFO Utils: Successfully started service 'sparkDriver' on port 37553.
24/11/24 08:38:01 INFO SparkEnv: Registering MapOutputTracker
24/11/24 08:38:01 INFO SparkEnv: Registering BlockManagerMaster
24/11/24 08:38:01 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/11/24 08:38:01 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/11/24 08:38:01 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/11/24 08:38:01 INFO DiskBlockManager: Created local directory at /mnt/tmp/blockmgr-36c7b5b1-518b-4d9c-9f2c-cd268fd34d05
24/11/24 08:38:01 INFO MemoryStore: MemoryStore started with capacity 912.3 MiB
24/11/24 08:38:01 INFO SparkEnv: Registering OutputCommitCoordinator
24/11/24 08:38:01 INFO SubResultCacheManager: Sub-result caches are disabled.
24/11/24 08:38:02 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/11/24 08:38:02 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/11/24 08:38:02 INFO SparkContext: Added JAR /home/hadoop/mysql-connector-j-9.1.0.jar at spark://ip-172-31-84-208.ec2.internal:37553/jars/mysql-connector-j-9.1.0.jar with timestamp 1732437481150
24/11/24 08:38:02 INFO Utils: Using 50 preallocated executors (minExecutors: 0). Set spark.dynamicAllocation.preallocateExecutors to `false` disable executor preallocation.
24/11/24 08:38:02 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at ip-172-31-84-208.ec2.internal/172.31.84.208:8032
24/11/24 08:38:02 INFO Configuration: resource-types.xml not found
24/11/24 08:38:02 INFO ResourceUtils: Unable to find 'resource-types.xml'.
24/11/24 08:38:02 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (12288 MB per container)
24/11/24 08:38:02 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
24/11/24 08:38:02 INFO Client: Setting up container launch context for our AM
24/11/24 08:38:02 INFO Client: Setting up the launch environment for our AM container
24/11/24 08:38:02 INFO Client: Preparing resources for our AM container
24/11/24 08:38:02 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
24/11/24 08:38:05 INFO Client: Uploading resource file:/mnt/tmp/spark-80d49319-c719-42a2-8f58-400128311aae/__spark_libs__6795716620935495594.zip -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0014/__spark_libs__6795716620935495594.zip
24/11/24 08:38:06 INFO Client: Uploading resource file:/home/hadoop/mysql-connector-java-8.0.xx.jar -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0014/mysql-connector-java-8.0.xx.jar
24/11/24 08:38:06 INFO Client: Deleted staging directory hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0014
24/11/24 08:38:06 ERROR SparkContext: Error initializing SparkContext.
java.io.FileNotFoundException: File file:/home/hadoop/mysql-connector-java-8.0.xx.jar does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:832) ~[hadoop-client-api-3.3.3-amzn-6.jar:?]
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1153) ~[hadoop-client-api-3.3.3-amzn-6.jar:?]
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:822) ~[hadoop-client-api-3.3.3-amzn-6.jar:?]
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:472) ~[hadoop-client-api-3.3.3-amzn-6.jar:?]
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:390) ~[hadoop-client-api-3.3.3-amzn-6.jar:?]
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:341) ~[hadoop-client-api-3.3.3-amzn-6.jar:?]
	at org.apache.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:461) ~[spark-yarn_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
	at org.apache.spark.deploy.yarn.Client.distribute$1(Client.scala:557) ~[spark-yarn_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
	at org.apache.spark.deploy.yarn.Client.$anonfun$prepareLocalResources$26(Client.scala:709) ~[spark-yarn_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62) ~[scala-library-2.12.15.jar:?]
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55) ~[scala-library-2.12.15.jar:?]
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49) ~[scala-library-2.12.15.jar:?]
	at org.apache.spark.deploy.yarn.Client.$anonfun$prepareLocalResources$25(Client.scala:708) ~[spark-yarn_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
	at org.apache.spark.deploy.yarn.Client.$anonfun$prepareLocalResources$25$adapted(Client.scala:707) ~[spark-yarn_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
	at scala.collection.immutable.List.foreach(List.scala:431) ~[scala-library-2.12.15.jar:?]
	at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:707) ~[spark-yarn_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
	at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:984) ~[spark-yarn_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
	at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:221) ~[spark-yarn_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
	at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:62) ~[spark-yarn_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
	at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:235) ~[spark-core_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:591) ~[spark-core_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58) ~[spark-core_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_432]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_432]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_432]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_432]
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247) ~[py4j-0.10.9.7.jar:?]
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374) ~[py4j-0.10.9.7.jar:?]
	at py4j.Gateway.invoke(Gateway.java:238) ~[py4j-0.10.9.7.jar:?]
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80) ~[py4j-0.10.9.7.jar:?]
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69) ~[py4j-0.10.9.7.jar:?]
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182) ~[py4j-0.10.9.7.jar:?]
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106) ~[py4j-0.10.9.7.jar:?]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_432]
24/11/24 08:38:06 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/11/24 08:38:06 INFO SparkUI: Stopped Spark web UI at http://ip-172-31-84-208.ec2.internal:4040
24/11/24 08:38:06 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to send shutdown message before the AM has registered!
24/11/24 08:38:06 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to request executors before the AM has registered!
24/11/24 08:38:06 INFO YarnClientSchedulerBackend: Shutting down all executors
24/11/24 08:38:06 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
24/11/24 08:38:06 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
24/11/24 08:38:06 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/11/24 08:38:06 INFO MemoryStore: MemoryStore cleared
24/11/24 08:38:06 INFO BlockManager: BlockManager stopped
24/11/24 08:38:06 INFO BlockManagerMaster: BlockManagerMaster stopped
24/11/24 08:38:06 WARN MetricsSystem: Stopping a MetricsSystem that is not running
24/11/24 08:38:06 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/11/24 08:38:06 INFO SparkContext: Successfully stopped SparkContext
Traceback (most recent call last):
  File "/home/hadoop/extract_to_raw.py", line 6, in <module>
    .config("spark.jars", "/home/hadoop/mysql-connector-j-9.1.0.jar") \
  File "/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/session.py", line 477, in getOrCreate
  File "/usr/lib/spark/python/lib/pyspark.zip/pyspark/context.py", line 515, in getOrCreate
  File "/usr/lib/spark/python/lib/pyspark.zip/pyspark/context.py", line 212, in __init__
  File "/usr/lib/spark/python/lib/pyspark.zip/pyspark/context.py", line 287, in _do_init
  File "/usr/lib/spark/python/lib/pyspark.zip/pyspark/context.py", line 420, in _initialize_context
  File "/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1588, in __call__
  File "/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 328, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.
: java.io.FileNotFoundException: File file:/home/hadoop/mysql-connector-java-8.0.xx.jar does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:832)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1153)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:822)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:472)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:390)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:341)
	at org.apache.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:461)
	at org.apache.spark.deploy.yarn.Client.distribute$1(Client.scala:557)
	at org.apache.spark.deploy.yarn.Client.$anonfun$prepareLocalResources$26(Client.scala:709)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.deploy.yarn.Client.$anonfun$prepareLocalResources$25(Client.scala:708)
	at org.apache.spark.deploy.yarn.Client.$anonfun$prepareLocalResources$25$adapted(Client.scala:707)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:707)
	at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:984)
	at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:221)
	at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:62)
	at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:235)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:591)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.lang.Thread.run(Thread.java:750)

24/11/24 08:38:06 INFO ShutdownHookManager: Shutdown hook called
24/11/24 08:38:06 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-4a119a99-c4ef-4cce-bffd-44d066a65ba7
24/11/24 08:38:06 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-80d49319-c719-42a2-8f58-400128311aae
24/11/24 08:39:46 INFO SparkContext: Running Spark version 3.4.1-amzn-1
24/11/24 08:39:46 INFO ResourceUtils: ==============================================================
24/11/24 08:39:46 INFO ResourceUtils: No custom resources configured for spark.driver.
24/11/24 08:39:46 INFO ResourceUtils: ==============================================================
24/11/24 08:39:46 INFO SparkContext: Submitted application: Extract_DB_to_S3
24/11/24 08:39:46 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 4, script: , vendor: , memory -> name: memory, amount: 9486, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/11/24 08:39:46 INFO ResourceProfile: Limiting resource is cpus at 4 tasks per executor
24/11/24 08:39:46 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/11/24 08:39:46 INFO SecurityManager: Changing view acls to: hadoop
24/11/24 08:39:46 INFO SecurityManager: Changing modify acls to: hadoop
24/11/24 08:39:46 INFO SecurityManager: Changing view acls groups to: 
24/11/24 08:39:46 INFO SecurityManager: Changing modify acls groups to: 
24/11/24 08:39:46 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: hadoop; groups with view permissions: EMPTY; users with modify permissions: hadoop; groups with modify permissions: EMPTY
24/11/24 08:39:46 INFO Utils: Successfully started service 'sparkDriver' on port 33361.
24/11/24 08:39:46 INFO SparkEnv: Registering MapOutputTracker
24/11/24 08:39:47 INFO SparkEnv: Registering BlockManagerMaster
24/11/24 08:39:47 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/11/24 08:39:47 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/11/24 08:39:47 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/11/24 08:39:47 INFO DiskBlockManager: Created local directory at /mnt/tmp/blockmgr-db3fb338-d2ec-455f-9a90-ec2cbd3d2043
24/11/24 08:39:47 INFO MemoryStore: MemoryStore started with capacity 912.3 MiB
24/11/24 08:39:47 INFO SparkEnv: Registering OutputCommitCoordinator
24/11/24 08:39:47 INFO SubResultCacheManager: Sub-result caches are disabled.
24/11/24 08:39:47 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/11/24 08:39:47 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/11/24 08:39:47 INFO SparkContext: Added JAR /home/hadoop/mysql-connector-j-9.1.0.jar at spark://ip-172-31-84-208.ec2.internal:33361/jars/mysql-connector-j-9.1.0.jar with timestamp 1732437586489
24/11/24 08:39:47 INFO Utils: Using 50 preallocated executors (minExecutors: 0). Set spark.dynamicAllocation.preallocateExecutors to `false` disable executor preallocation.
24/11/24 08:39:48 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at ip-172-31-84-208.ec2.internal/172.31.84.208:8032
24/11/24 08:39:52 INFO Configuration: resource-types.xml not found
24/11/24 08:39:52 INFO ResourceUtils: Unable to find 'resource-types.xml'.
24/11/24 08:39:52 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (12288 MB per container)
24/11/24 08:39:52 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
24/11/24 08:39:52 INFO Client: Setting up container launch context for our AM
24/11/24 08:39:52 INFO Client: Setting up the launch environment for our AM container
24/11/24 08:39:52 INFO Client: Preparing resources for our AM container
24/11/24 08:39:55 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
24/11/24 08:39:58 INFO Client: Uploading resource file:/mnt/tmp/spark-28258b36-ca71-4ce6-9068-02798f64edee/__spark_libs__6305821144027986727.zip -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0015/__spark_libs__6305821144027986727.zip
24/11/24 08:39:59 INFO Client: Uploading resource file:/home/hadoop/mysql-connector-j-9.1.0.jar -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0015/mysql-connector-j-9.1.0.jar
24/11/24 08:39:59 INFO Client: Uploading resource file:/etc/spark/conf.dist/hive-site.xml -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0015/hive-site.xml
24/11/24 08:39:59 INFO Client: Uploading resource file:/etc/hudi/conf.dist/hudi-defaults.conf -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0015/hudi-defaults.conf
24/11/24 08:39:59 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0015/pyspark.zip
24/11/24 08:39:59 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0015/py4j-0.10.9.7-src.zip
24/11/24 08:39:59 INFO Client: Uploading resource file:/mnt/tmp/spark-28258b36-ca71-4ce6-9068-02798f64edee/__spark_conf__2996748135211940788.zip -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0015/__spark_conf__.zip
24/11/24 08:39:59 INFO SecurityManager: Changing view acls to: hadoop
24/11/24 08:39:59 INFO SecurityManager: Changing modify acls to: hadoop
24/11/24 08:39:59 INFO SecurityManager: Changing view acls groups to: 
24/11/24 08:39:59 INFO SecurityManager: Changing modify acls groups to: 
24/11/24 08:39:59 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: hadoop; groups with view permissions: EMPTY; users with modify permissions: hadoop; groups with modify permissions: EMPTY
24/11/24 08:39:59 INFO Client: Submitting application application_1732429252608_0015 to ResourceManager
24/11/24 08:39:59 INFO YarnClientImpl: Submitted application application_1732429252608_0015
24/11/24 08:40:00 INFO Client: Application report for application_1732429252608_0015 (state: ACCEPTED)
24/11/24 08:40:00 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1732437599692
	 final status: UNDEFINED
	 tracking URL: http://ip-172-31-84-208.ec2.internal:20888/proxy/application_1732429252608_0015/
	 user: hadoop
24/11/24 08:40:01 INFO Client: Application report for application_1732429252608_0015 (state: ACCEPTED)
24/11/24 08:40:02 INFO Client: Application report for application_1732429252608_0015 (state: ACCEPTED)
24/11/24 08:40:03 INFO Client: Application report for application_1732429252608_0015 (state: ACCEPTED)
24/11/24 08:40:04 INFO Client: Application report for application_1732429252608_0015 (state: ACCEPTED)
24/11/24 08:40:04 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> ip-172-31-84-208.ec2.internal, PROXY_URI_BASES -> http://ip-172-31-84-208.ec2.internal:20888/proxy/application_1732429252608_0015), /proxy/application_1732429252608_0015
24/11/24 08:40:05 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
24/11/24 08:40:05 INFO Client: Application report for application_1732429252608_0015 (state: RUNNING)
24/11/24 08:40:05 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 172.31.80.66
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1732437599692
	 final status: UNDEFINED
	 tracking URL: http://ip-172-31-84-208.ec2.internal:20888/proxy/application_1732429252608_0015/
	 user: hadoop
24/11/24 08:40:05 INFO YarnClientSchedulerBackend: Application application_1732429252608_0015 has started running.
24/11/24 08:40:05 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35735.
24/11/24 08:40:05 INFO NettyBlockTransferService: Server created on ip-172-31-84-208.ec2.internal:35735
24/11/24 08:40:05 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/11/24 08:40:05 INFO BlockManager: external shuffle service port = 7337
24/11/24 08:40:05 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ip-172-31-84-208.ec2.internal, 35735, None)
24/11/24 08:40:05 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-84-208.ec2.internal:35735 with 912.3 MiB RAM, BlockManagerId(driver, ip-172-31-84-208.ec2.internal, 35735, None)
24/11/24 08:40:05 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ip-172-31-84-208.ec2.internal, 35735, None)
24/11/24 08:40:05 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ip-172-31-84-208.ec2.internal, 35735, None)
24/11/24 08:40:05 INFO SingleEventLogFileWriter: Logging events to hdfs:/var/log/spark/apps/application_1732429252608_0015.inprogress
24/11/24 08:40:06 INFO Utils: Using 50 preallocated executors (minExecutors: 0). Set spark.dynamicAllocation.preallocateExecutors to `false` disable executor preallocation.
24/11/24 08:40:06 INFO ServerInfo: Adding filter to /jobs: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 08:40:06 INFO ServerInfo: Adding filter to /jobs/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 08:40:06 INFO ServerInfo: Adding filter to /jobs/job: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 08:40:06 INFO ServerInfo: Adding filter to /jobs/job/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 08:40:06 INFO ServerInfo: Adding filter to /stages: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 08:40:06 INFO ServerInfo: Adding filter to /stages/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 08:40:06 INFO ServerInfo: Adding filter to /stages/stage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 08:40:06 INFO ServerInfo: Adding filter to /stages/stage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 08:40:06 INFO ServerInfo: Adding filter to /stages/pool: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 08:40:06 INFO ServerInfo: Adding filter to /stages/pool/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 08:40:06 INFO ServerInfo: Adding filter to /storage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 08:40:06 INFO ServerInfo: Adding filter to /storage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 08:40:06 INFO ServerInfo: Adding filter to /storage/rdd: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 08:40:06 INFO ServerInfo: Adding filter to /storage/rdd/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 08:40:06 INFO ServerInfo: Adding filter to /environment: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 08:40:06 INFO ServerInfo: Adding filter to /environment/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 08:40:06 INFO ServerInfo: Adding filter to /executors: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 08:40:06 INFO ServerInfo: Adding filter to /executors/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 08:40:06 INFO ServerInfo: Adding filter to /executors/threadDump: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 08:40:06 INFO ServerInfo: Adding filter to /executors/threadDump/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 08:40:06 INFO ServerInfo: Adding filter to /static: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 08:40:06 INFO ServerInfo: Adding filter to /: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 08:40:06 INFO ServerInfo: Adding filter to /api: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 08:40:06 INFO ServerInfo: Adding filter to /jobs/job/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 08:40:06 INFO ServerInfo: Adding filter to /stages/stage/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 08:40:06 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 08:40:06 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
24/11/24 08:40:06 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
24/11/24 08:40:06 INFO SharedState: Warehouse path is 'hdfs://ip-172-31-84-208.ec2.internal:8020/user/spark/warehouse'.
24/11/24 08:40:06 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 08:40:06 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 08:40:06 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 08:40:06 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 08:40:06 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 08:40:09 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.31.80.66:58034) with ID 2,  ResourceProfileId 0
24/11/24 08:40:09 INFO ExecutorMonitor: New executor 2 has registered (new total is 1)
24/11/24 08:40:09 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-80-66.ec2.internal:42625 with 4.8 GiB RAM, BlockManagerId(2, ip-172-31-80-66.ec2.internal, 42625, None)
Datos extrados de la base de datos:
24/11/24 08:40:11 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.31.90.72:44782) with ID 1,  ResourceProfileId 0
24/11/24 08:40:11 INFO ExecutorMonitor: New executor 1 has registered (new total is 2)
24/11/24 08:40:11 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-90-72.ec2.internal:38769 with 4.8 GiB RAM, BlockManagerId(1, ip-172-31-90-72.ec2.internal, 38769, None)
24/11/24 08:40:12 INFO CodeGenerator: Code generated in 391.664607 ms
24/11/24 08:40:12 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
24/11/24 08:40:12 INFO DAGScheduler: Got job 0 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/11/24 08:40:12 INFO DAGScheduler: Final stage: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0)
24/11/24 08:40:12 INFO DAGScheduler: Parents of final stage: List()
24/11/24 08:40:12 INFO DAGScheduler: Missing parents: List()
24/11/24 08:40:12 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
24/11/24 08:40:12 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 17.9 KiB, free 912.3 MiB)
24/11/24 08:40:13 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 912.3 MiB)
24/11/24 08:40:13 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ip-172-31-84-208.ec2.internal:35735 (size: 7.2 KiB, free: 912.3 MiB)
24/11/24 08:40:13 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1592
24/11/24 08:40:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/11/24 08:40:13 INFO YarnScheduler: Adding task set 0.0 with 1 tasks resource profile 0
24/11/24 08:40:13 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (ip-172-31-80-66.ec2.internal, executor 2, partition 0, PROCESS_LOCAL, 7220 bytes) 
24/11/24 08:40:13 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ip-172-31-80-66.ec2.internal:42625 (size: 7.2 KiB, free: 4.8 GiB)
24/11/24 08:40:15 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2185 ms on ip-172-31-80-66.ec2.internal (executor 2) (1/1)
24/11/24 08:40:15 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/11/24 08:40:15 INFO DAGScheduler: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0) finished in 2.498 s
24/11/24 08:40:15 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
24/11/24 08:40:15 INFO YarnScheduler: Killing all running tasks in stage 0: Stage finished
24/11/24 08:40:15 INFO DAGScheduler: Job 0 finished: showString at NativeMethodAccessorImpl.java:0, took 2.585721 s
24/11/24 08:40:15 INFO CodeGenerator: Code generated in 121.188345 ms
+-----------------+-------+------------------+----------------------------+-------------------+-------------------------+----------------+----+------------------+----+-------------+--------------+------+---------------+--------------------+----------+---------------------+------------+-----------------+------------------+-----------------+------------------+-------------------+
|fecha_reporte_web|id_caso|fecha_notificacion|codigo_divipola_departamento|nombre_departamento|codigo_divipola_municipio|nombre_municipio|edad|unidad_medida_edad|sexo|tipo_contagio|ubicacion_caso|estado|codigo_iso_pais|         nombre_pais|recuperado|fecha_inicio_sintomas|fecha_muerte|fecha_diagnostico|fecha_recuperacion|tipo_recuperacion|pertenencia_etnica|nombre_grupo_etnico|
+-----------------+-------+------------------+----------------------------+-------------------+-------------------------+----------------+----+------------------+----+-------------+--------------+------+---------------+--------------------+----------+---------------------+------------+-----------------+------------------+-----------------+------------------+-------------------+
| 6/3/2020 0:00:00|      1|  2/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  19|                 1|   F|    Importado|          Casa|  Leve|            380|              ITALIA|Recuperado|    27/2/2020 0:00:00|            | 6/3/2020 0:00:00| 13/3/2020 0:00:00|              PCR|                 6|                   |
| 9/3/2020 0:00:00|      2|  6/3/2020 0:00:00|                          76|              VALLE|                    76111|            BUGA|  34|                 1|   M|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     4/3/2020 0:00:00|            | 9/3/2020 0:00:00| 19/3/2020 0:00:00|              PCR|                 5|                   |
| 9/3/2020 0:00:00|      3|  7/3/2020 0:00:00|                           5|          ANTIOQUIA|                     5001|        MEDELLIN|  50|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|    29/2/2020 0:00:00|            | 9/3/2020 0:00:00| 15/3/2020 0:00:00|              PCR|                 6|                   |
|11/3/2020 0:00:00|      4|  9/3/2020 0:00:00|                           5|          ANTIOQUIA|                     5001|        MEDELLIN|  55|                 1|   M|  Relacionado|          Casa|  Leve|               |                    |Recuperado|     6/3/2020 0:00:00|            |11/3/2020 0:00:00| 26/3/2020 0:00:00|              PCR|                 6|                   |
|11/3/2020 0:00:00|      5|  9/3/2020 0:00:00|                           5|          ANTIOQUIA|                     5001|        MEDELLIN|  25|                 1|   M|  Relacionado|          Casa|  Leve|               |                    |Recuperado|     8/3/2020 0:00:00|            |11/3/2020 0:00:00| 23/3/2020 0:00:00|              PCR|                 6|                   |
|11/3/2020 0:00:00|      6| 10/3/2020 0:00:00|                           5|          ANTIOQUIA|                     5360|          ITAGUI|  27|                 1|   F|  Relacionado|          Casa|  Leve|               |                    |Recuperado|     6/3/2020 0:00:00|            |11/3/2020 0:00:00| 26/3/2020 0:00:00|              PCR|                 6|                   |
|11/3/2020 0:00:00|      7|  8/3/2020 0:00:00|                       13001|          CARTAGENA|                    13001|       CARTAGENA|  85|                 1|   F|    Importado|          Casa|  Leve|            840|ESTADOS UNIDOS DE...|Recuperado|     2/3/2020 0:00:00|            |11/3/2020 0:00:00| 17/3/2020 0:00:00|              PCR|                 6|                   |
|11/3/2020 0:00:00|      8|  9/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  22|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     6/3/2020 0:00:00|            |11/3/2020 0:00:00| 21/3/2020 0:00:00|              PCR|                 6|                   |
|11/3/2020 0:00:00|      9|  8/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  28|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     7/3/2020 0:00:00|            |11/3/2020 0:00:00| 23/3/2020 0:00:00|              PCR|                 6|                   |
|12/3/2020 0:00:00|     10| 12/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  36|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     6/3/2020 0:00:00|            |12/3/2020 0:00:00| 21/3/2020 0:00:00|              PCR|                 6|                   |
|12/3/2020 0:00:00|     11| 11/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  42|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     6/3/2020 0:00:00|            |12/3/2020 0:00:00| 31/3/2020 0:00:00|              PCR|                 6|                   |
|12/3/2020 0:00:00|     12| 10/3/2020 0:00:00|                          41|              HUILA|                    41001|           NEIVA|  74|                 1|   F|    Importado|          Casa|  Leve|            380|              ITALIA|Recuperado|     6/3/2020 0:00:00|            |12/3/2020 0:00:00|  9/4/2020 0:00:00|              PCR|                 6|                   |
|12/3/2020 0:00:00|     13| 10/3/2020 0:00:00|                          41|              HUILA|                    41001|           NEIVA|  68|                 1|   F|  Relacionado|          Casa|  Leve|               |                    |Recuperado|     6/3/2020 0:00:00|            |12/3/2020 0:00:00| 30/3/2020 0:00:00|              PCR|                 6|                   |
|13/3/2020 0:00:00|     14| 10/3/2020 0:00:00|                          76|              VALLE|                    76520|         PALMIRA|  48|                 1|   M|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     7/3/2020 0:00:00|            |13/3/2020 0:00:00| 21/3/2020 0:00:00|              PCR|                 5|                   |
|13/3/2020 0:00:00|     15| 13/3/2020 0:00:00|                          50|               META|                    50001|   VILLAVICENCIO|  30|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     9/3/2020 0:00:00|            |13/3/2020 0:00:00| 23/3/2020 0:00:00|              PCR|                 6|                   |
|13/3/2020 0:00:00|     16| 11/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  61|                 1|   F|    Importado|          Casa|  Leve|            380|              ITALIA|Recuperado|     8/3/2020 0:00:00|            |13/3/2020 0:00:00| 23/3/2020 0:00:00|              PCR|                 5|                   |
|14/3/2020 0:00:00|     17| 11/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  73|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|    28/2/2020 0:00:00|            |14/3/2020 0:00:00| 14/3/2020 0:00:00|              PCR|                 6|                   |
|14/3/2020 0:00:00|     18| 11/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  54|                 1|   M|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     7/3/2020 0:00:00|            |14/3/2020 0:00:00|  7/4/2020 0:00:00|           Tiempo|                 6|                   |
|14/3/2020 0:00:00|     19| 12/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  54|                 1|   F|  Relacionado|          Casa|  Leve|               |                    |Recuperado|     9/3/2020 0:00:00|            |14/3/2020 0:00:00| 24/3/2020 0:00:00|              PCR|                 6|                   |
|14/3/2020 0:00:00|     20| 11/3/2020 0:00:00|                           5|          ANTIOQUIA|                     5001|        MEDELLIN|  26|                 1|   F|  Relacionado|          Casa|  Leve|               |                    |Recuperado|     9/3/2020 0:00:00|            |14/3/2020 0:00:00| 24/3/2020 0:00:00|              PCR|                 6|                   |
+-----------------+-------+------------------+----------------------------+-------------------+-------------------------+----------------+----+------------------+----+-------------+--------------+------+---------------+--------------------+----------+---------------------+------------+-----------------+------------------+-----------------+------------------+-------------------+
only showing top 20 rows

24/11/24 08:40:16 INFO BlockManagerInfo: Removed broadcast_0_piece0 on ip-172-31-80-66.ec2.internal:42625 in memory (size: 7.2 KiB, free: 4.8 GiB)
24/11/24 08:40:16 INFO BlockManagerInfo: Removed broadcast_0_piece0 on ip-172-31-84-208.ec2.internal:35735 in memory (size: 7.2 KiB, free: 912.3 MiB)
24/11/24 08:40:16 INFO ClientConfigurationFactory: Set initial getObject socket timeout to 2000 ms.
24/11/24 08:40:18 INFO ParquetUtils: Using user defined output committer for Parquet: com.amazon.emr.committer.EmrOptimizedSparkSqlParquetOutputCommitter
24/11/24 08:40:18 INFO SQLConfCommitterProvider: Getting user defined output committer class com.amazon.emr.committer.EmrOptimizedSparkSqlParquetOutputCommitter
24/11/24 08:40:18 INFO EmrOptimizedParquetOutputCommitter: EMR Optimized Committer: ENABLED
24/11/24 08:40:18 INFO EmrOptimizedParquetOutputCommitter: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileSystemOptimizedCommitter
24/11/24 08:40:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
24/11/24 08:40:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: true
24/11/24 08:40:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
24/11/24 08:40:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: true
24/11/24 08:40:18 INFO SQLConfCommitterProvider: Using output committer class com.amazon.emr.committer.EmrOptimizedSparkSqlParquetOutputCommitter
24/11/24 08:40:18 INFO FileSystemOptimizedCommitter: Nothing to setup as successful task attempt outputs are written directly
24/11/24 08:40:18 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
24/11/24 08:40:18 INFO DAGScheduler: Got job 1 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/11/24 08:40:18 INFO DAGScheduler: Final stage: ResultStage 1 (parquet at NativeMethodAccessorImpl.java:0)
24/11/24 08:40:18 INFO DAGScheduler: Parents of final stage: List()
24/11/24 08:40:18 INFO DAGScheduler: Missing parents: List()
24/11/24 08:40:18 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
24/11/24 08:40:18 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 261.7 KiB, free 912.0 MiB)
24/11/24 08:40:18 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 97.1 KiB, free 911.9 MiB)
24/11/24 08:40:18 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on ip-172-31-84-208.ec2.internal:35735 (size: 97.1 KiB, free: 912.2 MiB)
24/11/24 08:40:18 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1592
24/11/24 08:40:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/11/24 08:40:18 INFO YarnScheduler: Adding task set 1.0 with 1 tasks resource profile 0
24/11/24 08:40:18 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (ip-172-31-80-66.ec2.internal, executor 2, partition 0, PROCESS_LOCAL, 7220 bytes) 
24/11/24 08:40:18 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on ip-172-31-80-66.ec2.internal:42625 (size: 97.1 KiB, free: 4.8 GiB)
24/11/24 08:40:20 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2007 ms on ip-172-31-80-66.ec2.internal (executor 2) (1/1)
24/11/24 08:40:20 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
24/11/24 08:40:20 INFO DAGScheduler: ResultStage 1 (parquet at NativeMethodAccessorImpl.java:0) finished in 2.088 s
24/11/24 08:40:20 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
24/11/24 08:40:20 INFO YarnScheduler: Killing all running tasks in stage 1: Stage finished
24/11/24 08:40:20 INFO DAGScheduler: Job 1 finished: parquet at NativeMethodAccessorImpl.java:0, took 2.097448 s
24/11/24 08:40:20 INFO FileFormatWriter: Start to commit write Job 81e2deab-4dfc-4ad5-94a2-d584b61fb479.
24/11/24 08:40:20 INFO MultipartUploadOutputStream: close closed:false s3://p3-tet/raw/_SUCCESS
24/11/24 08:40:20 INFO FileFormatWriter: Write Job 81e2deab-4dfc-4ad5-94a2-d584b61fb479 committed. Elapsed time: 153 ms.
24/11/24 08:40:20 INFO FileFormatWriter: Finished processing stats for write job 81e2deab-4dfc-4ad5-94a2-d584b61fb479.
Datos extrados y guardados en S3 en la zona raw.
24/11/24 08:40:20 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/11/24 08:40:20 INFO SparkUI: Stopped Spark web UI at http://ip-172-31-84-208.ec2.internal:4040
24/11/24 08:40:20 INFO YarnClientSchedulerBackend: Interrupting monitor thread
24/11/24 08:40:20 INFO YarnClientSchedulerBackend: Shutting down all executors
24/11/24 08:40:20 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
24/11/24 08:40:20 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
24/11/24 08:40:20 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/11/24 08:40:20 INFO MemoryStore: MemoryStore cleared
24/11/24 08:40:20 INFO BlockManager: BlockManager stopped
24/11/24 08:40:20 INFO BlockManagerMaster: BlockManagerMaster stopped
24/11/24 08:40:20 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/11/24 08:40:20 INFO SparkContext: Successfully stopped SparkContext
24/11/24 08:40:21 INFO ShutdownHookManager: Shutdown hook called
24/11/24 08:40:21 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-28258b36-ca71-4ce6-9068-02798f64edee
24/11/24 08:40:21 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-41c22f5c-603d-4e3b-a29f-50e3e34464c0
24/11/24 08:40:21 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-28258b36-ca71-4ce6-9068-02798f64edee/pyspark-852aad1b-7351-44a0-ae1e-dfb6e60e01c2
24/11/24 09:06:04 INFO SparkContext: Running Spark version 3.4.1-amzn-1
24/11/24 09:06:04 INFO ResourceUtils: ==============================================================
24/11/24 09:06:04 INFO ResourceUtils: No custom resources configured for spark.driver.
24/11/24 09:06:04 INFO ResourceUtils: ==============================================================
24/11/24 09:06:04 INFO SparkContext: Submitted application: Extract_DB_to_S3
24/11/24 09:06:04 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 4, script: , vendor: , memory -> name: memory, amount: 9486, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/11/24 09:06:04 INFO ResourceProfile: Limiting resource is cpus at 4 tasks per executor
24/11/24 09:06:04 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/11/24 09:06:04 INFO SecurityManager: Changing view acls to: hadoop
24/11/24 09:06:04 INFO SecurityManager: Changing modify acls to: hadoop
24/11/24 09:06:04 INFO SecurityManager: Changing view acls groups to: 
24/11/24 09:06:04 INFO SecurityManager: Changing modify acls groups to: 
24/11/24 09:06:04 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: hadoop; groups with view permissions: EMPTY; users with modify permissions: hadoop; groups with modify permissions: EMPTY
24/11/24 09:06:05 INFO Utils: Successfully started service 'sparkDriver' on port 36229.
24/11/24 09:06:05 INFO SparkEnv: Registering MapOutputTracker
24/11/24 09:06:05 INFO SparkEnv: Registering BlockManagerMaster
24/11/24 09:06:05 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/11/24 09:06:05 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/11/24 09:06:05 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/11/24 09:06:05 INFO DiskBlockManager: Created local directory at /mnt/tmp/blockmgr-8c47de9b-1ba2-4ab7-93c8-55f284be1fa7
24/11/24 09:06:05 INFO MemoryStore: MemoryStore started with capacity 912.3 MiB
24/11/24 09:06:05 INFO SparkEnv: Registering OutputCommitCoordinator
24/11/24 09:06:05 INFO SubResultCacheManager: Sub-result caches are disabled.
24/11/24 09:06:05 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/11/24 09:06:05 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/11/24 09:06:05 INFO SparkContext: Added JAR /home/hadoop/mysql-connector-j-9.1.0.jar at spark://ip-172-31-84-208.ec2.internal:36229/jars/mysql-connector-j-9.1.0.jar with timestamp 1732439164732
24/11/24 09:06:05 INFO Utils: Using 50 preallocated executors (minExecutors: 0). Set spark.dynamicAllocation.preallocateExecutors to `false` disable executor preallocation.
24/11/24 09:06:05 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at ip-172-31-84-208.ec2.internal/172.31.84.208:8032
24/11/24 09:06:06 INFO Configuration: resource-types.xml not found
24/11/24 09:06:06 INFO ResourceUtils: Unable to find 'resource-types.xml'.
24/11/24 09:06:06 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (12288 MB per container)
24/11/24 09:06:06 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
24/11/24 09:06:06 INFO Client: Setting up container launch context for our AM
24/11/24 09:06:06 INFO Client: Setting up the launch environment for our AM container
24/11/24 09:06:06 INFO Client: Preparing resources for our AM container
24/11/24 09:06:06 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
24/11/24 09:06:09 INFO Client: Uploading resource file:/mnt/tmp/spark-20dd751e-627c-47b0-8a01-327aaf0b59e5/__spark_libs__8300396132081129192.zip -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0016/__spark_libs__8300396132081129192.zip
24/11/24 09:06:10 INFO Client: Uploading resource file:/home/hadoop/mysql-connector-j-9.1.0.jar -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0016/mysql-connector-j-9.1.0.jar
24/11/24 09:06:10 INFO Client: Uploading resource file:/etc/spark/conf.dist/hive-site.xml -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0016/hive-site.xml
24/11/24 09:06:10 INFO Client: Uploading resource file:/etc/hudi/conf.dist/hudi-defaults.conf -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0016/hudi-defaults.conf
24/11/24 09:06:10 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0016/pyspark.zip
24/11/24 09:06:10 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0016/py4j-0.10.9.7-src.zip
24/11/24 09:06:10 INFO Client: Uploading resource file:/mnt/tmp/spark-20dd751e-627c-47b0-8a01-327aaf0b59e5/__spark_conf__194883058097592305.zip -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0016/__spark_conf__.zip
24/11/24 09:06:10 INFO SecurityManager: Changing view acls to: hadoop
24/11/24 09:06:10 INFO SecurityManager: Changing modify acls to: hadoop
24/11/24 09:06:10 INFO SecurityManager: Changing view acls groups to: 
24/11/24 09:06:10 INFO SecurityManager: Changing modify acls groups to: 
24/11/24 09:06:10 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: hadoop; groups with view permissions: EMPTY; users with modify permissions: hadoop; groups with modify permissions: EMPTY
24/11/24 09:06:10 INFO Client: Submitting application application_1732429252608_0016 to ResourceManager
24/11/24 09:06:10 INFO YarnClientImpl: Submitted application application_1732429252608_0016
24/11/24 09:06:11 INFO Client: Application report for application_1732429252608_0016 (state: ACCEPTED)
24/11/24 09:06:11 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1732439170666
	 final status: UNDEFINED
	 tracking URL: http://ip-172-31-84-208.ec2.internal:20888/proxy/application_1732429252608_0016/
	 user: hadoop
24/11/24 09:06:12 INFO Client: Application report for application_1732429252608_0016 (state: ACCEPTED)
24/11/24 09:06:13 INFO Client: Application report for application_1732429252608_0016 (state: ACCEPTED)
24/11/24 09:06:14 INFO Client: Application report for application_1732429252608_0016 (state: ACCEPTED)
24/11/24 09:06:15 INFO Client: Application report for application_1732429252608_0016 (state: ACCEPTED)
24/11/24 09:06:15 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> ip-172-31-84-208.ec2.internal, PROXY_URI_BASES -> http://ip-172-31-84-208.ec2.internal:20888/proxy/application_1732429252608_0016), /proxy/application_1732429252608_0016
24/11/24 09:06:16 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
24/11/24 09:06:16 INFO Client: Application report for application_1732429252608_0016 (state: RUNNING)
24/11/24 09:06:16 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 172.31.80.66
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1732439170666
	 final status: UNDEFINED
	 tracking URL: http://ip-172-31-84-208.ec2.internal:20888/proxy/application_1732429252608_0016/
	 user: hadoop
24/11/24 09:06:16 INFO YarnClientSchedulerBackend: Application application_1732429252608_0016 has started running.
24/11/24 09:06:16 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43299.
24/11/24 09:06:16 INFO NettyBlockTransferService: Server created on ip-172-31-84-208.ec2.internal:43299
24/11/24 09:06:16 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/11/24 09:06:16 INFO BlockManager: external shuffle service port = 7337
24/11/24 09:06:16 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ip-172-31-84-208.ec2.internal, 43299, None)
24/11/24 09:06:16 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-84-208.ec2.internal:43299 with 912.3 MiB RAM, BlockManagerId(driver, ip-172-31-84-208.ec2.internal, 43299, None)
24/11/24 09:06:16 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ip-172-31-84-208.ec2.internal, 43299, None)
24/11/24 09:06:16 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ip-172-31-84-208.ec2.internal, 43299, None)
24/11/24 09:06:16 INFO SingleEventLogFileWriter: Logging events to hdfs:/var/log/spark/apps/application_1732429252608_0016.inprogress
24/11/24 09:06:17 INFO Utils: Using 50 preallocated executors (minExecutors: 0). Set spark.dynamicAllocation.preallocateExecutors to `false` disable executor preallocation.
24/11/24 09:06:17 INFO ServerInfo: Adding filter to /jobs: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:06:17 INFO ServerInfo: Adding filter to /jobs/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:06:17 INFO ServerInfo: Adding filter to /jobs/job: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:06:17 INFO ServerInfo: Adding filter to /jobs/job/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:06:17 INFO ServerInfo: Adding filter to /stages: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:06:17 INFO ServerInfo: Adding filter to /stages/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:06:17 INFO ServerInfo: Adding filter to /stages/stage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:06:17 INFO ServerInfo: Adding filter to /stages/stage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:06:17 INFO ServerInfo: Adding filter to /stages/pool: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:06:17 INFO ServerInfo: Adding filter to /stages/pool/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:06:17 INFO ServerInfo: Adding filter to /storage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:06:17 INFO ServerInfo: Adding filter to /storage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:06:17 INFO ServerInfo: Adding filter to /storage/rdd: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:06:17 INFO ServerInfo: Adding filter to /storage/rdd/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:06:17 INFO ServerInfo: Adding filter to /environment: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:06:17 INFO ServerInfo: Adding filter to /environment/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:06:17 INFO ServerInfo: Adding filter to /executors: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:06:17 INFO ServerInfo: Adding filter to /executors/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:06:17 INFO ServerInfo: Adding filter to /executors/threadDump: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:06:17 INFO ServerInfo: Adding filter to /executors/threadDump/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:06:17 INFO ServerInfo: Adding filter to /static: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:06:17 INFO ServerInfo: Adding filter to /: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:06:17 INFO ServerInfo: Adding filter to /api: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:06:17 INFO ServerInfo: Adding filter to /jobs/job/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:06:17 INFO ServerInfo: Adding filter to /stages/stage/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:06:17 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:06:17 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
24/11/24 09:06:17 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
24/11/24 09:06:17 INFO SharedState: Warehouse path is 'hdfs://ip-172-31-84-208.ec2.internal:8020/user/spark/warehouse'.
24/11/24 09:06:17 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:06:17 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:06:17 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:06:17 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:06:17 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:06:19 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.31.80.66:43522) with ID 2,  ResourceProfileId 0
24/11/24 09:06:19 INFO ExecutorMonitor: New executor 2 has registered (new total is 1)
24/11/24 09:06:20 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-80-66.ec2.internal:37049 with 4.8 GiB RAM, BlockManagerId(2, ip-172-31-80-66.ec2.internal, 37049, None)
Datos extrados de la base de datos:
24/11/24 09:06:21 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.31.90.72:55674) with ID 1,  ResourceProfileId 0
24/11/24 09:06:21 INFO ExecutorMonitor: New executor 1 has registered (new total is 2)
24/11/24 09:06:21 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-90-72.ec2.internal:41533 with 4.8 GiB RAM, BlockManagerId(1, ip-172-31-90-72.ec2.internal, 41533, None)
24/11/24 09:06:23 INFO CodeGenerator: Code generated in 357.477405 ms
24/11/24 09:06:23 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
24/11/24 09:06:23 INFO DAGScheduler: Got job 0 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/11/24 09:06:23 INFO DAGScheduler: Final stage: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0)
24/11/24 09:06:23 INFO DAGScheduler: Parents of final stage: List()
24/11/24 09:06:23 INFO DAGScheduler: Missing parents: List()
24/11/24 09:06:23 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
24/11/24 09:06:23 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 17.9 KiB, free 912.3 MiB)
24/11/24 09:06:23 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 912.3 MiB)
24/11/24 09:06:23 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ip-172-31-84-208.ec2.internal:43299 (size: 7.2 KiB, free: 912.3 MiB)
24/11/24 09:06:23 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1592
24/11/24 09:06:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/11/24 09:06:23 INFO YarnScheduler: Adding task set 0.0 with 1 tasks resource profile 0
24/11/24 09:06:23 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (ip-172-31-90-72.ec2.internal, executor 1, partition 0, PROCESS_LOCAL, 7220 bytes) 
24/11/24 09:06:23 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ip-172-31-90-72.ec2.internal:41533 (size: 7.2 KiB, free: 4.8 GiB)
24/11/24 09:06:25 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2210 ms on ip-172-31-90-72.ec2.internal (executor 1) (1/1)
24/11/24 09:06:25 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/11/24 09:06:25 INFO DAGScheduler: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0) finished in 2.447 s
24/11/24 09:06:25 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
24/11/24 09:06:25 INFO YarnScheduler: Killing all running tasks in stage 0: Stage finished
24/11/24 09:06:25 INFO DAGScheduler: Job 0 finished: showString at NativeMethodAccessorImpl.java:0, took 2.521217 s
24/11/24 09:06:25 INFO CodeGenerator: Code generated in 53.523375 ms
+-----------------+-------+------------------+----------------------------+-------------------+-------------------------+----------------+----+------------------+----+-------------+--------------+------+---------------+--------------------+----------+---------------------+------------+-----------------+------------------+-----------------+------------------+-------------------+
|fecha_reporte_web|id_caso|fecha_notificacion|codigo_divipola_departamento|nombre_departamento|codigo_divipola_municipio|nombre_municipio|edad|unidad_medida_edad|sexo|tipo_contagio|ubicacion_caso|estado|codigo_iso_pais|         nombre_pais|recuperado|fecha_inicio_sintomas|fecha_muerte|fecha_diagnostico|fecha_recuperacion|tipo_recuperacion|pertenencia_etnica|nombre_grupo_etnico|
+-----------------+-------+------------------+----------------------------+-------------------+-------------------------+----------------+----+------------------+----+-------------+--------------+------+---------------+--------------------+----------+---------------------+------------+-----------------+------------------+-----------------+------------------+-------------------+
| 6/3/2020 0:00:00|      1|  2/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  19|                 1|   F|    Importado|          Casa|  Leve|            380|              ITALIA|Recuperado|    27/2/2020 0:00:00|            | 6/3/2020 0:00:00| 13/3/2020 0:00:00|              PCR|                 6|                   |
| 9/3/2020 0:00:00|      2|  6/3/2020 0:00:00|                          76|              VALLE|                    76111|            BUGA|  34|                 1|   M|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     4/3/2020 0:00:00|            | 9/3/2020 0:00:00| 19/3/2020 0:00:00|              PCR|                 5|                   |
| 9/3/2020 0:00:00|      3|  7/3/2020 0:00:00|                           5|          ANTIOQUIA|                     5001|        MEDELLIN|  50|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|    29/2/2020 0:00:00|            | 9/3/2020 0:00:00| 15/3/2020 0:00:00|              PCR|                 6|                   |
|11/3/2020 0:00:00|      4|  9/3/2020 0:00:00|                           5|          ANTIOQUIA|                     5001|        MEDELLIN|  55|                 1|   M|  Relacionado|          Casa|  Leve|               |                    |Recuperado|     6/3/2020 0:00:00|            |11/3/2020 0:00:00| 26/3/2020 0:00:00|              PCR|                 6|                   |
|11/3/2020 0:00:00|      5|  9/3/2020 0:00:00|                           5|          ANTIOQUIA|                     5001|        MEDELLIN|  25|                 1|   M|  Relacionado|          Casa|  Leve|               |                    |Recuperado|     8/3/2020 0:00:00|            |11/3/2020 0:00:00| 23/3/2020 0:00:00|              PCR|                 6|                   |
|11/3/2020 0:00:00|      6| 10/3/2020 0:00:00|                           5|          ANTIOQUIA|                     5360|          ITAGUI|  27|                 1|   F|  Relacionado|          Casa|  Leve|               |                    |Recuperado|     6/3/2020 0:00:00|            |11/3/2020 0:00:00| 26/3/2020 0:00:00|              PCR|                 6|                   |
|11/3/2020 0:00:00|      7|  8/3/2020 0:00:00|                       13001|          CARTAGENA|                    13001|       CARTAGENA|  85|                 1|   F|    Importado|          Casa|  Leve|            840|ESTADOS UNIDOS DE...|Recuperado|     2/3/2020 0:00:00|            |11/3/2020 0:00:00| 17/3/2020 0:00:00|              PCR|                 6|                   |
|11/3/2020 0:00:00|      8|  9/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  22|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     6/3/2020 0:00:00|            |11/3/2020 0:00:00| 21/3/2020 0:00:00|              PCR|                 6|                   |
|11/3/2020 0:00:00|      9|  8/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  28|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     7/3/2020 0:00:00|            |11/3/2020 0:00:00| 23/3/2020 0:00:00|              PCR|                 6|                   |
|12/3/2020 0:00:00|     10| 12/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  36|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     6/3/2020 0:00:00|            |12/3/2020 0:00:00| 21/3/2020 0:00:00|              PCR|                 6|                   |
|12/3/2020 0:00:00|     11| 11/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  42|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     6/3/2020 0:00:00|            |12/3/2020 0:00:00| 31/3/2020 0:00:00|              PCR|                 6|                   |
|12/3/2020 0:00:00|     12| 10/3/2020 0:00:00|                          41|              HUILA|                    41001|           NEIVA|  74|                 1|   F|    Importado|          Casa|  Leve|            380|              ITALIA|Recuperado|     6/3/2020 0:00:00|            |12/3/2020 0:00:00|  9/4/2020 0:00:00|              PCR|                 6|                   |
|12/3/2020 0:00:00|     13| 10/3/2020 0:00:00|                          41|              HUILA|                    41001|           NEIVA|  68|                 1|   F|  Relacionado|          Casa|  Leve|               |                    |Recuperado|     6/3/2020 0:00:00|            |12/3/2020 0:00:00| 30/3/2020 0:00:00|              PCR|                 6|                   |
|13/3/2020 0:00:00|     14| 10/3/2020 0:00:00|                          76|              VALLE|                    76520|         PALMIRA|  48|                 1|   M|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     7/3/2020 0:00:00|            |13/3/2020 0:00:00| 21/3/2020 0:00:00|              PCR|                 5|                   |
|13/3/2020 0:00:00|     15| 13/3/2020 0:00:00|                          50|               META|                    50001|   VILLAVICENCIO|  30|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     9/3/2020 0:00:00|            |13/3/2020 0:00:00| 23/3/2020 0:00:00|              PCR|                 6|                   |
|13/3/2020 0:00:00|     16| 11/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  61|                 1|   F|    Importado|          Casa|  Leve|            380|              ITALIA|Recuperado|     8/3/2020 0:00:00|            |13/3/2020 0:00:00| 23/3/2020 0:00:00|              PCR|                 5|                   |
|14/3/2020 0:00:00|     17| 11/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  73|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|    28/2/2020 0:00:00|            |14/3/2020 0:00:00| 14/3/2020 0:00:00|              PCR|                 6|                   |
|14/3/2020 0:00:00|     18| 11/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  54|                 1|   M|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     7/3/2020 0:00:00|            |14/3/2020 0:00:00|  7/4/2020 0:00:00|           Tiempo|                 6|                   |
|14/3/2020 0:00:00|     19| 12/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  54|                 1|   F|  Relacionado|          Casa|  Leve|               |                    |Recuperado|     9/3/2020 0:00:00|            |14/3/2020 0:00:00| 24/3/2020 0:00:00|              PCR|                 6|                   |
|14/3/2020 0:00:00|     20| 11/3/2020 0:00:00|                           5|          ANTIOQUIA|                     5001|        MEDELLIN|  26|                 1|   F|  Relacionado|          Casa|  Leve|               |                    |Recuperado|     9/3/2020 0:00:00|            |14/3/2020 0:00:00| 24/3/2020 0:00:00|              PCR|                 6|                   |
+-----------------+-------+------------------+----------------------------+-------------------+-------------------------+----------------+----+------------------+----+-------------+--------------+------+---------------+--------------------+----------+---------------------+------------+-----------------+------------------+-----------------+------------------+-------------------+
only showing top 20 rows

24/11/24 09:06:26 INFO BlockManagerInfo: Removed broadcast_0_piece0 on ip-172-31-84-208.ec2.internal:43299 in memory (size: 7.2 KiB, free: 912.3 MiB)
24/11/24 09:06:26 INFO BlockManagerInfo: Removed broadcast_0_piece0 on ip-172-31-90-72.ec2.internal:41533 in memory (size: 7.2 KiB, free: 4.8 GiB)
24/11/24 09:06:27 INFO ClientConfigurationFactory: Set initial getObject socket timeout to 2000 ms.
24/11/24 09:06:28 INFO ParquetUtils: Using user defined output committer for Parquet: com.amazon.emr.committer.EmrOptimizedSparkSqlParquetOutputCommitter
24/11/24 09:06:28 INFO SQLConfCommitterProvider: Getting user defined output committer class com.amazon.emr.committer.EmrOptimizedSparkSqlParquetOutputCommitter
24/11/24 09:06:28 INFO EmrOptimizedParquetOutputCommitter: EMR Optimized Committer: ENABLED
24/11/24 09:06:28 INFO EmrOptimizedParquetOutputCommitter: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileSystemOptimizedCommitter
24/11/24 09:06:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
24/11/24 09:06:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: true
24/11/24 09:06:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
24/11/24 09:06:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: true
24/11/24 09:06:28 INFO SQLConfCommitterProvider: Using output committer class com.amazon.emr.committer.EmrOptimizedSparkSqlParquetOutputCommitter
24/11/24 09:06:28 INFO FileSystemOptimizedCommitter: Nothing to setup as successful task attempt outputs are written directly
24/11/24 09:06:28 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
24/11/24 09:06:28 INFO DAGScheduler: Got job 1 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/11/24 09:06:28 INFO DAGScheduler: Final stage: ResultStage 1 (parquet at NativeMethodAccessorImpl.java:0)
24/11/24 09:06:28 INFO DAGScheduler: Parents of final stage: List()
24/11/24 09:06:28 INFO DAGScheduler: Missing parents: List()
24/11/24 09:06:28 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
24/11/24 09:06:28 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 261.7 KiB, free 912.0 MiB)
24/11/24 09:06:28 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 97.1 KiB, free 911.9 MiB)
24/11/24 09:06:28 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on ip-172-31-84-208.ec2.internal:43299 (size: 97.1 KiB, free: 912.2 MiB)
24/11/24 09:06:28 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1592
24/11/24 09:06:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/11/24 09:06:28 INFO YarnScheduler: Adding task set 1.0 with 1 tasks resource profile 0
24/11/24 09:06:28 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (ip-172-31-80-66.ec2.internal, executor 2, partition 0, PROCESS_LOCAL, 7220 bytes) 
24/11/24 09:06:28 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on ip-172-31-80-66.ec2.internal:37049 (size: 97.1 KiB, free: 4.8 GiB)
24/11/24 09:06:32 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 4024 ms on ip-172-31-80-66.ec2.internal (executor 2) (1/1)
24/11/24 09:06:32 INFO DAGScheduler: ResultStage 1 (parquet at NativeMethodAccessorImpl.java:0) finished in 4.091 s
24/11/24 09:06:32 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
24/11/24 09:06:32 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
24/11/24 09:06:32 INFO YarnScheduler: Killing all running tasks in stage 1: Stage finished
24/11/24 09:06:32 INFO DAGScheduler: Job 1 finished: parquet at NativeMethodAccessorImpl.java:0, took 4.099035 s
24/11/24 09:06:32 INFO FileFormatWriter: Start to commit write Job 47ab0cda-3ec3-4a49-83ad-55fbede72f57.
24/11/24 09:06:32 INFO MultipartUploadOutputStream: close closed:false s3://p3-tet/raw/_SUCCESS
24/11/24 09:06:32 INFO FileFormatWriter: Write Job 47ab0cda-3ec3-4a49-83ad-55fbede72f57 committed. Elapsed time: 95 ms.
24/11/24 09:06:32 INFO FileFormatWriter: Finished processing stats for write job 47ab0cda-3ec3-4a49-83ad-55fbede72f57.
Datos extrados y guardados en S3 en la zona raw.
24/11/24 09:06:32 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/11/24 09:06:32 INFO SparkUI: Stopped Spark web UI at http://ip-172-31-84-208.ec2.internal:4040
24/11/24 09:06:32 INFO YarnClientSchedulerBackend: Interrupting monitor thread
24/11/24 09:06:32 INFO YarnClientSchedulerBackend: Shutting down all executors
24/11/24 09:06:32 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
24/11/24 09:06:32 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
24/11/24 09:06:32 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/11/24 09:06:32 INFO MemoryStore: MemoryStore cleared
24/11/24 09:06:32 INFO BlockManager: BlockManager stopped
24/11/24 09:06:32 INFO BlockManagerMaster: BlockManagerMaster stopped
24/11/24 09:06:32 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/11/24 09:06:32 INFO SparkContext: Successfully stopped SparkContext
24/11/24 09:06:33 INFO ShutdownHookManager: Shutdown hook called
24/11/24 09:06:33 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-68b04630-d6a2-4c82-be14-d6468024cd2e
24/11/24 09:06:33 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-20dd751e-627c-47b0-8a01-327aaf0b59e5
24/11/24 09:06:33 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-20dd751e-627c-47b0-8a01-327aaf0b59e5/pyspark-3ca5897c-f1be-431b-950c-73ca193a85f6
24/11/24 09:07:05 INFO SparkContext: Running Spark version 3.4.1-amzn-1
24/11/24 09:07:05 INFO ResourceUtils: ==============================================================
24/11/24 09:07:05 INFO ResourceUtils: No custom resources configured for spark.driver.
24/11/24 09:07:05 INFO ResourceUtils: ==============================================================
24/11/24 09:07:05 INFO SparkContext: Submitted application: Extract_DB_to_S3
24/11/24 09:07:05 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 4, script: , vendor: , memory -> name: memory, amount: 9486, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/11/24 09:07:05 INFO ResourceProfile: Limiting resource is cpus at 4 tasks per executor
24/11/24 09:07:05 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/11/24 09:07:05 INFO SecurityManager: Changing view acls to: hadoop
24/11/24 09:07:05 INFO SecurityManager: Changing modify acls to: hadoop
24/11/24 09:07:05 INFO SecurityManager: Changing view acls groups to: 
24/11/24 09:07:05 INFO SecurityManager: Changing modify acls groups to: 
24/11/24 09:07:05 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: hadoop; groups with view permissions: EMPTY; users with modify permissions: hadoop; groups with modify permissions: EMPTY
24/11/24 09:07:05 INFO Utils: Successfully started service 'sparkDriver' on port 38171.
24/11/24 09:07:05 INFO SparkEnv: Registering MapOutputTracker
24/11/24 09:07:05 INFO SparkEnv: Registering BlockManagerMaster
24/11/24 09:07:05 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/11/24 09:07:05 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/11/24 09:07:05 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/11/24 09:07:05 INFO DiskBlockManager: Created local directory at /mnt/tmp/blockmgr-7d06079c-da8e-499d-839f-38ea681f1a94
24/11/24 09:07:05 INFO MemoryStore: MemoryStore started with capacity 912.3 MiB
24/11/24 09:07:05 INFO SparkEnv: Registering OutputCommitCoordinator
24/11/24 09:07:05 INFO SubResultCacheManager: Sub-result caches are disabled.
24/11/24 09:07:06 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/11/24 09:07:06 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/11/24 09:07:06 INFO SparkContext: Added JAR /home/hadoop/mysql-connector-j-9.1.0.jar at spark://ip-172-31-84-208.ec2.internal:38171/jars/mysql-connector-j-9.1.0.jar with timestamp 1732439225255
24/11/24 09:07:06 INFO Utils: Using 50 preallocated executors (minExecutors: 0). Set spark.dynamicAllocation.preallocateExecutors to `false` disable executor preallocation.
24/11/24 09:07:06 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at ip-172-31-84-208.ec2.internal/172.31.84.208:8032
24/11/24 09:07:06 INFO Configuration: resource-types.xml not found
24/11/24 09:07:06 INFO ResourceUtils: Unable to find 'resource-types.xml'.
24/11/24 09:07:06 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (12288 MB per container)
24/11/24 09:07:06 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
24/11/24 09:07:06 INFO Client: Setting up container launch context for our AM
24/11/24 09:07:06 INFO Client: Setting up the launch environment for our AM container
24/11/24 09:07:06 INFO Client: Preparing resources for our AM container
24/11/24 09:07:07 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
24/11/24 09:07:09 INFO Client: Uploading resource file:/mnt/tmp/spark-82057d80-2691-441e-a95c-1ae36a906d4c/__spark_libs__299959414629454848.zip -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0017/__spark_libs__299959414629454848.zip
24/11/24 09:07:10 INFO Client: Uploading resource file:/home/hadoop/mysql-connector-j-9.1.0.jar -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0017/mysql-connector-j-9.1.0.jar
24/11/24 09:07:10 INFO Client: Uploading resource file:/etc/spark/conf.dist/hive-site.xml -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0017/hive-site.xml
24/11/24 09:07:10 INFO Client: Uploading resource file:/etc/hudi/conf.dist/hudi-defaults.conf -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0017/hudi-defaults.conf
24/11/24 09:07:10 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0017/pyspark.zip
24/11/24 09:07:10 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0017/py4j-0.10.9.7-src.zip
24/11/24 09:07:11 INFO Client: Uploading resource file:/mnt/tmp/spark-82057d80-2691-441e-a95c-1ae36a906d4c/__spark_conf__6514596056193261002.zip -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0017/__spark_conf__.zip
24/11/24 09:07:11 INFO SecurityManager: Changing view acls to: hadoop
24/11/24 09:07:11 INFO SecurityManager: Changing modify acls to: hadoop
24/11/24 09:07:11 INFO SecurityManager: Changing view acls groups to: 
24/11/24 09:07:11 INFO SecurityManager: Changing modify acls groups to: 
24/11/24 09:07:11 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: hadoop; groups with view permissions: EMPTY; users with modify permissions: hadoop; groups with modify permissions: EMPTY
24/11/24 09:07:11 INFO Client: Submitting application application_1732429252608_0017 to ResourceManager
24/11/24 09:07:11 INFO YarnClientImpl: Submitted application application_1732429252608_0017
24/11/24 09:07:12 INFO Client: Application report for application_1732429252608_0017 (state: ACCEPTED)
24/11/24 09:07:12 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1732439231158
	 final status: UNDEFINED
	 tracking URL: http://ip-172-31-84-208.ec2.internal:20888/proxy/application_1732429252608_0017/
	 user: hadoop
24/11/24 09:07:13 INFO Client: Application report for application_1732429252608_0017 (state: ACCEPTED)
24/11/24 09:07:14 INFO Client: Application report for application_1732429252608_0017 (state: ACCEPTED)
24/11/24 09:07:15 INFO Client: Application report for application_1732429252608_0017 (state: ACCEPTED)
24/11/24 09:07:16 INFO Client: Application report for application_1732429252608_0017 (state: ACCEPTED)
24/11/24 09:07:16 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> ip-172-31-84-208.ec2.internal, PROXY_URI_BASES -> http://ip-172-31-84-208.ec2.internal:20888/proxy/application_1732429252608_0017), /proxy/application_1732429252608_0017
24/11/24 09:07:16 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
24/11/24 09:07:17 INFO Client: Application report for application_1732429252608_0017 (state: RUNNING)
24/11/24 09:07:17 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 172.31.80.66
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1732439231158
	 final status: UNDEFINED
	 tracking URL: http://ip-172-31-84-208.ec2.internal:20888/proxy/application_1732429252608_0017/
	 user: hadoop
24/11/24 09:07:17 INFO YarnClientSchedulerBackend: Application application_1732429252608_0017 has started running.
24/11/24 09:07:17 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37365.
24/11/24 09:07:17 INFO NettyBlockTransferService: Server created on ip-172-31-84-208.ec2.internal:37365
24/11/24 09:07:17 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/11/24 09:07:17 INFO BlockManager: external shuffle service port = 7337
24/11/24 09:07:17 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ip-172-31-84-208.ec2.internal, 37365, None)
24/11/24 09:07:17 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-84-208.ec2.internal:37365 with 912.3 MiB RAM, BlockManagerId(driver, ip-172-31-84-208.ec2.internal, 37365, None)
24/11/24 09:07:17 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ip-172-31-84-208.ec2.internal, 37365, None)
24/11/24 09:07:17 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ip-172-31-84-208.ec2.internal, 37365, None)
24/11/24 09:07:17 INFO SingleEventLogFileWriter: Logging events to hdfs:/var/log/spark/apps/application_1732429252608_0017.inprogress
24/11/24 09:07:17 INFO Utils: Using 50 preallocated executors (minExecutors: 0). Set spark.dynamicAllocation.preallocateExecutors to `false` disable executor preallocation.
24/11/24 09:07:17 INFO ServerInfo: Adding filter to /jobs: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:07:17 INFO ServerInfo: Adding filter to /jobs/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:07:17 INFO ServerInfo: Adding filter to /jobs/job: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:07:17 INFO ServerInfo: Adding filter to /jobs/job/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:07:17 INFO ServerInfo: Adding filter to /stages: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:07:17 INFO ServerInfo: Adding filter to /stages/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:07:17 INFO ServerInfo: Adding filter to /stages/stage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:07:17 INFO ServerInfo: Adding filter to /stages/stage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:07:17 INFO ServerInfo: Adding filter to /stages/pool: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:07:17 INFO ServerInfo: Adding filter to /stages/pool/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:07:17 INFO ServerInfo: Adding filter to /storage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:07:17 INFO ServerInfo: Adding filter to /storage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:07:17 INFO ServerInfo: Adding filter to /storage/rdd: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:07:17 INFO ServerInfo: Adding filter to /storage/rdd/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:07:17 INFO ServerInfo: Adding filter to /environment: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:07:17 INFO ServerInfo: Adding filter to /environment/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:07:17 INFO ServerInfo: Adding filter to /executors: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:07:17 INFO ServerInfo: Adding filter to /executors/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:07:17 INFO ServerInfo: Adding filter to /executors/threadDump: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:07:17 INFO ServerInfo: Adding filter to /executors/threadDump/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:07:17 INFO ServerInfo: Adding filter to /static: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:07:17 INFO ServerInfo: Adding filter to /: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:07:17 INFO ServerInfo: Adding filter to /api: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:07:17 INFO ServerInfo: Adding filter to /jobs/job/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:07:17 INFO ServerInfo: Adding filter to /stages/stage/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:07:17 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:07:17 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
24/11/24 09:07:17 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
24/11/24 09:07:17 INFO SharedState: Warehouse path is 'hdfs://ip-172-31-84-208.ec2.internal:8020/user/spark/warehouse'.
24/11/24 09:07:17 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:07:17 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:07:17 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:07:17 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:07:17 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:07:20 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.31.80.66:50718) with ID 2,  ResourceProfileId 0
24/11/24 09:07:20 INFO ExecutorMonitor: New executor 2 has registered (new total is 1)
24/11/24 09:07:20 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-80-66.ec2.internal:42231 with 4.8 GiB RAM, BlockManagerId(2, ip-172-31-80-66.ec2.internal, 42231, None)
Datos extrados de la base de datos:
24/11/24 09:07:22 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.31.90.72:40228) with ID 1,  ResourceProfileId 0
24/11/24 09:07:22 INFO ExecutorMonitor: New executor 1 has registered (new total is 2)
24/11/24 09:07:22 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-90-72.ec2.internal:37599 with 4.8 GiB RAM, BlockManagerId(1, ip-172-31-90-72.ec2.internal, 37599, None)
24/11/24 09:07:23 INFO CodeGenerator: Code generated in 253.662209 ms
24/11/24 09:07:23 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
24/11/24 09:07:23 INFO DAGScheduler: Got job 0 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/11/24 09:07:23 INFO DAGScheduler: Final stage: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0)
24/11/24 09:07:23 INFO DAGScheduler: Parents of final stage: List()
24/11/24 09:07:23 INFO DAGScheduler: Missing parents: List()
24/11/24 09:07:23 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
24/11/24 09:07:23 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 17.9 KiB, free 912.3 MiB)
24/11/24 09:07:23 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 912.3 MiB)
24/11/24 09:07:23 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ip-172-31-84-208.ec2.internal:37365 (size: 7.2 KiB, free: 912.3 MiB)
24/11/24 09:07:23 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1592
24/11/24 09:07:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/11/24 09:07:23 INFO YarnScheduler: Adding task set 0.0 with 1 tasks resource profile 0
24/11/24 09:07:23 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (ip-172-31-80-66.ec2.internal, executor 2, partition 0, PROCESS_LOCAL, 7220 bytes) 
24/11/24 09:07:23 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ip-172-31-80-66.ec2.internal:42231 (size: 7.2 KiB, free: 4.8 GiB)
24/11/24 09:07:25 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2049 ms on ip-172-31-80-66.ec2.internal (executor 2) (1/1)
24/11/24 09:07:25 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/11/24 09:07:25 INFO DAGScheduler: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0) finished in 2.237 s
24/11/24 09:07:25 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
24/11/24 09:07:25 INFO YarnScheduler: Killing all running tasks in stage 0: Stage finished
24/11/24 09:07:25 INFO DAGScheduler: Job 0 finished: showString at NativeMethodAccessorImpl.java:0, took 2.293797 s
24/11/24 09:07:25 INFO CodeGenerator: Code generated in 59.348219 ms
+-----------------+-------+------------------+----------------------------+-------------------+-------------------------+----------------+----+------------------+----+-------------+--------------+------+---------------+--------------------+----------+---------------------+------------+-----------------+------------------+-----------------+------------------+-------------------+
|fecha_reporte_web|id_caso|fecha_notificacion|codigo_divipola_departamento|nombre_departamento|codigo_divipola_municipio|nombre_municipio|edad|unidad_medida_edad|sexo|tipo_contagio|ubicacion_caso|estado|codigo_iso_pais|         nombre_pais|recuperado|fecha_inicio_sintomas|fecha_muerte|fecha_diagnostico|fecha_recuperacion|tipo_recuperacion|pertenencia_etnica|nombre_grupo_etnico|
+-----------------+-------+------------------+----------------------------+-------------------+-------------------------+----------------+----+------------------+----+-------------+--------------+------+---------------+--------------------+----------+---------------------+------------+-----------------+------------------+-----------------+------------------+-------------------+
| 6/3/2020 0:00:00|      1|  2/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  19|                 1|   F|    Importado|          Casa|  Leve|            380|              ITALIA|Recuperado|    27/2/2020 0:00:00|            | 6/3/2020 0:00:00| 13/3/2020 0:00:00|              PCR|                 6|                   |
| 9/3/2020 0:00:00|      2|  6/3/2020 0:00:00|                          76|              VALLE|                    76111|            BUGA|  34|                 1|   M|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     4/3/2020 0:00:00|            | 9/3/2020 0:00:00| 19/3/2020 0:00:00|              PCR|                 5|                   |
| 9/3/2020 0:00:00|      3|  7/3/2020 0:00:00|                           5|          ANTIOQUIA|                     5001|        MEDELLIN|  50|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|    29/2/2020 0:00:00|            | 9/3/2020 0:00:00| 15/3/2020 0:00:00|              PCR|                 6|                   |
|11/3/2020 0:00:00|      4|  9/3/2020 0:00:00|                           5|          ANTIOQUIA|                     5001|        MEDELLIN|  55|                 1|   M|  Relacionado|          Casa|  Leve|               |                    |Recuperado|     6/3/2020 0:00:00|            |11/3/2020 0:00:00| 26/3/2020 0:00:00|              PCR|                 6|                   |
|11/3/2020 0:00:00|      5|  9/3/2020 0:00:00|                           5|          ANTIOQUIA|                     5001|        MEDELLIN|  25|                 1|   M|  Relacionado|          Casa|  Leve|               |                    |Recuperado|     8/3/2020 0:00:00|            |11/3/2020 0:00:00| 23/3/2020 0:00:00|              PCR|                 6|                   |
|11/3/2020 0:00:00|      6| 10/3/2020 0:00:00|                           5|          ANTIOQUIA|                     5360|          ITAGUI|  27|                 1|   F|  Relacionado|          Casa|  Leve|               |                    |Recuperado|     6/3/2020 0:00:00|            |11/3/2020 0:00:00| 26/3/2020 0:00:00|              PCR|                 6|                   |
|11/3/2020 0:00:00|      7|  8/3/2020 0:00:00|                       13001|          CARTAGENA|                    13001|       CARTAGENA|  85|                 1|   F|    Importado|          Casa|  Leve|            840|ESTADOS UNIDOS DE...|Recuperado|     2/3/2020 0:00:00|            |11/3/2020 0:00:00| 17/3/2020 0:00:00|              PCR|                 6|                   |
|11/3/2020 0:00:00|      8|  9/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  22|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     6/3/2020 0:00:00|            |11/3/2020 0:00:00| 21/3/2020 0:00:00|              PCR|                 6|                   |
|11/3/2020 0:00:00|      9|  8/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  28|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     7/3/2020 0:00:00|            |11/3/2020 0:00:00| 23/3/2020 0:00:00|              PCR|                 6|                   |
|12/3/2020 0:00:00|     10| 12/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  36|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     6/3/2020 0:00:00|            |12/3/2020 0:00:00| 21/3/2020 0:00:00|              PCR|                 6|                   |
|12/3/2020 0:00:00|     11| 11/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  42|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     6/3/2020 0:00:00|            |12/3/2020 0:00:00| 31/3/2020 0:00:00|              PCR|                 6|                   |
|12/3/2020 0:00:00|     12| 10/3/2020 0:00:00|                          41|              HUILA|                    41001|           NEIVA|  74|                 1|   F|    Importado|          Casa|  Leve|            380|              ITALIA|Recuperado|     6/3/2020 0:00:00|            |12/3/2020 0:00:00|  9/4/2020 0:00:00|              PCR|                 6|                   |
|12/3/2020 0:00:00|     13| 10/3/2020 0:00:00|                          41|              HUILA|                    41001|           NEIVA|  68|                 1|   F|  Relacionado|          Casa|  Leve|               |                    |Recuperado|     6/3/2020 0:00:00|            |12/3/2020 0:00:00| 30/3/2020 0:00:00|              PCR|                 6|                   |
|13/3/2020 0:00:00|     14| 10/3/2020 0:00:00|                          76|              VALLE|                    76520|         PALMIRA|  48|                 1|   M|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     7/3/2020 0:00:00|            |13/3/2020 0:00:00| 21/3/2020 0:00:00|              PCR|                 5|                   |
|13/3/2020 0:00:00|     15| 13/3/2020 0:00:00|                          50|               META|                    50001|   VILLAVICENCIO|  30|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     9/3/2020 0:00:00|            |13/3/2020 0:00:00| 23/3/2020 0:00:00|              PCR|                 6|                   |
|13/3/2020 0:00:00|     16| 11/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  61|                 1|   F|    Importado|          Casa|  Leve|            380|              ITALIA|Recuperado|     8/3/2020 0:00:00|            |13/3/2020 0:00:00| 23/3/2020 0:00:00|              PCR|                 5|                   |
|14/3/2020 0:00:00|     17| 11/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  73|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|    28/2/2020 0:00:00|            |14/3/2020 0:00:00| 14/3/2020 0:00:00|              PCR|                 6|                   |
|14/3/2020 0:00:00|     18| 11/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  54|                 1|   M|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     7/3/2020 0:00:00|            |14/3/2020 0:00:00|  7/4/2020 0:00:00|           Tiempo|                 6|                   |
|14/3/2020 0:00:00|     19| 12/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  54|                 1|   F|  Relacionado|          Casa|  Leve|               |                    |Recuperado|     9/3/2020 0:00:00|            |14/3/2020 0:00:00| 24/3/2020 0:00:00|              PCR|                 6|                   |
|14/3/2020 0:00:00|     20| 11/3/2020 0:00:00|                           5|          ANTIOQUIA|                     5001|        MEDELLIN|  26|                 1|   F|  Relacionado|          Casa|  Leve|               |                    |Recuperado|     9/3/2020 0:00:00|            |14/3/2020 0:00:00| 24/3/2020 0:00:00|              PCR|                 6|                   |
+-----------------+-------+------------------+----------------------------+-------------------+-------------------------+----------------+----+------------------+----+-------------+--------------+------+---------------+--------------------+----------+---------------------+------------+-----------------+------------------+-----------------+------------------+-------------------+
only showing top 20 rows

24/11/24 09:07:26 INFO BlockManagerInfo: Removed broadcast_0_piece0 on ip-172-31-84-208.ec2.internal:37365 in memory (size: 7.2 KiB, free: 912.3 MiB)
24/11/24 09:07:26 INFO BlockManagerInfo: Removed broadcast_0_piece0 on ip-172-31-80-66.ec2.internal:42231 in memory (size: 7.2 KiB, free: 4.8 GiB)
24/11/24 09:07:27 INFO ClientConfigurationFactory: Set initial getObject socket timeout to 2000 ms.
24/11/24 09:07:28 INFO ParquetUtils: Using user defined output committer for Parquet: com.amazon.emr.committer.EmrOptimizedSparkSqlParquetOutputCommitter
24/11/24 09:07:28 INFO SQLConfCommitterProvider: Getting user defined output committer class com.amazon.emr.committer.EmrOptimizedSparkSqlParquetOutputCommitter
24/11/24 09:07:28 INFO EmrOptimizedParquetOutputCommitter: EMR Optimized Committer: ENABLED
24/11/24 09:07:28 INFO EmrOptimizedParquetOutputCommitter: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileSystemOptimizedCommitter
24/11/24 09:07:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
24/11/24 09:07:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: true
24/11/24 09:07:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
24/11/24 09:07:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: true
24/11/24 09:07:28 INFO SQLConfCommitterProvider: Using output committer class com.amazon.emr.committer.EmrOptimizedSparkSqlParquetOutputCommitter
24/11/24 09:07:28 INFO FileSystemOptimizedCommitter: Nothing to setup as successful task attempt outputs are written directly
24/11/24 09:07:28 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
24/11/24 09:07:28 INFO DAGScheduler: Got job 1 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/11/24 09:07:28 INFO DAGScheduler: Final stage: ResultStage 1 (parquet at NativeMethodAccessorImpl.java:0)
24/11/24 09:07:28 INFO DAGScheduler: Parents of final stage: List()
24/11/24 09:07:28 INFO DAGScheduler: Missing parents: List()
24/11/24 09:07:28 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
24/11/24 09:07:28 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 261.7 KiB, free 912.0 MiB)
24/11/24 09:07:28 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 97.2 KiB, free 911.9 MiB)
24/11/24 09:07:28 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on ip-172-31-84-208.ec2.internal:37365 (size: 97.2 KiB, free: 912.2 MiB)
24/11/24 09:07:28 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1592
24/11/24 09:07:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/11/24 09:07:28 INFO YarnScheduler: Adding task set 1.0 with 1 tasks resource profile 0
24/11/24 09:07:28 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (ip-172-31-80-66.ec2.internal, executor 2, partition 0, PROCESS_LOCAL, 7220 bytes) 
24/11/24 09:07:28 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on ip-172-31-80-66.ec2.internal:42231 (size: 97.2 KiB, free: 4.8 GiB)
24/11/24 09:07:30 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2042 ms on ip-172-31-80-66.ec2.internal (executor 2) (1/1)
24/11/24 09:07:30 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
24/11/24 09:07:30 INFO DAGScheduler: ResultStage 1 (parquet at NativeMethodAccessorImpl.java:0) finished in 2.119 s
24/11/24 09:07:30 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
24/11/24 09:07:30 INFO YarnScheduler: Killing all running tasks in stage 1: Stage finished
24/11/24 09:07:30 INFO DAGScheduler: Job 1 finished: parquet at NativeMethodAccessorImpl.java:0, took 2.127175 s
24/11/24 09:07:30 INFO FileFormatWriter: Start to commit write Job 752f2cfe-5ca1-4e07-8179-83cce0f6f957.
24/11/24 09:07:30 INFO MultipartUploadOutputStream: close closed:false s3://p3-tet/raw/_SUCCESS
24/11/24 09:07:30 INFO FileFormatWriter: Write Job 752f2cfe-5ca1-4e07-8179-83cce0f6f957 committed. Elapsed time: 81 ms.
24/11/24 09:07:30 INFO FileFormatWriter: Finished processing stats for write job 752f2cfe-5ca1-4e07-8179-83cce0f6f957.
Datos extrados y guardados en S3 en la zona raw.
24/11/24 09:07:30 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/11/24 09:07:30 INFO SparkUI: Stopped Spark web UI at http://ip-172-31-84-208.ec2.internal:4040
24/11/24 09:07:30 INFO YarnClientSchedulerBackend: Interrupting monitor thread
24/11/24 09:07:30 INFO YarnClientSchedulerBackend: Shutting down all executors
24/11/24 09:07:30 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
24/11/24 09:07:30 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
24/11/24 09:07:30 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/11/24 09:07:30 INFO MemoryStore: MemoryStore cleared
24/11/24 09:07:30 INFO BlockManager: BlockManager stopped
24/11/24 09:07:30 INFO BlockManagerMaster: BlockManagerMaster stopped
24/11/24 09:07:30 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/11/24 09:07:30 INFO SparkContext: Successfully stopped SparkContext
24/11/24 09:07:31 INFO ShutdownHookManager: Shutdown hook called
24/11/24 09:07:31 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-56552723-3674-4d7a-a4c2-2256f6d8c5b8
24/11/24 09:07:31 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-82057d80-2691-441e-a95c-1ae36a906d4c/pyspark-7bd99bd0-0c1a-4956-92d9-d85eb75dc4ee
24/11/24 09:07:31 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-82057d80-2691-441e-a95c-1ae36a906d4c
24/11/24 09:08:04 INFO SparkContext: Running Spark version 3.4.1-amzn-1
24/11/24 09:08:04 INFO ResourceUtils: ==============================================================
24/11/24 09:08:04 INFO ResourceUtils: No custom resources configured for spark.driver.
24/11/24 09:08:04 INFO ResourceUtils: ==============================================================
24/11/24 09:08:04 INFO SparkContext: Submitted application: Extract_DB_to_S3
24/11/24 09:08:04 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 4, script: , vendor: , memory -> name: memory, amount: 9486, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/11/24 09:08:04 INFO ResourceProfile: Limiting resource is cpus at 4 tasks per executor
24/11/24 09:08:04 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/11/24 09:08:04 INFO SecurityManager: Changing view acls to: hadoop
24/11/24 09:08:04 INFO SecurityManager: Changing modify acls to: hadoop
24/11/24 09:08:04 INFO SecurityManager: Changing view acls groups to: 
24/11/24 09:08:04 INFO SecurityManager: Changing modify acls groups to: 
24/11/24 09:08:04 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: hadoop; groups with view permissions: EMPTY; users with modify permissions: hadoop; groups with modify permissions: EMPTY
24/11/24 09:08:04 INFO Utils: Successfully started service 'sparkDriver' on port 44173.
24/11/24 09:08:04 INFO SparkEnv: Registering MapOutputTracker
24/11/24 09:08:04 INFO SparkEnv: Registering BlockManagerMaster
24/11/24 09:08:04 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/11/24 09:08:04 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/11/24 09:08:04 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/11/24 09:08:04 INFO DiskBlockManager: Created local directory at /mnt/tmp/blockmgr-05a2887e-fb84-4445-9a69-59cec90cd0d5
24/11/24 09:08:04 INFO MemoryStore: MemoryStore started with capacity 912.3 MiB
24/11/24 09:08:04 INFO SparkEnv: Registering OutputCommitCoordinator
24/11/24 09:08:04 INFO SubResultCacheManager: Sub-result caches are disabled.
24/11/24 09:08:05 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/11/24 09:08:05 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/11/24 09:08:05 INFO SparkContext: Added JAR /home/hadoop/mysql-connector-j-9.1.0.jar at spark://ip-172-31-84-208.ec2.internal:44173/jars/mysql-connector-j-9.1.0.jar with timestamp 1732439284285
24/11/24 09:08:05 INFO Utils: Using 50 preallocated executors (minExecutors: 0). Set spark.dynamicAllocation.preallocateExecutors to `false` disable executor preallocation.
24/11/24 09:08:05 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at ip-172-31-84-208.ec2.internal/172.31.84.208:8032
24/11/24 09:08:06 INFO Configuration: resource-types.xml not found
24/11/24 09:08:06 INFO ResourceUtils: Unable to find 'resource-types.xml'.
24/11/24 09:08:06 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (12288 MB per container)
24/11/24 09:08:06 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
24/11/24 09:08:06 INFO Client: Setting up container launch context for our AM
24/11/24 09:08:06 INFO Client: Setting up the launch environment for our AM container
24/11/24 09:08:06 INFO Client: Preparing resources for our AM container
24/11/24 09:08:06 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
24/11/24 09:08:09 INFO Client: Uploading resource file:/mnt/tmp/spark-69803c41-c61b-4833-b4e7-3db9e73d315e/__spark_libs__7681829013052208340.zip -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0018/__spark_libs__7681829013052208340.zip
24/11/24 09:08:09 INFO Client: Uploading resource file:/home/hadoop/mysql-connector-j-9.1.0.jar -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0018/mysql-connector-j-9.1.0.jar
24/11/24 09:08:09 INFO Client: Uploading resource file:/etc/spark/conf.dist/hive-site.xml -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0018/hive-site.xml
24/11/24 09:08:09 INFO Client: Uploading resource file:/etc/hudi/conf.dist/hudi-defaults.conf -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0018/hudi-defaults.conf
24/11/24 09:08:10 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0018/pyspark.zip
24/11/24 09:08:10 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0018/py4j-0.10.9.7-src.zip
24/11/24 09:08:10 INFO Client: Uploading resource file:/mnt/tmp/spark-69803c41-c61b-4833-b4e7-3db9e73d315e/__spark_conf__8219284738813611739.zip -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0018/__spark_conf__.zip
24/11/24 09:08:10 INFO SecurityManager: Changing view acls to: hadoop
24/11/24 09:08:10 INFO SecurityManager: Changing modify acls to: hadoop
24/11/24 09:08:10 INFO SecurityManager: Changing view acls groups to: 
24/11/24 09:08:10 INFO SecurityManager: Changing modify acls groups to: 
24/11/24 09:08:10 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: hadoop; groups with view permissions: EMPTY; users with modify permissions: hadoop; groups with modify permissions: EMPTY
24/11/24 09:08:10 INFO Client: Submitting application application_1732429252608_0018 to ResourceManager
24/11/24 09:08:10 INFO YarnClientImpl: Submitted application application_1732429252608_0018
24/11/24 09:08:11 INFO Client: Application report for application_1732429252608_0018 (state: ACCEPTED)
24/11/24 09:08:11 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1732439290638
	 final status: UNDEFINED
	 tracking URL: http://ip-172-31-84-208.ec2.internal:20888/proxy/application_1732429252608_0018/
	 user: hadoop
24/11/24 09:08:12 INFO Client: Application report for application_1732429252608_0018 (state: ACCEPTED)
24/11/24 09:08:13 INFO Client: Application report for application_1732429252608_0018 (state: ACCEPTED)
24/11/24 09:08:14 INFO Client: Application report for application_1732429252608_0018 (state: ACCEPTED)
24/11/24 09:08:15 INFO Client: Application report for application_1732429252608_0018 (state: ACCEPTED)
24/11/24 09:08:15 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> ip-172-31-84-208.ec2.internal, PROXY_URI_BASES -> http://ip-172-31-84-208.ec2.internal:20888/proxy/application_1732429252608_0018), /proxy/application_1732429252608_0018
24/11/24 09:08:16 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
24/11/24 09:08:16 INFO Client: Application report for application_1732429252608_0018 (state: RUNNING)
24/11/24 09:08:16 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 172.31.80.66
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1732439290638
	 final status: UNDEFINED
	 tracking URL: http://ip-172-31-84-208.ec2.internal:20888/proxy/application_1732429252608_0018/
	 user: hadoop
24/11/24 09:08:16 INFO YarnClientSchedulerBackend: Application application_1732429252608_0018 has started running.
24/11/24 09:08:16 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37901.
24/11/24 09:08:16 INFO NettyBlockTransferService: Server created on ip-172-31-84-208.ec2.internal:37901
24/11/24 09:08:16 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/11/24 09:08:16 INFO BlockManager: external shuffle service port = 7337
24/11/24 09:08:16 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ip-172-31-84-208.ec2.internal, 37901, None)
24/11/24 09:08:16 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-84-208.ec2.internal:37901 with 912.3 MiB RAM, BlockManagerId(driver, ip-172-31-84-208.ec2.internal, 37901, None)
24/11/24 09:08:16 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ip-172-31-84-208.ec2.internal, 37901, None)
24/11/24 09:08:16 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ip-172-31-84-208.ec2.internal, 37901, None)
24/11/24 09:08:16 INFO SingleEventLogFileWriter: Logging events to hdfs:/var/log/spark/apps/application_1732429252608_0018.inprogress
24/11/24 09:08:17 INFO Utils: Using 50 preallocated executors (minExecutors: 0). Set spark.dynamicAllocation.preallocateExecutors to `false` disable executor preallocation.
24/11/24 09:08:17 INFO ServerInfo: Adding filter to /jobs: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:08:17 INFO ServerInfo: Adding filter to /jobs/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:08:17 INFO ServerInfo: Adding filter to /jobs/job: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:08:17 INFO ServerInfo: Adding filter to /jobs/job/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:08:17 INFO ServerInfo: Adding filter to /stages: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:08:17 INFO ServerInfo: Adding filter to /stages/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:08:17 INFO ServerInfo: Adding filter to /stages/stage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:08:17 INFO ServerInfo: Adding filter to /stages/stage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:08:17 INFO ServerInfo: Adding filter to /stages/pool: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:08:17 INFO ServerInfo: Adding filter to /stages/pool/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:08:17 INFO ServerInfo: Adding filter to /storage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:08:17 INFO ServerInfo: Adding filter to /storage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:08:17 INFO ServerInfo: Adding filter to /storage/rdd: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:08:17 INFO ServerInfo: Adding filter to /storage/rdd/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:08:17 INFO ServerInfo: Adding filter to /environment: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:08:17 INFO ServerInfo: Adding filter to /environment/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:08:17 INFO ServerInfo: Adding filter to /executors: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:08:17 INFO ServerInfo: Adding filter to /executors/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:08:17 INFO ServerInfo: Adding filter to /executors/threadDump: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:08:17 INFO ServerInfo: Adding filter to /executors/threadDump/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:08:17 INFO ServerInfo: Adding filter to /static: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:08:17 INFO ServerInfo: Adding filter to /: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:08:17 INFO ServerInfo: Adding filter to /api: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:08:17 INFO ServerInfo: Adding filter to /jobs/job/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:08:17 INFO ServerInfo: Adding filter to /stages/stage/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:08:17 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:08:17 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
24/11/24 09:08:17 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
24/11/24 09:08:17 INFO SharedState: Warehouse path is 'hdfs://ip-172-31-84-208.ec2.internal:8020/user/spark/warehouse'.
24/11/24 09:08:17 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:08:17 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:08:17 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:08:17 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:08:17 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:08:19 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.31.80.66:46818) with ID 1,  ResourceProfileId 0
24/11/24 09:08:19 INFO ExecutorMonitor: New executor 1 has registered (new total is 1)
24/11/24 09:08:19 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-80-66.ec2.internal:36335 with 4.8 GiB RAM, BlockManagerId(1, ip-172-31-80-66.ec2.internal, 36335, None)
Datos extrados de la base de datos:
24/11/24 09:08:21 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.31.90.72:34032) with ID 2,  ResourceProfileId 0
24/11/24 09:08:21 INFO ExecutorMonitor: New executor 2 has registered (new total is 2)
24/11/24 09:08:21 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-90-72.ec2.internal:42335 with 4.8 GiB RAM, BlockManagerId(2, ip-172-31-90-72.ec2.internal, 42335, None)
24/11/24 09:08:22 INFO CodeGenerator: Code generated in 292.912086 ms
24/11/24 09:08:22 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
24/11/24 09:08:22 INFO DAGScheduler: Got job 0 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/11/24 09:08:22 INFO DAGScheduler: Final stage: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0)
24/11/24 09:08:22 INFO DAGScheduler: Parents of final stage: List()
24/11/24 09:08:22 INFO DAGScheduler: Missing parents: List()
24/11/24 09:08:22 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
24/11/24 09:08:22 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 17.9 KiB, free 912.3 MiB)
24/11/24 09:08:22 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 912.3 MiB)
24/11/24 09:08:22 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ip-172-31-84-208.ec2.internal:37901 (size: 7.2 KiB, free: 912.3 MiB)
24/11/24 09:08:22 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1592
24/11/24 09:08:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/11/24 09:08:23 INFO YarnScheduler: Adding task set 0.0 with 1 tasks resource profile 0
24/11/24 09:08:23 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (ip-172-31-90-72.ec2.internal, executor 2, partition 0, PROCESS_LOCAL, 7220 bytes) 
24/11/24 09:08:23 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ip-172-31-90-72.ec2.internal:42335 (size: 7.2 KiB, free: 4.8 GiB)
24/11/24 09:08:24 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1880 ms on ip-172-31-90-72.ec2.internal (executor 2) (1/1)
24/11/24 09:08:24 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/11/24 09:08:24 INFO DAGScheduler: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0) finished in 2.107 s
24/11/24 09:08:24 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
24/11/24 09:08:24 INFO YarnScheduler: Killing all running tasks in stage 0: Stage finished
24/11/24 09:08:24 INFO DAGScheduler: Job 0 finished: showString at NativeMethodAccessorImpl.java:0, took 2.167853 s
24/11/24 09:08:25 INFO CodeGenerator: Code generated in 55.572888 ms
+-----------------+-------+------------------+----------------------------+-------------------+-------------------------+----------------+----+------------------+----+-------------+--------------+------+---------------+--------------------+----------+---------------------+------------+-----------------+------------------+-----------------+------------------+-------------------+
|fecha_reporte_web|id_caso|fecha_notificacion|codigo_divipola_departamento|nombre_departamento|codigo_divipola_municipio|nombre_municipio|edad|unidad_medida_edad|sexo|tipo_contagio|ubicacion_caso|estado|codigo_iso_pais|         nombre_pais|recuperado|fecha_inicio_sintomas|fecha_muerte|fecha_diagnostico|fecha_recuperacion|tipo_recuperacion|pertenencia_etnica|nombre_grupo_etnico|
+-----------------+-------+------------------+----------------------------+-------------------+-------------------------+----------------+----+------------------+----+-------------+--------------+------+---------------+--------------------+----------+---------------------+------------+-----------------+------------------+-----------------+------------------+-------------------+
| 6/3/2020 0:00:00|      1|  2/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  19|                 1|   F|    Importado|          Casa|  Leve|            380|              ITALIA|Recuperado|    27/2/2020 0:00:00|            | 6/3/2020 0:00:00| 13/3/2020 0:00:00|              PCR|                 6|                   |
| 9/3/2020 0:00:00|      2|  6/3/2020 0:00:00|                          76|              VALLE|                    76111|            BUGA|  34|                 1|   M|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     4/3/2020 0:00:00|            | 9/3/2020 0:00:00| 19/3/2020 0:00:00|              PCR|                 5|                   |
| 9/3/2020 0:00:00|      3|  7/3/2020 0:00:00|                           5|          ANTIOQUIA|                     5001|        MEDELLIN|  50|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|    29/2/2020 0:00:00|            | 9/3/2020 0:00:00| 15/3/2020 0:00:00|              PCR|                 6|                   |
|11/3/2020 0:00:00|      4|  9/3/2020 0:00:00|                           5|          ANTIOQUIA|                     5001|        MEDELLIN|  55|                 1|   M|  Relacionado|          Casa|  Leve|               |                    |Recuperado|     6/3/2020 0:00:00|            |11/3/2020 0:00:00| 26/3/2020 0:00:00|              PCR|                 6|                   |
|11/3/2020 0:00:00|      5|  9/3/2020 0:00:00|                           5|          ANTIOQUIA|                     5001|        MEDELLIN|  25|                 1|   M|  Relacionado|          Casa|  Leve|               |                    |Recuperado|     8/3/2020 0:00:00|            |11/3/2020 0:00:00| 23/3/2020 0:00:00|              PCR|                 6|                   |
|11/3/2020 0:00:00|      6| 10/3/2020 0:00:00|                           5|          ANTIOQUIA|                     5360|          ITAGUI|  27|                 1|   F|  Relacionado|          Casa|  Leve|               |                    |Recuperado|     6/3/2020 0:00:00|            |11/3/2020 0:00:00| 26/3/2020 0:00:00|              PCR|                 6|                   |
|11/3/2020 0:00:00|      7|  8/3/2020 0:00:00|                       13001|          CARTAGENA|                    13001|       CARTAGENA|  85|                 1|   F|    Importado|          Casa|  Leve|            840|ESTADOS UNIDOS DE...|Recuperado|     2/3/2020 0:00:00|            |11/3/2020 0:00:00| 17/3/2020 0:00:00|              PCR|                 6|                   |
|11/3/2020 0:00:00|      8|  9/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  22|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     6/3/2020 0:00:00|            |11/3/2020 0:00:00| 21/3/2020 0:00:00|              PCR|                 6|                   |
|11/3/2020 0:00:00|      9|  8/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  28|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     7/3/2020 0:00:00|            |11/3/2020 0:00:00| 23/3/2020 0:00:00|              PCR|                 6|                   |
|12/3/2020 0:00:00|     10| 12/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  36|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     6/3/2020 0:00:00|            |12/3/2020 0:00:00| 21/3/2020 0:00:00|              PCR|                 6|                   |
|12/3/2020 0:00:00|     11| 11/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  42|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     6/3/2020 0:00:00|            |12/3/2020 0:00:00| 31/3/2020 0:00:00|              PCR|                 6|                   |
|12/3/2020 0:00:00|     12| 10/3/2020 0:00:00|                          41|              HUILA|                    41001|           NEIVA|  74|                 1|   F|    Importado|          Casa|  Leve|            380|              ITALIA|Recuperado|     6/3/2020 0:00:00|            |12/3/2020 0:00:00|  9/4/2020 0:00:00|              PCR|                 6|                   |
|12/3/2020 0:00:00|     13| 10/3/2020 0:00:00|                          41|              HUILA|                    41001|           NEIVA|  68|                 1|   F|  Relacionado|          Casa|  Leve|               |                    |Recuperado|     6/3/2020 0:00:00|            |12/3/2020 0:00:00| 30/3/2020 0:00:00|              PCR|                 6|                   |
|13/3/2020 0:00:00|     14| 10/3/2020 0:00:00|                          76|              VALLE|                    76520|         PALMIRA|  48|                 1|   M|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     7/3/2020 0:00:00|            |13/3/2020 0:00:00| 21/3/2020 0:00:00|              PCR|                 5|                   |
|13/3/2020 0:00:00|     15| 13/3/2020 0:00:00|                          50|               META|                    50001|   VILLAVICENCIO|  30|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     9/3/2020 0:00:00|            |13/3/2020 0:00:00| 23/3/2020 0:00:00|              PCR|                 6|                   |
|13/3/2020 0:00:00|     16| 11/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  61|                 1|   F|    Importado|          Casa|  Leve|            380|              ITALIA|Recuperado|     8/3/2020 0:00:00|            |13/3/2020 0:00:00| 23/3/2020 0:00:00|              PCR|                 5|                   |
|14/3/2020 0:00:00|     17| 11/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  73|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|    28/2/2020 0:00:00|            |14/3/2020 0:00:00| 14/3/2020 0:00:00|              PCR|                 6|                   |
|14/3/2020 0:00:00|     18| 11/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  54|                 1|   M|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     7/3/2020 0:00:00|            |14/3/2020 0:00:00|  7/4/2020 0:00:00|           Tiempo|                 6|                   |
|14/3/2020 0:00:00|     19| 12/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  54|                 1|   F|  Relacionado|          Casa|  Leve|               |                    |Recuperado|     9/3/2020 0:00:00|            |14/3/2020 0:00:00| 24/3/2020 0:00:00|              PCR|                 6|                   |
|14/3/2020 0:00:00|     20| 11/3/2020 0:00:00|                           5|          ANTIOQUIA|                     5001|        MEDELLIN|  26|                 1|   F|  Relacionado|          Casa|  Leve|               |                    |Recuperado|     9/3/2020 0:00:00|            |14/3/2020 0:00:00| 24/3/2020 0:00:00|              PCR|                 6|                   |
+-----------------+-------+------------------+----------------------------+-------------------+-------------------------+----------------+----+------------------+----+-------------+--------------+------+---------------+--------------------+----------+---------------------+------------+-----------------+------------------+-----------------+------------------+-------------------+
only showing top 20 rows

24/11/24 09:08:25 INFO ClientConfigurationFactory: Set initial getObject socket timeout to 2000 ms.
24/11/24 09:08:26 INFO BlockManagerInfo: Removed broadcast_0_piece0 on ip-172-31-84-208.ec2.internal:37901 in memory (size: 7.2 KiB, free: 912.3 MiB)
24/11/24 09:08:26 INFO BlockManagerInfo: Removed broadcast_0_piece0 on ip-172-31-90-72.ec2.internal:42335 in memory (size: 7.2 KiB, free: 4.8 GiB)
24/11/24 09:08:27 INFO ParquetUtils: Using user defined output committer for Parquet: com.amazon.emr.committer.EmrOptimizedSparkSqlParquetOutputCommitter
24/11/24 09:08:27 INFO SQLConfCommitterProvider: Getting user defined output committer class com.amazon.emr.committer.EmrOptimizedSparkSqlParquetOutputCommitter
24/11/24 09:08:27 INFO EmrOptimizedParquetOutputCommitter: EMR Optimized Committer: ENABLED
24/11/24 09:08:27 INFO EmrOptimizedParquetOutputCommitter: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileSystemOptimizedCommitter
24/11/24 09:08:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
24/11/24 09:08:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: true
24/11/24 09:08:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
24/11/24 09:08:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: true
24/11/24 09:08:27 INFO SQLConfCommitterProvider: Using output committer class com.amazon.emr.committer.EmrOptimizedSparkSqlParquetOutputCommitter
24/11/24 09:08:27 INFO FileSystemOptimizedCommitter: Nothing to setup as successful task attempt outputs are written directly
24/11/24 09:08:27 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
24/11/24 09:08:27 INFO DAGScheduler: Got job 1 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/11/24 09:08:27 INFO DAGScheduler: Final stage: ResultStage 1 (parquet at NativeMethodAccessorImpl.java:0)
24/11/24 09:08:27 INFO DAGScheduler: Parents of final stage: List()
24/11/24 09:08:27 INFO DAGScheduler: Missing parents: List()
24/11/24 09:08:27 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
24/11/24 09:08:27 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 261.7 KiB, free 912.0 MiB)
24/11/24 09:08:27 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 97.1 KiB, free 911.9 MiB)
24/11/24 09:08:27 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on ip-172-31-84-208.ec2.internal:37901 (size: 97.1 KiB, free: 912.2 MiB)
24/11/24 09:08:27 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1592
24/11/24 09:08:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/11/24 09:08:27 INFO YarnScheduler: Adding task set 1.0 with 1 tasks resource profile 0
24/11/24 09:08:27 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (ip-172-31-80-66.ec2.internal, executor 1, partition 0, PROCESS_LOCAL, 7220 bytes) 
24/11/24 09:08:27 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on ip-172-31-80-66.ec2.internal:36335 (size: 97.1 KiB, free: 4.8 GiB)
24/11/24 09:08:31 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 3858 ms on ip-172-31-80-66.ec2.internal (executor 1) (1/1)
24/11/24 09:08:31 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
24/11/24 09:08:31 INFO DAGScheduler: ResultStage 1 (parquet at NativeMethodAccessorImpl.java:0) finished in 3.934 s
24/11/24 09:08:31 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
24/11/24 09:08:31 INFO YarnScheduler: Killing all running tasks in stage 1: Stage finished
24/11/24 09:08:31 INFO DAGScheduler: Job 1 finished: parquet at NativeMethodAccessorImpl.java:0, took 3.941835 s
24/11/24 09:08:31 INFO FileFormatWriter: Start to commit write Job 558963cf-b4ea-43e4-80a1-826037e55f07.
24/11/24 09:08:31 INFO MultipartUploadOutputStream: close closed:false s3://p3-tet/raw/_SUCCESS
24/11/24 09:08:31 INFO FileFormatWriter: Write Job 558963cf-b4ea-43e4-80a1-826037e55f07 committed. Elapsed time: 85 ms.
24/11/24 09:08:31 INFO FileFormatWriter: Finished processing stats for write job 558963cf-b4ea-43e4-80a1-826037e55f07.
Datos extrados y guardados en S3 en la zona raw.
24/11/24 09:08:31 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/11/24 09:08:31 INFO SparkUI: Stopped Spark web UI at http://ip-172-31-84-208.ec2.internal:4040
24/11/24 09:08:31 INFO YarnClientSchedulerBackend: Interrupting monitor thread
24/11/24 09:08:31 INFO YarnClientSchedulerBackend: Shutting down all executors
24/11/24 09:08:31 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
24/11/24 09:08:31 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
24/11/24 09:08:31 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/11/24 09:08:31 INFO MemoryStore: MemoryStore cleared
24/11/24 09:08:31 INFO BlockManager: BlockManager stopped
24/11/24 09:08:31 INFO BlockManagerMaster: BlockManagerMaster stopped
24/11/24 09:08:31 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/11/24 09:08:31 INFO SparkContext: Successfully stopped SparkContext
24/11/24 09:08:31 INFO ShutdownHookManager: Shutdown hook called
24/11/24 09:08:31 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-410159fd-c5ac-456a-89d8-c7a743e3aea5
24/11/24 09:08:31 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-69803c41-c61b-4833-b4e7-3db9e73d315e
24/11/24 09:08:31 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-69803c41-c61b-4833-b4e7-3db9e73d315e/pyspark-dc049dbc-3d4e-46e8-bae7-9d638e367288
24/11/24 09:09:04 INFO SparkContext: Running Spark version 3.4.1-amzn-1
24/11/24 09:09:04 INFO ResourceUtils: ==============================================================
24/11/24 09:09:04 INFO ResourceUtils: No custom resources configured for spark.driver.
24/11/24 09:09:04 INFO ResourceUtils: ==============================================================
24/11/24 09:09:04 INFO SparkContext: Submitted application: Extract_DB_to_S3
24/11/24 09:09:04 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 4, script: , vendor: , memory -> name: memory, amount: 9486, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/11/24 09:09:04 INFO ResourceProfile: Limiting resource is cpus at 4 tasks per executor
24/11/24 09:09:04 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/11/24 09:09:04 INFO SecurityManager: Changing view acls to: hadoop
24/11/24 09:09:04 INFO SecurityManager: Changing modify acls to: hadoop
24/11/24 09:09:04 INFO SecurityManager: Changing view acls groups to: 
24/11/24 09:09:04 INFO SecurityManager: Changing modify acls groups to: 
24/11/24 09:09:04 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: hadoop; groups with view permissions: EMPTY; users with modify permissions: hadoop; groups with modify permissions: EMPTY
24/11/24 09:09:04 INFO Utils: Successfully started service 'sparkDriver' on port 42843.
24/11/24 09:09:04 INFO SparkEnv: Registering MapOutputTracker
24/11/24 09:09:04 INFO SparkEnv: Registering BlockManagerMaster
24/11/24 09:09:04 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/11/24 09:09:04 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/11/24 09:09:04 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/11/24 09:09:04 INFO DiskBlockManager: Created local directory at /mnt/tmp/blockmgr-fed9bcd7-92cc-497d-98bc-f3e9b016ba37
24/11/24 09:09:04 INFO MemoryStore: MemoryStore started with capacity 912.3 MiB
24/11/24 09:09:04 INFO SparkEnv: Registering OutputCommitCoordinator
24/11/24 09:09:04 INFO SubResultCacheManager: Sub-result caches are disabled.
24/11/24 09:09:05 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/11/24 09:09:05 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/11/24 09:09:05 INFO SparkContext: Added JAR /home/hadoop/mysql-connector-j-9.1.0.jar at spark://ip-172-31-84-208.ec2.internal:42843/jars/mysql-connector-j-9.1.0.jar with timestamp 1732439344202
24/11/24 09:09:05 INFO Utils: Using 50 preallocated executors (minExecutors: 0). Set spark.dynamicAllocation.preallocateExecutors to `false` disable executor preallocation.
24/11/24 09:09:05 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at ip-172-31-84-208.ec2.internal/172.31.84.208:8032
24/11/24 09:09:05 INFO Configuration: resource-types.xml not found
24/11/24 09:09:05 INFO ResourceUtils: Unable to find 'resource-types.xml'.
24/11/24 09:09:05 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (12288 MB per container)
24/11/24 09:09:05 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
24/11/24 09:09:05 INFO Client: Setting up container launch context for our AM
24/11/24 09:09:05 INFO Client: Setting up the launch environment for our AM container
24/11/24 09:09:05 INFO Client: Preparing resources for our AM container
24/11/24 09:09:05 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
24/11/24 09:09:08 INFO Client: Uploading resource file:/mnt/tmp/spark-44fc0bdd-d2c7-496e-b191-7d3c3c0c0ff4/__spark_libs__8504674546294192757.zip -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0019/__spark_libs__8504674546294192757.zip
24/11/24 09:09:09 INFO Client: Uploading resource file:/home/hadoop/mysql-connector-j-9.1.0.jar -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0019/mysql-connector-j-9.1.0.jar
24/11/24 09:09:09 INFO Client: Uploading resource file:/etc/spark/conf.dist/hive-site.xml -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0019/hive-site.xml
24/11/24 09:09:09 INFO Client: Uploading resource file:/etc/hudi/conf.dist/hudi-defaults.conf -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0019/hudi-defaults.conf
24/11/24 09:09:09 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0019/pyspark.zip
24/11/24 09:09:09 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0019/py4j-0.10.9.7-src.zip
24/11/24 09:09:09 INFO Client: Uploading resource file:/mnt/tmp/spark-44fc0bdd-d2c7-496e-b191-7d3c3c0c0ff4/__spark_conf__8882529399577361771.zip -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0019/__spark_conf__.zip
24/11/24 09:09:09 INFO SecurityManager: Changing view acls to: hadoop
24/11/24 09:09:09 INFO SecurityManager: Changing modify acls to: hadoop
24/11/24 09:09:09 INFO SecurityManager: Changing view acls groups to: 
24/11/24 09:09:09 INFO SecurityManager: Changing modify acls groups to: 
24/11/24 09:09:09 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: hadoop; groups with view permissions: EMPTY; users with modify permissions: hadoop; groups with modify permissions: EMPTY
24/11/24 09:09:10 INFO Client: Submitting application application_1732429252608_0019 to ResourceManager
24/11/24 09:09:10 INFO YarnClientImpl: Submitted application application_1732429252608_0019
24/11/24 09:09:11 INFO Client: Application report for application_1732429252608_0019 (state: ACCEPTED)
24/11/24 09:09:11 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1732439350035
	 final status: UNDEFINED
	 tracking URL: http://ip-172-31-84-208.ec2.internal:20888/proxy/application_1732429252608_0019/
	 user: hadoop
24/11/24 09:09:12 INFO Client: Application report for application_1732429252608_0019 (state: ACCEPTED)
24/11/24 09:09:13 INFO Client: Application report for application_1732429252608_0019 (state: ACCEPTED)
24/11/24 09:09:14 INFO Client: Application report for application_1732429252608_0019 (state: ACCEPTED)
24/11/24 09:09:15 INFO Client: Application report for application_1732429252608_0019 (state: RUNNING)
24/11/24 09:09:15 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 172.31.80.66
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1732439350035
	 final status: UNDEFINED
	 tracking URL: http://ip-172-31-84-208.ec2.internal:20888/proxy/application_1732429252608_0019/
	 user: hadoop
24/11/24 09:09:15 INFO YarnClientSchedulerBackend: Application application_1732429252608_0019 has started running.
24/11/24 09:09:15 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38097.
24/11/24 09:09:15 INFO NettyBlockTransferService: Server created on ip-172-31-84-208.ec2.internal:38097
24/11/24 09:09:15 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/11/24 09:09:15 INFO BlockManager: external shuffle service port = 7337
24/11/24 09:09:15 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ip-172-31-84-208.ec2.internal, 38097, None)
24/11/24 09:09:15 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-84-208.ec2.internal:38097 with 912.3 MiB RAM, BlockManagerId(driver, ip-172-31-84-208.ec2.internal, 38097, None)
24/11/24 09:09:15 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ip-172-31-84-208.ec2.internal, 38097, None)
24/11/24 09:09:15 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ip-172-31-84-208.ec2.internal, 38097, None)
24/11/24 09:09:15 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> ip-172-31-84-208.ec2.internal, PROXY_URI_BASES -> http://ip-172-31-84-208.ec2.internal:20888/proxy/application_1732429252608_0019), /proxy/application_1732429252608_0019
24/11/24 09:09:15 INFO SingleEventLogFileWriter: Logging events to hdfs:/var/log/spark/apps/application_1732429252608_0019.inprogress
24/11/24 09:09:15 INFO Utils: Using 50 preallocated executors (minExecutors: 0). Set spark.dynamicAllocation.preallocateExecutors to `false` disable executor preallocation.
24/11/24 09:09:15 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to request executors before the AM has registered!
24/11/24 09:09:15 INFO ServerInfo: Adding filter to /jobs: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:09:15 INFO ServerInfo: Adding filter to /jobs/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:09:15 INFO ServerInfo: Adding filter to /jobs/job: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:09:15 INFO ServerInfo: Adding filter to /jobs/job/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:09:15 INFO ServerInfo: Adding filter to /stages: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:09:15 INFO ServerInfo: Adding filter to /stages/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:09:15 INFO ServerInfo: Adding filter to /stages/stage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:09:15 INFO ServerInfo: Adding filter to /stages/stage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:09:15 INFO ServerInfo: Adding filter to /stages/pool: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:09:15 INFO ServerInfo: Adding filter to /stages/pool/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:09:15 INFO ServerInfo: Adding filter to /storage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:09:15 INFO ServerInfo: Adding filter to /storage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:09:15 INFO ServerInfo: Adding filter to /storage/rdd: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:09:15 INFO ServerInfo: Adding filter to /storage/rdd/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:09:15 INFO ServerInfo: Adding filter to /environment: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:09:15 INFO ServerInfo: Adding filter to /environment/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:09:15 INFO ServerInfo: Adding filter to /executors: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:09:15 INFO ServerInfo: Adding filter to /executors/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:09:15 INFO ServerInfo: Adding filter to /executors/threadDump: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:09:15 INFO ServerInfo: Adding filter to /executors/threadDump/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:09:15 INFO ServerInfo: Adding filter to /static: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:09:15 INFO ServerInfo: Adding filter to /: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:09:15 INFO ServerInfo: Adding filter to /api: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:09:15 INFO ServerInfo: Adding filter to /jobs/job/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:09:15 INFO ServerInfo: Adding filter to /stages/stage/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:09:15 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:09:15 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
24/11/24 09:09:15 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
24/11/24 09:09:15 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
24/11/24 09:09:15 INFO SharedState: Warehouse path is 'hdfs://ip-172-31-84-208.ec2.internal:8020/user/spark/warehouse'.
24/11/24 09:09:15 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:09:15 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:09:15 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:09:15 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:09:15 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:09:19 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.31.80.66:47418) with ID 1,  ResourceProfileId 0
24/11/24 09:09:19 INFO ExecutorMonitor: New executor 1 has registered (new total is 1)
24/11/24 09:09:19 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-80-66.ec2.internal:33251 with 4.8 GiB RAM, BlockManagerId(1, ip-172-31-80-66.ec2.internal, 33251, None)
Datos extrados de la base de datos:
24/11/24 09:09:21 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.31.90.72:52618) with ID 2,  ResourceProfileId 0
24/11/24 09:09:21 INFO ExecutorMonitor: New executor 2 has registered (new total is 2)
24/11/24 09:09:21 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-90-72.ec2.internal:34619 with 4.8 GiB RAM, BlockManagerId(2, ip-172-31-90-72.ec2.internal, 34619, None)
24/11/24 09:09:21 INFO CodeGenerator: Code generated in 249.976091 ms
24/11/24 09:09:21 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
24/11/24 09:09:21 INFO DAGScheduler: Got job 0 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/11/24 09:09:21 INFO DAGScheduler: Final stage: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0)
24/11/24 09:09:21 INFO DAGScheduler: Parents of final stage: List()
24/11/24 09:09:21 INFO DAGScheduler: Missing parents: List()
24/11/24 09:09:21 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
24/11/24 09:09:21 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 17.9 KiB, free 912.3 MiB)
24/11/24 09:09:21 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 912.3 MiB)
24/11/24 09:09:21 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ip-172-31-84-208.ec2.internal:38097 (size: 7.2 KiB, free: 912.3 MiB)
24/11/24 09:09:21 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1592
24/11/24 09:09:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/11/24 09:09:21 INFO YarnScheduler: Adding task set 0.0 with 1 tasks resource profile 0
24/11/24 09:09:21 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (ip-172-31-80-66.ec2.internal, executor 1, partition 0, PROCESS_LOCAL, 7220 bytes) 
24/11/24 09:09:22 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ip-172-31-80-66.ec2.internal:33251 (size: 7.2 KiB, free: 4.8 GiB)
24/11/24 09:09:24 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2131 ms on ip-172-31-80-66.ec2.internal (executor 1) (1/1)
24/11/24 09:09:24 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/11/24 09:09:24 INFO DAGScheduler: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0) finished in 2.386 s
24/11/24 09:09:24 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
24/11/24 09:09:24 INFO YarnScheduler: Killing all running tasks in stage 0: Stage finished
24/11/24 09:09:24 INFO DAGScheduler: Job 0 finished: showString at NativeMethodAccessorImpl.java:0, took 2.450804 s
24/11/24 09:09:24 INFO CodeGenerator: Code generated in 92.395242 ms
+-----------------+-------+------------------+----------------------------+-------------------+-------------------------+----------------+----+------------------+----+-------------+--------------+------+---------------+--------------------+----------+---------------------+------------+-----------------+------------------+-----------------+------------------+-------------------+
|fecha_reporte_web|id_caso|fecha_notificacion|codigo_divipola_departamento|nombre_departamento|codigo_divipola_municipio|nombre_municipio|edad|unidad_medida_edad|sexo|tipo_contagio|ubicacion_caso|estado|codigo_iso_pais|         nombre_pais|recuperado|fecha_inicio_sintomas|fecha_muerte|fecha_diagnostico|fecha_recuperacion|tipo_recuperacion|pertenencia_etnica|nombre_grupo_etnico|
+-----------------+-------+------------------+----------------------------+-------------------+-------------------------+----------------+----+------------------+----+-------------+--------------+------+---------------+--------------------+----------+---------------------+------------+-----------------+------------------+-----------------+------------------+-------------------+
| 6/3/2020 0:00:00|      1|  2/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  19|                 1|   F|    Importado|          Casa|  Leve|            380|              ITALIA|Recuperado|    27/2/2020 0:00:00|            | 6/3/2020 0:00:00| 13/3/2020 0:00:00|              PCR|                 6|                   |
| 9/3/2020 0:00:00|      2|  6/3/2020 0:00:00|                          76|              VALLE|                    76111|            BUGA|  34|                 1|   M|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     4/3/2020 0:00:00|            | 9/3/2020 0:00:00| 19/3/2020 0:00:00|              PCR|                 5|                   |
| 9/3/2020 0:00:00|      3|  7/3/2020 0:00:00|                           5|          ANTIOQUIA|                     5001|        MEDELLIN|  50|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|    29/2/2020 0:00:00|            | 9/3/2020 0:00:00| 15/3/2020 0:00:00|              PCR|                 6|                   |
|11/3/2020 0:00:00|      4|  9/3/2020 0:00:00|                           5|          ANTIOQUIA|                     5001|        MEDELLIN|  55|                 1|   M|  Relacionado|          Casa|  Leve|               |                    |Recuperado|     6/3/2020 0:00:00|            |11/3/2020 0:00:00| 26/3/2020 0:00:00|              PCR|                 6|                   |
|11/3/2020 0:00:00|      5|  9/3/2020 0:00:00|                           5|          ANTIOQUIA|                     5001|        MEDELLIN|  25|                 1|   M|  Relacionado|          Casa|  Leve|               |                    |Recuperado|     8/3/2020 0:00:00|            |11/3/2020 0:00:00| 23/3/2020 0:00:00|              PCR|                 6|                   |
|11/3/2020 0:00:00|      6| 10/3/2020 0:00:00|                           5|          ANTIOQUIA|                     5360|          ITAGUI|  27|                 1|   F|  Relacionado|          Casa|  Leve|               |                    |Recuperado|     6/3/2020 0:00:00|            |11/3/2020 0:00:00| 26/3/2020 0:00:00|              PCR|                 6|                   |
|11/3/2020 0:00:00|      7|  8/3/2020 0:00:00|                       13001|          CARTAGENA|                    13001|       CARTAGENA|  85|                 1|   F|    Importado|          Casa|  Leve|            840|ESTADOS UNIDOS DE...|Recuperado|     2/3/2020 0:00:00|            |11/3/2020 0:00:00| 17/3/2020 0:00:00|              PCR|                 6|                   |
|11/3/2020 0:00:00|      8|  9/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  22|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     6/3/2020 0:00:00|            |11/3/2020 0:00:00| 21/3/2020 0:00:00|              PCR|                 6|                   |
|11/3/2020 0:00:00|      9|  8/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  28|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     7/3/2020 0:00:00|            |11/3/2020 0:00:00| 23/3/2020 0:00:00|              PCR|                 6|                   |
|12/3/2020 0:00:00|     10| 12/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  36|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     6/3/2020 0:00:00|            |12/3/2020 0:00:00| 21/3/2020 0:00:00|              PCR|                 6|                   |
|12/3/2020 0:00:00|     11| 11/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  42|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     6/3/2020 0:00:00|            |12/3/2020 0:00:00| 31/3/2020 0:00:00|              PCR|                 6|                   |
|12/3/2020 0:00:00|     12| 10/3/2020 0:00:00|                          41|              HUILA|                    41001|           NEIVA|  74|                 1|   F|    Importado|          Casa|  Leve|            380|              ITALIA|Recuperado|     6/3/2020 0:00:00|            |12/3/2020 0:00:00|  9/4/2020 0:00:00|              PCR|                 6|                   |
|12/3/2020 0:00:00|     13| 10/3/2020 0:00:00|                          41|              HUILA|                    41001|           NEIVA|  68|                 1|   F|  Relacionado|          Casa|  Leve|               |                    |Recuperado|     6/3/2020 0:00:00|            |12/3/2020 0:00:00| 30/3/2020 0:00:00|              PCR|                 6|                   |
|13/3/2020 0:00:00|     14| 10/3/2020 0:00:00|                          76|              VALLE|                    76520|         PALMIRA|  48|                 1|   M|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     7/3/2020 0:00:00|            |13/3/2020 0:00:00| 21/3/2020 0:00:00|              PCR|                 5|                   |
|13/3/2020 0:00:00|     15| 13/3/2020 0:00:00|                          50|               META|                    50001|   VILLAVICENCIO|  30|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     9/3/2020 0:00:00|            |13/3/2020 0:00:00| 23/3/2020 0:00:00|              PCR|                 6|                   |
|13/3/2020 0:00:00|     16| 11/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  61|                 1|   F|    Importado|          Casa|  Leve|            380|              ITALIA|Recuperado|     8/3/2020 0:00:00|            |13/3/2020 0:00:00| 23/3/2020 0:00:00|              PCR|                 5|                   |
|14/3/2020 0:00:00|     17| 11/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  73|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|    28/2/2020 0:00:00|            |14/3/2020 0:00:00| 14/3/2020 0:00:00|              PCR|                 6|                   |
|14/3/2020 0:00:00|     18| 11/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  54|                 1|   M|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     7/3/2020 0:00:00|            |14/3/2020 0:00:00|  7/4/2020 0:00:00|           Tiempo|                 6|                   |
|14/3/2020 0:00:00|     19| 12/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  54|                 1|   F|  Relacionado|          Casa|  Leve|               |                    |Recuperado|     9/3/2020 0:00:00|            |14/3/2020 0:00:00| 24/3/2020 0:00:00|              PCR|                 6|                   |
|14/3/2020 0:00:00|     20| 11/3/2020 0:00:00|                           5|          ANTIOQUIA|                     5001|        MEDELLIN|  26|                 1|   F|  Relacionado|          Casa|  Leve|               |                    |Recuperado|     9/3/2020 0:00:00|            |14/3/2020 0:00:00| 24/3/2020 0:00:00|              PCR|                 6|                   |
+-----------------+-------+------------------+----------------------------+-------------------+-------------------------+----------------+----+------------------+----+-------------+--------------+------+---------------+--------------------+----------+---------------------+------------+-----------------+------------------+-----------------+------------------+-------------------+
only showing top 20 rows

24/11/24 09:09:25 INFO BlockManagerInfo: Removed broadcast_0_piece0 on ip-172-31-84-208.ec2.internal:38097 in memory (size: 7.2 KiB, free: 912.3 MiB)
24/11/24 09:09:25 INFO BlockManagerInfo: Removed broadcast_0_piece0 on ip-172-31-80-66.ec2.internal:33251 in memory (size: 7.2 KiB, free: 4.8 GiB)
24/11/24 09:09:25 INFO ClientConfigurationFactory: Set initial getObject socket timeout to 2000 ms.
24/11/24 09:09:26 INFO ParquetUtils: Using user defined output committer for Parquet: com.amazon.emr.committer.EmrOptimizedSparkSqlParquetOutputCommitter
24/11/24 09:09:26 INFO SQLConfCommitterProvider: Getting user defined output committer class com.amazon.emr.committer.EmrOptimizedSparkSqlParquetOutputCommitter
24/11/24 09:09:26 INFO EmrOptimizedParquetOutputCommitter: EMR Optimized Committer: ENABLED
24/11/24 09:09:26 INFO EmrOptimizedParquetOutputCommitter: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileSystemOptimizedCommitter
24/11/24 09:09:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
24/11/24 09:09:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: true
24/11/24 09:09:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
24/11/24 09:09:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: true
24/11/24 09:09:26 INFO SQLConfCommitterProvider: Using output committer class com.amazon.emr.committer.EmrOptimizedSparkSqlParquetOutputCommitter
24/11/24 09:09:26 INFO FileSystemOptimizedCommitter: Nothing to setup as successful task attempt outputs are written directly
24/11/24 09:09:26 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
24/11/24 09:09:26 INFO DAGScheduler: Got job 1 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/11/24 09:09:26 INFO DAGScheduler: Final stage: ResultStage 1 (parquet at NativeMethodAccessorImpl.java:0)
24/11/24 09:09:26 INFO DAGScheduler: Parents of final stage: List()
24/11/24 09:09:26 INFO DAGScheduler: Missing parents: List()
24/11/24 09:09:26 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
24/11/24 09:09:26 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 261.7 KiB, free 912.0 MiB)
24/11/24 09:09:26 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 97.1 KiB, free 911.9 MiB)
24/11/24 09:09:26 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on ip-172-31-84-208.ec2.internal:38097 (size: 97.1 KiB, free: 912.2 MiB)
24/11/24 09:09:26 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1592
24/11/24 09:09:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/11/24 09:09:26 INFO YarnScheduler: Adding task set 1.0 with 1 tasks resource profile 0
24/11/24 09:09:26 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (ip-172-31-80-66.ec2.internal, executor 1, partition 0, PROCESS_LOCAL, 7220 bytes) 
24/11/24 09:09:26 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on ip-172-31-80-66.ec2.internal:33251 (size: 97.1 KiB, free: 4.8 GiB)
24/11/24 09:09:28 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2125 ms on ip-172-31-80-66.ec2.internal (executor 1) (1/1)
24/11/24 09:09:28 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
24/11/24 09:09:28 INFO DAGScheduler: ResultStage 1 (parquet at NativeMethodAccessorImpl.java:0) finished in 2.198 s
24/11/24 09:09:28 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
24/11/24 09:09:28 INFO YarnScheduler: Killing all running tasks in stage 1: Stage finished
24/11/24 09:09:28 INFO DAGScheduler: Job 1 finished: parquet at NativeMethodAccessorImpl.java:0, took 2.205476 s
24/11/24 09:09:28 INFO FileFormatWriter: Start to commit write Job a1c1dfe0-b6f7-41de-88b2-6c2a12989c27.
24/11/24 09:09:29 INFO MultipartUploadOutputStream: close closed:false s3://p3-tet/raw/_SUCCESS
24/11/24 09:09:29 INFO FileFormatWriter: Write Job a1c1dfe0-b6f7-41de-88b2-6c2a12989c27 committed. Elapsed time: 113 ms.
24/11/24 09:09:29 INFO FileFormatWriter: Finished processing stats for write job a1c1dfe0-b6f7-41de-88b2-6c2a12989c27.
Datos extrados y guardados en S3 en la zona raw.
24/11/24 09:09:29 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/11/24 09:09:29 INFO SparkUI: Stopped Spark web UI at http://ip-172-31-84-208.ec2.internal:4040
24/11/24 09:09:29 INFO YarnClientSchedulerBackend: Interrupting monitor thread
24/11/24 09:09:29 INFO YarnClientSchedulerBackend: Shutting down all executors
24/11/24 09:09:29 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
24/11/24 09:09:29 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
24/11/24 09:09:29 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/11/24 09:09:29 INFO MemoryStore: MemoryStore cleared
24/11/24 09:09:29 INFO BlockManager: BlockManager stopped
24/11/24 09:09:29 INFO BlockManagerMaster: BlockManagerMaster stopped
24/11/24 09:09:29 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/11/24 09:09:29 INFO SparkContext: Successfully stopped SparkContext
24/11/24 09:09:29 INFO ShutdownHookManager: Shutdown hook called
24/11/24 09:09:29 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-44fc0bdd-d2c7-496e-b191-7d3c3c0c0ff4
24/11/24 09:09:29 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-ff8575c8-50fe-4e94-abdd-e588094c794a
24/11/24 09:09:29 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-44fc0bdd-d2c7-496e-b191-7d3c3c0c0ff4/pyspark-5cc698cd-a779-4cf7-a715-a81b739c64be
24/11/24 09:10:04 INFO SparkContext: Running Spark version 3.4.1-amzn-1
24/11/24 09:10:04 INFO ResourceUtils: ==============================================================
24/11/24 09:10:04 INFO ResourceUtils: No custom resources configured for spark.driver.
24/11/24 09:10:04 INFO ResourceUtils: ==============================================================
24/11/24 09:10:04 INFO SparkContext: Submitted application: Extract_DB_to_S3
24/11/24 09:10:04 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 4, script: , vendor: , memory -> name: memory, amount: 9486, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/11/24 09:10:04 INFO ResourceProfile: Limiting resource is cpus at 4 tasks per executor
24/11/24 09:10:04 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/11/24 09:10:04 INFO SecurityManager: Changing view acls to: hadoop
24/11/24 09:10:04 INFO SecurityManager: Changing modify acls to: hadoop
24/11/24 09:10:04 INFO SecurityManager: Changing view acls groups to: 
24/11/24 09:10:04 INFO SecurityManager: Changing modify acls groups to: 
24/11/24 09:10:04 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: hadoop; groups with view permissions: EMPTY; users with modify permissions: hadoop; groups with modify permissions: EMPTY
24/11/24 09:10:04 INFO Utils: Successfully started service 'sparkDriver' on port 36199.
24/11/24 09:10:04 INFO SparkEnv: Registering MapOutputTracker
24/11/24 09:10:04 INFO SparkEnv: Registering BlockManagerMaster
24/11/24 09:10:04 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/11/24 09:10:04 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/11/24 09:10:04 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/11/24 09:10:04 INFO DiskBlockManager: Created local directory at /mnt/tmp/blockmgr-b566e516-9a74-41e5-997d-2b926f35fb3e
24/11/24 09:10:04 INFO MemoryStore: MemoryStore started with capacity 912.3 MiB
24/11/24 09:10:04 INFO SparkEnv: Registering OutputCommitCoordinator
24/11/24 09:10:04 INFO SubResultCacheManager: Sub-result caches are disabled.
24/11/24 09:10:05 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/11/24 09:10:05 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/11/24 09:10:05 INFO SparkContext: Added JAR /home/hadoop/mysql-connector-j-9.1.0.jar at spark://ip-172-31-84-208.ec2.internal:36199/jars/mysql-connector-j-9.1.0.jar with timestamp 1732439404178
24/11/24 09:10:05 INFO Utils: Using 50 preallocated executors (minExecutors: 0). Set spark.dynamicAllocation.preallocateExecutors to `false` disable executor preallocation.
24/11/24 09:10:05 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at ip-172-31-84-208.ec2.internal/172.31.84.208:8032
24/11/24 09:10:06 INFO Configuration: resource-types.xml not found
24/11/24 09:10:06 INFO ResourceUtils: Unable to find 'resource-types.xml'.
24/11/24 09:10:06 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (12288 MB per container)
24/11/24 09:10:06 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
24/11/24 09:10:06 INFO Client: Setting up container launch context for our AM
24/11/24 09:10:06 INFO Client: Setting up the launch environment for our AM container
24/11/24 09:10:06 INFO Client: Preparing resources for our AM container
24/11/24 09:10:06 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
24/11/24 09:10:08 INFO Client: Uploading resource file:/mnt/tmp/spark-60c28254-5125-4195-9568-7cc7d580f984/__spark_libs__449819983656604480.zip -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0020/__spark_libs__449819983656604480.zip
24/11/24 09:10:09 INFO Client: Uploading resource file:/home/hadoop/mysql-connector-j-9.1.0.jar -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0020/mysql-connector-j-9.1.0.jar
24/11/24 09:10:09 INFO Client: Uploading resource file:/etc/spark/conf.dist/hive-site.xml -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0020/hive-site.xml
24/11/24 09:10:09 INFO Client: Uploading resource file:/etc/hudi/conf.dist/hudi-defaults.conf -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0020/hudi-defaults.conf
24/11/24 09:10:09 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0020/pyspark.zip
24/11/24 09:10:09 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0020/py4j-0.10.9.7-src.zip
24/11/24 09:10:10 INFO Client: Uploading resource file:/mnt/tmp/spark-60c28254-5125-4195-9568-7cc7d580f984/__spark_conf__7951476883142727779.zip -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0020/__spark_conf__.zip
24/11/24 09:10:10 INFO SecurityManager: Changing view acls to: hadoop
24/11/24 09:10:10 INFO SecurityManager: Changing modify acls to: hadoop
24/11/24 09:10:10 INFO SecurityManager: Changing view acls groups to: 
24/11/24 09:10:10 INFO SecurityManager: Changing modify acls groups to: 
24/11/24 09:10:10 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: hadoop; groups with view permissions: EMPTY; users with modify permissions: hadoop; groups with modify permissions: EMPTY
24/11/24 09:10:10 INFO Client: Submitting application application_1732429252608_0020 to ResourceManager
24/11/24 09:10:10 INFO YarnClientImpl: Submitted application application_1732429252608_0020
24/11/24 09:10:11 INFO Client: Application report for application_1732429252608_0020 (state: ACCEPTED)
24/11/24 09:10:11 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1732439410182
	 final status: UNDEFINED
	 tracking URL: http://ip-172-31-84-208.ec2.internal:20888/proxy/application_1732429252608_0020/
	 user: hadoop
24/11/24 09:10:12 INFO Client: Application report for application_1732429252608_0020 (state: ACCEPTED)
24/11/24 09:10:13 INFO Client: Application report for application_1732429252608_0020 (state: ACCEPTED)
24/11/24 09:10:14 INFO Client: Application report for application_1732429252608_0020 (state: ACCEPTED)
24/11/24 09:10:15 INFO Client: Application report for application_1732429252608_0020 (state: ACCEPTED)
24/11/24 09:10:15 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> ip-172-31-84-208.ec2.internal, PROXY_URI_BASES -> http://ip-172-31-84-208.ec2.internal:20888/proxy/application_1732429252608_0020), /proxy/application_1732429252608_0020
24/11/24 09:10:15 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
24/11/24 09:10:16 INFO Client: Application report for application_1732429252608_0020 (state: RUNNING)
24/11/24 09:10:16 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 172.31.90.72
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1732439410182
	 final status: UNDEFINED
	 tracking URL: http://ip-172-31-84-208.ec2.internal:20888/proxy/application_1732429252608_0020/
	 user: hadoop
24/11/24 09:10:16 INFO YarnClientSchedulerBackend: Application application_1732429252608_0020 has started running.
24/11/24 09:10:16 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35145.
24/11/24 09:10:16 INFO NettyBlockTransferService: Server created on ip-172-31-84-208.ec2.internal:35145
24/11/24 09:10:16 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/11/24 09:10:16 INFO BlockManager: external shuffle service port = 7337
24/11/24 09:10:16 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ip-172-31-84-208.ec2.internal, 35145, None)
24/11/24 09:10:16 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-84-208.ec2.internal:35145 with 912.3 MiB RAM, BlockManagerId(driver, ip-172-31-84-208.ec2.internal, 35145, None)
24/11/24 09:10:16 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ip-172-31-84-208.ec2.internal, 35145, None)
24/11/24 09:10:16 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ip-172-31-84-208.ec2.internal, 35145, None)
24/11/24 09:10:16 INFO SingleEventLogFileWriter: Logging events to hdfs:/var/log/spark/apps/application_1732429252608_0020.inprogress
24/11/24 09:10:16 INFO Utils: Using 50 preallocated executors (minExecutors: 0). Set spark.dynamicAllocation.preallocateExecutors to `false` disable executor preallocation.
24/11/24 09:10:16 INFO ServerInfo: Adding filter to /jobs: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:10:16 INFO ServerInfo: Adding filter to /jobs/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:10:16 INFO ServerInfo: Adding filter to /jobs/job: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:10:16 INFO ServerInfo: Adding filter to /jobs/job/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:10:16 INFO ServerInfo: Adding filter to /stages: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:10:16 INFO ServerInfo: Adding filter to /stages/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:10:16 INFO ServerInfo: Adding filter to /stages/stage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:10:16 INFO ServerInfo: Adding filter to /stages/stage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:10:16 INFO ServerInfo: Adding filter to /stages/pool: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:10:16 INFO ServerInfo: Adding filter to /stages/pool/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:10:16 INFO ServerInfo: Adding filter to /storage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:10:16 INFO ServerInfo: Adding filter to /storage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:10:16 INFO ServerInfo: Adding filter to /storage/rdd: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:10:16 INFO ServerInfo: Adding filter to /storage/rdd/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:10:16 INFO ServerInfo: Adding filter to /environment: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:10:16 INFO ServerInfo: Adding filter to /environment/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:10:16 INFO ServerInfo: Adding filter to /executors: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:10:16 INFO ServerInfo: Adding filter to /executors/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:10:16 INFO ServerInfo: Adding filter to /executors/threadDump: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:10:16 INFO ServerInfo: Adding filter to /executors/threadDump/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:10:16 INFO ServerInfo: Adding filter to /static: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:10:16 INFO ServerInfo: Adding filter to /: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:10:16 INFO ServerInfo: Adding filter to /api: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:10:16 INFO ServerInfo: Adding filter to /jobs/job/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:10:16 INFO ServerInfo: Adding filter to /stages/stage/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:10:16 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:10:16 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
24/11/24 09:10:17 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
24/11/24 09:10:17 INFO SharedState: Warehouse path is 'hdfs://ip-172-31-84-208.ec2.internal:8020/user/spark/warehouse'.
24/11/24 09:10:17 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:10:17 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:10:17 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:10:17 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:10:17 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:10:19 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.31.90.72:35002) with ID 1,  ResourceProfileId 0
24/11/24 09:10:19 INFO ExecutorMonitor: New executor 1 has registered (new total is 1)
24/11/24 09:10:19 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-90-72.ec2.internal:45141 with 4.8 GiB RAM, BlockManagerId(1, ip-172-31-90-72.ec2.internal, 45141, None)
Datos extrados de la base de datos:
24/11/24 09:10:21 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.31.80.66:34200) with ID 2,  ResourceProfileId 0
24/11/24 09:10:21 INFO ExecutorMonitor: New executor 2 has registered (new total is 2)
24/11/24 09:10:21 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-80-66.ec2.internal:45395 with 4.8 GiB RAM, BlockManagerId(2, ip-172-31-80-66.ec2.internal, 45395, None)
24/11/24 09:10:22 INFO CodeGenerator: Code generated in 254.982498 ms
24/11/24 09:10:22 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
24/11/24 09:10:22 INFO DAGScheduler: Got job 0 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/11/24 09:10:22 INFO DAGScheduler: Final stage: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0)
24/11/24 09:10:22 INFO DAGScheduler: Parents of final stage: List()
24/11/24 09:10:22 INFO DAGScheduler: Missing parents: List()
24/11/24 09:10:22 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
24/11/24 09:10:22 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 17.9 KiB, free 912.3 MiB)
24/11/24 09:10:22 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 912.3 MiB)
24/11/24 09:10:22 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ip-172-31-84-208.ec2.internal:35145 (size: 7.2 KiB, free: 912.3 MiB)
24/11/24 09:10:22 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1592
24/11/24 09:10:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/11/24 09:10:22 INFO YarnScheduler: Adding task set 0.0 with 1 tasks resource profile 0
24/11/24 09:10:22 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (ip-172-31-90-72.ec2.internal, executor 1, partition 0, PROCESS_LOCAL, 7220 bytes) 
24/11/24 09:10:23 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ip-172-31-90-72.ec2.internal:45141 (size: 7.2 KiB, free: 4.8 GiB)
24/11/24 09:10:24 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2090 ms on ip-172-31-90-72.ec2.internal (executor 1) (1/1)
24/11/24 09:10:24 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/11/24 09:10:24 INFO DAGScheduler: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0) finished in 2.331 s
24/11/24 09:10:24 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
24/11/24 09:10:24 INFO YarnScheduler: Killing all running tasks in stage 0: Stage finished
24/11/24 09:10:24 INFO DAGScheduler: Job 0 finished: showString at NativeMethodAccessorImpl.java:0, took 2.386017 s
24/11/24 09:10:24 INFO CodeGenerator: Code generated in 59.557905 ms
+-----------------+-------+------------------+----------------------------+-------------------+-------------------------+----------------+----+------------------+----+-------------+--------------+------+---------------+--------------------+----------+---------------------+------------+-----------------+------------------+-----------------+------------------+-------------------+
|fecha_reporte_web|id_caso|fecha_notificacion|codigo_divipola_departamento|nombre_departamento|codigo_divipola_municipio|nombre_municipio|edad|unidad_medida_edad|sexo|tipo_contagio|ubicacion_caso|estado|codigo_iso_pais|         nombre_pais|recuperado|fecha_inicio_sintomas|fecha_muerte|fecha_diagnostico|fecha_recuperacion|tipo_recuperacion|pertenencia_etnica|nombre_grupo_etnico|
+-----------------+-------+------------------+----------------------------+-------------------+-------------------------+----------------+----+------------------+----+-------------+--------------+------+---------------+--------------------+----------+---------------------+------------+-----------------+------------------+-----------------+------------------+-------------------+
| 6/3/2020 0:00:00|      1|  2/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  19|                 1|   F|    Importado|          Casa|  Leve|            380|              ITALIA|Recuperado|    27/2/2020 0:00:00|            | 6/3/2020 0:00:00| 13/3/2020 0:00:00|              PCR|                 6|                   |
| 9/3/2020 0:00:00|      2|  6/3/2020 0:00:00|                          76|              VALLE|                    76111|            BUGA|  34|                 1|   M|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     4/3/2020 0:00:00|            | 9/3/2020 0:00:00| 19/3/2020 0:00:00|              PCR|                 5|                   |
| 9/3/2020 0:00:00|      3|  7/3/2020 0:00:00|                           5|          ANTIOQUIA|                     5001|        MEDELLIN|  50|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|    29/2/2020 0:00:00|            | 9/3/2020 0:00:00| 15/3/2020 0:00:00|              PCR|                 6|                   |
|11/3/2020 0:00:00|      4|  9/3/2020 0:00:00|                           5|          ANTIOQUIA|                     5001|        MEDELLIN|  55|                 1|   M|  Relacionado|          Casa|  Leve|               |                    |Recuperado|     6/3/2020 0:00:00|            |11/3/2020 0:00:00| 26/3/2020 0:00:00|              PCR|                 6|                   |
|11/3/2020 0:00:00|      5|  9/3/2020 0:00:00|                           5|          ANTIOQUIA|                     5001|        MEDELLIN|  25|                 1|   M|  Relacionado|          Casa|  Leve|               |                    |Recuperado|     8/3/2020 0:00:00|            |11/3/2020 0:00:00| 23/3/2020 0:00:00|              PCR|                 6|                   |
|11/3/2020 0:00:00|      6| 10/3/2020 0:00:00|                           5|          ANTIOQUIA|                     5360|          ITAGUI|  27|                 1|   F|  Relacionado|          Casa|  Leve|               |                    |Recuperado|     6/3/2020 0:00:00|            |11/3/2020 0:00:00| 26/3/2020 0:00:00|              PCR|                 6|                   |
|11/3/2020 0:00:00|      7|  8/3/2020 0:00:00|                       13001|          CARTAGENA|                    13001|       CARTAGENA|  85|                 1|   F|    Importado|          Casa|  Leve|            840|ESTADOS UNIDOS DE...|Recuperado|     2/3/2020 0:00:00|            |11/3/2020 0:00:00| 17/3/2020 0:00:00|              PCR|                 6|                   |
|11/3/2020 0:00:00|      8|  9/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  22|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     6/3/2020 0:00:00|            |11/3/2020 0:00:00| 21/3/2020 0:00:00|              PCR|                 6|                   |
|11/3/2020 0:00:00|      9|  8/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  28|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     7/3/2020 0:00:00|            |11/3/2020 0:00:00| 23/3/2020 0:00:00|              PCR|                 6|                   |
|12/3/2020 0:00:00|     10| 12/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  36|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     6/3/2020 0:00:00|            |12/3/2020 0:00:00| 21/3/2020 0:00:00|              PCR|                 6|                   |
|12/3/2020 0:00:00|     11| 11/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  42|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     6/3/2020 0:00:00|            |12/3/2020 0:00:00| 31/3/2020 0:00:00|              PCR|                 6|                   |
|12/3/2020 0:00:00|     12| 10/3/2020 0:00:00|                          41|              HUILA|                    41001|           NEIVA|  74|                 1|   F|    Importado|          Casa|  Leve|            380|              ITALIA|Recuperado|     6/3/2020 0:00:00|            |12/3/2020 0:00:00|  9/4/2020 0:00:00|              PCR|                 6|                   |
|12/3/2020 0:00:00|     13| 10/3/2020 0:00:00|                          41|              HUILA|                    41001|           NEIVA|  68|                 1|   F|  Relacionado|          Casa|  Leve|               |                    |Recuperado|     6/3/2020 0:00:00|            |12/3/2020 0:00:00| 30/3/2020 0:00:00|              PCR|                 6|                   |
|13/3/2020 0:00:00|     14| 10/3/2020 0:00:00|                          76|              VALLE|                    76520|         PALMIRA|  48|                 1|   M|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     7/3/2020 0:00:00|            |13/3/2020 0:00:00| 21/3/2020 0:00:00|              PCR|                 5|                   |
|13/3/2020 0:00:00|     15| 13/3/2020 0:00:00|                          50|               META|                    50001|   VILLAVICENCIO|  30|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     9/3/2020 0:00:00|            |13/3/2020 0:00:00| 23/3/2020 0:00:00|              PCR|                 6|                   |
|13/3/2020 0:00:00|     16| 11/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  61|                 1|   F|    Importado|          Casa|  Leve|            380|              ITALIA|Recuperado|     8/3/2020 0:00:00|            |13/3/2020 0:00:00| 23/3/2020 0:00:00|              PCR|                 5|                   |
|14/3/2020 0:00:00|     17| 11/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  73|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|    28/2/2020 0:00:00|            |14/3/2020 0:00:00| 14/3/2020 0:00:00|              PCR|                 6|                   |
|14/3/2020 0:00:00|     18| 11/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  54|                 1|   M|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     7/3/2020 0:00:00|            |14/3/2020 0:00:00|  7/4/2020 0:00:00|           Tiempo|                 6|                   |
|14/3/2020 0:00:00|     19| 12/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  54|                 1|   F|  Relacionado|          Casa|  Leve|               |                    |Recuperado|     9/3/2020 0:00:00|            |14/3/2020 0:00:00| 24/3/2020 0:00:00|              PCR|                 6|                   |
|14/3/2020 0:00:00|     20| 11/3/2020 0:00:00|                           5|          ANTIOQUIA|                     5001|        MEDELLIN|  26|                 1|   F|  Relacionado|          Casa|  Leve|               |                    |Recuperado|     9/3/2020 0:00:00|            |14/3/2020 0:00:00| 24/3/2020 0:00:00|              PCR|                 6|                   |
+-----------------+-------+------------------+----------------------------+-------------------+-------------------------+----------------+----+------------------+----+-------------+--------------+------+---------------+--------------------+----------+---------------------+------------+-----------------+------------------+-----------------+------------------+-------------------+
only showing top 20 rows

24/11/24 09:10:25 INFO BlockManagerInfo: Removed broadcast_0_piece0 on ip-172-31-84-208.ec2.internal:35145 in memory (size: 7.2 KiB, free: 912.3 MiB)
24/11/24 09:10:25 INFO BlockManagerInfo: Removed broadcast_0_piece0 on ip-172-31-90-72.ec2.internal:45141 in memory (size: 7.2 KiB, free: 4.8 GiB)
24/11/24 09:10:25 INFO ClientConfigurationFactory: Set initial getObject socket timeout to 2000 ms.
24/11/24 09:10:27 INFO ParquetUtils: Using user defined output committer for Parquet: com.amazon.emr.committer.EmrOptimizedSparkSqlParquetOutputCommitter
24/11/24 09:10:27 INFO SQLConfCommitterProvider: Getting user defined output committer class com.amazon.emr.committer.EmrOptimizedSparkSqlParquetOutputCommitter
24/11/24 09:10:27 INFO EmrOptimizedParquetOutputCommitter: EMR Optimized Committer: ENABLED
24/11/24 09:10:27 INFO EmrOptimizedParquetOutputCommitter: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileSystemOptimizedCommitter
24/11/24 09:10:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
24/11/24 09:10:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: true
24/11/24 09:10:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
24/11/24 09:10:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: true
24/11/24 09:10:27 INFO SQLConfCommitterProvider: Using output committer class com.amazon.emr.committer.EmrOptimizedSparkSqlParquetOutputCommitter
24/11/24 09:10:27 INFO FileSystemOptimizedCommitter: Nothing to setup as successful task attempt outputs are written directly
24/11/24 09:10:27 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
24/11/24 09:10:27 INFO DAGScheduler: Got job 1 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/11/24 09:10:27 INFO DAGScheduler: Final stage: ResultStage 1 (parquet at NativeMethodAccessorImpl.java:0)
24/11/24 09:10:27 INFO DAGScheduler: Parents of final stage: List()
24/11/24 09:10:27 INFO DAGScheduler: Missing parents: List()
24/11/24 09:10:27 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
24/11/24 09:10:27 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 261.7 KiB, free 912.0 MiB)
24/11/24 09:10:27 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 97.1 KiB, free 911.9 MiB)
24/11/24 09:10:27 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on ip-172-31-84-208.ec2.internal:35145 (size: 97.1 KiB, free: 912.2 MiB)
24/11/24 09:10:27 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1592
24/11/24 09:10:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/11/24 09:10:27 INFO YarnScheduler: Adding task set 1.0 with 1 tasks resource profile 0
24/11/24 09:10:27 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (ip-172-31-90-72.ec2.internal, executor 1, partition 0, PROCESS_LOCAL, 7220 bytes) 
24/11/24 09:10:27 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on ip-172-31-90-72.ec2.internal:45141 (size: 97.1 KiB, free: 4.8 GiB)
24/11/24 09:10:29 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2082 ms on ip-172-31-90-72.ec2.internal (executor 1) (1/1)
24/11/24 09:10:29 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
24/11/24 09:10:29 INFO DAGScheduler: ResultStage 1 (parquet at NativeMethodAccessorImpl.java:0) finished in 2.159 s
24/11/24 09:10:29 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
24/11/24 09:10:29 INFO YarnScheduler: Killing all running tasks in stage 1: Stage finished
24/11/24 09:10:29 INFO DAGScheduler: Job 1 finished: parquet at NativeMethodAccessorImpl.java:0, took 2.164321 s
24/11/24 09:10:29 INFO FileFormatWriter: Start to commit write Job e7ba15ba-84bd-4ce8-83c9-9ce6172f1ca5.
24/11/24 09:10:29 INFO MultipartUploadOutputStream: close closed:false s3://p3-tet/raw/_SUCCESS
24/11/24 09:10:29 INFO FileFormatWriter: Write Job e7ba15ba-84bd-4ce8-83c9-9ce6172f1ca5 committed. Elapsed time: 87 ms.
24/11/24 09:10:29 INFO FileFormatWriter: Finished processing stats for write job e7ba15ba-84bd-4ce8-83c9-9ce6172f1ca5.
Datos extrados y guardados en S3 en la zona raw.
24/11/24 09:10:29 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/11/24 09:10:29 INFO SparkUI: Stopped Spark web UI at http://ip-172-31-84-208.ec2.internal:4040
24/11/24 09:10:29 INFO YarnClientSchedulerBackend: Interrupting monitor thread
24/11/24 09:10:29 INFO YarnClientSchedulerBackend: Shutting down all executors
24/11/24 09:10:29 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
24/11/24 09:10:29 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
24/11/24 09:10:29 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/11/24 09:10:29 INFO MemoryStore: MemoryStore cleared
24/11/24 09:10:29 INFO BlockManager: BlockManager stopped
24/11/24 09:10:29 INFO BlockManagerMaster: BlockManagerMaster stopped
24/11/24 09:10:29 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/11/24 09:10:29 INFO SparkContext: Successfully stopped SparkContext
24/11/24 09:10:30 INFO ShutdownHookManager: Shutdown hook called
24/11/24 09:10:30 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-e9644c23-278e-407b-a991-d3dbffe92bee
24/11/24 09:10:30 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-60c28254-5125-4195-9568-7cc7d580f984
24/11/24 09:10:30 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-60c28254-5125-4195-9568-7cc7d580f984/pyspark-e4904467-8c9e-4fa2-b532-6739cb26b05f
24/11/24 09:11:04 INFO SparkContext: Running Spark version 3.4.1-amzn-1
24/11/24 09:11:04 INFO ResourceUtils: ==============================================================
24/11/24 09:11:04 INFO ResourceUtils: No custom resources configured for spark.driver.
24/11/24 09:11:04 INFO ResourceUtils: ==============================================================
24/11/24 09:11:04 INFO SparkContext: Submitted application: Extract_DB_to_S3
24/11/24 09:11:04 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 4, script: , vendor: , memory -> name: memory, amount: 9486, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/11/24 09:11:04 INFO ResourceProfile: Limiting resource is cpus at 4 tasks per executor
24/11/24 09:11:04 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/11/24 09:11:04 INFO SecurityManager: Changing view acls to: hadoop
24/11/24 09:11:04 INFO SecurityManager: Changing modify acls to: hadoop
24/11/24 09:11:04 INFO SecurityManager: Changing view acls groups to: 
24/11/24 09:11:04 INFO SecurityManager: Changing modify acls groups to: 
24/11/24 09:11:04 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: hadoop; groups with view permissions: EMPTY; users with modify permissions: hadoop; groups with modify permissions: EMPTY
24/11/24 09:11:05 INFO Utils: Successfully started service 'sparkDriver' on port 40603.
24/11/24 09:11:05 INFO SparkEnv: Registering MapOutputTracker
24/11/24 09:11:05 INFO SparkEnv: Registering BlockManagerMaster
24/11/24 09:11:05 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/11/24 09:11:05 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/11/24 09:11:05 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/11/24 09:11:05 INFO DiskBlockManager: Created local directory at /mnt/tmp/blockmgr-84c425ed-515b-46a4-90fc-4b89a4d8bff7
24/11/24 09:11:05 INFO MemoryStore: MemoryStore started with capacity 912.3 MiB
24/11/24 09:11:05 INFO SparkEnv: Registering OutputCommitCoordinator
24/11/24 09:11:05 INFO SubResultCacheManager: Sub-result caches are disabled.
24/11/24 09:11:05 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/11/24 09:11:05 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/11/24 09:11:05 INFO SparkContext: Added JAR /home/hadoop/mysql-connector-j-9.1.0.jar at spark://ip-172-31-84-208.ec2.internal:40603/jars/mysql-connector-j-9.1.0.jar with timestamp 1732439464708
24/11/24 09:11:05 INFO Utils: Using 50 preallocated executors (minExecutors: 0). Set spark.dynamicAllocation.preallocateExecutors to `false` disable executor preallocation.
24/11/24 09:11:05 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at ip-172-31-84-208.ec2.internal/172.31.84.208:8032
24/11/24 09:11:06 INFO Configuration: resource-types.xml not found
24/11/24 09:11:06 INFO ResourceUtils: Unable to find 'resource-types.xml'.
24/11/24 09:11:06 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (12288 MB per container)
24/11/24 09:11:06 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
24/11/24 09:11:06 INFO Client: Setting up container launch context for our AM
24/11/24 09:11:06 INFO Client: Setting up the launch environment for our AM container
24/11/24 09:11:06 INFO Client: Preparing resources for our AM container
24/11/24 09:11:06 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
24/11/24 09:11:09 INFO Client: Uploading resource file:/mnt/tmp/spark-5caa0a21-4c5f-4a42-be89-01ea8662df8a/__spark_libs__4488481838610541144.zip -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0021/__spark_libs__4488481838610541144.zip
24/11/24 09:11:10 INFO Client: Uploading resource file:/home/hadoop/mysql-connector-j-9.1.0.jar -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0021/mysql-connector-j-9.1.0.jar
24/11/24 09:11:10 INFO Client: Uploading resource file:/etc/spark/conf.dist/hive-site.xml -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0021/hive-site.xml
24/11/24 09:11:10 INFO Client: Uploading resource file:/etc/hudi/conf.dist/hudi-defaults.conf -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0021/hudi-defaults.conf
24/11/24 09:11:10 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0021/pyspark.zip
24/11/24 09:11:10 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0021/py4j-0.10.9.7-src.zip
24/11/24 09:11:10 INFO Client: Uploading resource file:/mnt/tmp/spark-5caa0a21-4c5f-4a42-be89-01ea8662df8a/__spark_conf__1486939421669756597.zip -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0021/__spark_conf__.zip
24/11/24 09:11:11 INFO SecurityManager: Changing view acls to: hadoop
24/11/24 09:11:11 INFO SecurityManager: Changing modify acls to: hadoop
24/11/24 09:11:11 INFO SecurityManager: Changing view acls groups to: 
24/11/24 09:11:11 INFO SecurityManager: Changing modify acls groups to: 
24/11/24 09:11:11 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: hadoop; groups with view permissions: EMPTY; users with modify permissions: hadoop; groups with modify permissions: EMPTY
24/11/24 09:11:11 INFO Client: Submitting application application_1732429252608_0021 to ResourceManager
24/11/24 09:11:11 INFO YarnClientImpl: Submitted application application_1732429252608_0021
24/11/24 09:11:12 INFO Client: Application report for application_1732429252608_0021 (state: ACCEPTED)
24/11/24 09:11:12 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1732439471059
	 final status: UNDEFINED
	 tracking URL: http://ip-172-31-84-208.ec2.internal:20888/proxy/application_1732429252608_0021/
	 user: hadoop
24/11/24 09:11:13 INFO Client: Application report for application_1732429252608_0021 (state: ACCEPTED)
24/11/24 09:11:14 INFO Client: Application report for application_1732429252608_0021 (state: ACCEPTED)
24/11/24 09:11:15 INFO Client: Application report for application_1732429252608_0021 (state: ACCEPTED)
24/11/24 09:11:16 INFO Client: Application report for application_1732429252608_0021 (state: ACCEPTED)
24/11/24 09:11:16 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> ip-172-31-84-208.ec2.internal, PROXY_URI_BASES -> http://ip-172-31-84-208.ec2.internal:20888/proxy/application_1732429252608_0021), /proxy/application_1732429252608_0021
24/11/24 09:11:16 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
24/11/24 09:11:17 INFO Client: Application report for application_1732429252608_0021 (state: RUNNING)
24/11/24 09:11:17 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 172.31.80.66
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1732439471059
	 final status: UNDEFINED
	 tracking URL: http://ip-172-31-84-208.ec2.internal:20888/proxy/application_1732429252608_0021/
	 user: hadoop
24/11/24 09:11:17 INFO YarnClientSchedulerBackend: Application application_1732429252608_0021 has started running.
24/11/24 09:11:17 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42387.
24/11/24 09:11:17 INFO NettyBlockTransferService: Server created on ip-172-31-84-208.ec2.internal:42387
24/11/24 09:11:17 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/11/24 09:11:17 INFO BlockManager: external shuffle service port = 7337
24/11/24 09:11:17 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ip-172-31-84-208.ec2.internal, 42387, None)
24/11/24 09:11:17 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-84-208.ec2.internal:42387 with 912.3 MiB RAM, BlockManagerId(driver, ip-172-31-84-208.ec2.internal, 42387, None)
24/11/24 09:11:17 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ip-172-31-84-208.ec2.internal, 42387, None)
24/11/24 09:11:17 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ip-172-31-84-208.ec2.internal, 42387, None)
24/11/24 09:11:17 INFO SingleEventLogFileWriter: Logging events to hdfs:/var/log/spark/apps/application_1732429252608_0021.inprogress
24/11/24 09:11:17 INFO Utils: Using 50 preallocated executors (minExecutors: 0). Set spark.dynamicAllocation.preallocateExecutors to `false` disable executor preallocation.
24/11/24 09:11:17 INFO ServerInfo: Adding filter to /jobs: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:11:17 INFO ServerInfo: Adding filter to /jobs/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:11:17 INFO ServerInfo: Adding filter to /jobs/job: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:11:17 INFO ServerInfo: Adding filter to /jobs/job/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:11:17 INFO ServerInfo: Adding filter to /stages: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:11:17 INFO ServerInfo: Adding filter to /stages/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:11:17 INFO ServerInfo: Adding filter to /stages/stage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:11:17 INFO ServerInfo: Adding filter to /stages/stage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:11:17 INFO ServerInfo: Adding filter to /stages/pool: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:11:17 INFO ServerInfo: Adding filter to /stages/pool/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:11:17 INFO ServerInfo: Adding filter to /storage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:11:17 INFO ServerInfo: Adding filter to /storage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:11:17 INFO ServerInfo: Adding filter to /storage/rdd: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:11:17 INFO ServerInfo: Adding filter to /storage/rdd/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:11:17 INFO ServerInfo: Adding filter to /environment: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:11:17 INFO ServerInfo: Adding filter to /environment/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:11:17 INFO ServerInfo: Adding filter to /executors: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:11:17 INFO ServerInfo: Adding filter to /executors/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:11:17 INFO ServerInfo: Adding filter to /executors/threadDump: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:11:17 INFO ServerInfo: Adding filter to /executors/threadDump/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:11:17 INFO ServerInfo: Adding filter to /static: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:11:17 INFO ServerInfo: Adding filter to /: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:11:17 INFO ServerInfo: Adding filter to /api: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:11:17 INFO ServerInfo: Adding filter to /jobs/job/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:11:17 INFO ServerInfo: Adding filter to /stages/stage/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:11:17 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:11:17 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
24/11/24 09:11:17 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
24/11/24 09:11:17 INFO SharedState: Warehouse path is 'hdfs://ip-172-31-84-208.ec2.internal:8020/user/spark/warehouse'.
24/11/24 09:11:17 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:11:17 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:11:17 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:11:17 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:11:17 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:11:20 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.31.80.66:38640) with ID 2,  ResourceProfileId 0
24/11/24 09:11:20 INFO ExecutorMonitor: New executor 2 has registered (new total is 1)
24/11/24 09:11:20 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-80-66.ec2.internal:34243 with 4.8 GiB RAM, BlockManagerId(2, ip-172-31-80-66.ec2.internal, 34243, None)
Datos extrados de la base de datos:
24/11/24 09:11:22 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.31.90.72:56436) with ID 1,  ResourceProfileId 0
24/11/24 09:11:22 INFO ExecutorMonitor: New executor 1 has registered (new total is 2)
24/11/24 09:11:22 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-90-72.ec2.internal:42507 with 4.8 GiB RAM, BlockManagerId(1, ip-172-31-90-72.ec2.internal, 42507, None)
24/11/24 09:11:23 INFO CodeGenerator: Code generated in 289.96341 ms
24/11/24 09:11:23 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
24/11/24 09:11:23 INFO DAGScheduler: Got job 0 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/11/24 09:11:23 INFO DAGScheduler: Final stage: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0)
24/11/24 09:11:23 INFO DAGScheduler: Parents of final stage: List()
24/11/24 09:11:23 INFO DAGScheduler: Missing parents: List()
24/11/24 09:11:23 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
24/11/24 09:11:23 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 17.9 KiB, free 912.3 MiB)
24/11/24 09:11:23 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 912.3 MiB)
24/11/24 09:11:23 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ip-172-31-84-208.ec2.internal:42387 (size: 7.2 KiB, free: 912.3 MiB)
24/11/24 09:11:23 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1592
24/11/24 09:11:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/11/24 09:11:23 INFO YarnScheduler: Adding task set 0.0 with 1 tasks resource profile 0
24/11/24 09:11:23 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (ip-172-31-80-66.ec2.internal, executor 2, partition 0, PROCESS_LOCAL, 7220 bytes) 
24/11/24 09:11:24 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ip-172-31-80-66.ec2.internal:34243 (size: 7.2 KiB, free: 4.8 GiB)
24/11/24 09:11:25 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2024 ms on ip-172-31-80-66.ec2.internal (executor 2) (1/1)
24/11/24 09:11:25 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/11/24 09:11:25 INFO DAGScheduler: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0) finished in 2.208 s
24/11/24 09:11:25 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
24/11/24 09:11:25 INFO YarnScheduler: Killing all running tasks in stage 0: Stage finished
24/11/24 09:11:25 INFO DAGScheduler: Job 0 finished: showString at NativeMethodAccessorImpl.java:0, took 2.267108 s
24/11/24 09:11:25 INFO CodeGenerator: Code generated in 58.107472 ms
+-----------------+-------+------------------+----------------------------+-------------------+-------------------------+----------------+----+------------------+----+-------------+--------------+------+---------------+--------------------+----------+---------------------+------------+-----------------+------------------+-----------------+------------------+-------------------+
|fecha_reporte_web|id_caso|fecha_notificacion|codigo_divipola_departamento|nombre_departamento|codigo_divipola_municipio|nombre_municipio|edad|unidad_medida_edad|sexo|tipo_contagio|ubicacion_caso|estado|codigo_iso_pais|         nombre_pais|recuperado|fecha_inicio_sintomas|fecha_muerte|fecha_diagnostico|fecha_recuperacion|tipo_recuperacion|pertenencia_etnica|nombre_grupo_etnico|
+-----------------+-------+------------------+----------------------------+-------------------+-------------------------+----------------+----+------------------+----+-------------+--------------+------+---------------+--------------------+----------+---------------------+------------+-----------------+------------------+-----------------+------------------+-------------------+
| 6/3/2020 0:00:00|      1|  2/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  19|                 1|   F|    Importado|          Casa|  Leve|            380|              ITALIA|Recuperado|    27/2/2020 0:00:00|            | 6/3/2020 0:00:00| 13/3/2020 0:00:00|              PCR|                 6|                   |
| 9/3/2020 0:00:00|      2|  6/3/2020 0:00:00|                          76|              VALLE|                    76111|            BUGA|  34|                 1|   M|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     4/3/2020 0:00:00|            | 9/3/2020 0:00:00| 19/3/2020 0:00:00|              PCR|                 5|                   |
| 9/3/2020 0:00:00|      3|  7/3/2020 0:00:00|                           5|          ANTIOQUIA|                     5001|        MEDELLIN|  50|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|    29/2/2020 0:00:00|            | 9/3/2020 0:00:00| 15/3/2020 0:00:00|              PCR|                 6|                   |
|11/3/2020 0:00:00|      4|  9/3/2020 0:00:00|                           5|          ANTIOQUIA|                     5001|        MEDELLIN|  55|                 1|   M|  Relacionado|          Casa|  Leve|               |                    |Recuperado|     6/3/2020 0:00:00|            |11/3/2020 0:00:00| 26/3/2020 0:00:00|              PCR|                 6|                   |
|11/3/2020 0:00:00|      5|  9/3/2020 0:00:00|                           5|          ANTIOQUIA|                     5001|        MEDELLIN|  25|                 1|   M|  Relacionado|          Casa|  Leve|               |                    |Recuperado|     8/3/2020 0:00:00|            |11/3/2020 0:00:00| 23/3/2020 0:00:00|              PCR|                 6|                   |
|11/3/2020 0:00:00|      6| 10/3/2020 0:00:00|                           5|          ANTIOQUIA|                     5360|          ITAGUI|  27|                 1|   F|  Relacionado|          Casa|  Leve|               |                    |Recuperado|     6/3/2020 0:00:00|            |11/3/2020 0:00:00| 26/3/2020 0:00:00|              PCR|                 6|                   |
|11/3/2020 0:00:00|      7|  8/3/2020 0:00:00|                       13001|          CARTAGENA|                    13001|       CARTAGENA|  85|                 1|   F|    Importado|          Casa|  Leve|            840|ESTADOS UNIDOS DE...|Recuperado|     2/3/2020 0:00:00|            |11/3/2020 0:00:00| 17/3/2020 0:00:00|              PCR|                 6|                   |
|11/3/2020 0:00:00|      8|  9/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  22|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     6/3/2020 0:00:00|            |11/3/2020 0:00:00| 21/3/2020 0:00:00|              PCR|                 6|                   |
|11/3/2020 0:00:00|      9|  8/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  28|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     7/3/2020 0:00:00|            |11/3/2020 0:00:00| 23/3/2020 0:00:00|              PCR|                 6|                   |
|12/3/2020 0:00:00|     10| 12/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  36|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     6/3/2020 0:00:00|            |12/3/2020 0:00:00| 21/3/2020 0:00:00|              PCR|                 6|                   |
|12/3/2020 0:00:00|     11| 11/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  42|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     6/3/2020 0:00:00|            |12/3/2020 0:00:00| 31/3/2020 0:00:00|              PCR|                 6|                   |
|12/3/2020 0:00:00|     12| 10/3/2020 0:00:00|                          41|              HUILA|                    41001|           NEIVA|  74|                 1|   F|    Importado|          Casa|  Leve|            380|              ITALIA|Recuperado|     6/3/2020 0:00:00|            |12/3/2020 0:00:00|  9/4/2020 0:00:00|              PCR|                 6|                   |
|12/3/2020 0:00:00|     13| 10/3/2020 0:00:00|                          41|              HUILA|                    41001|           NEIVA|  68|                 1|   F|  Relacionado|          Casa|  Leve|               |                    |Recuperado|     6/3/2020 0:00:00|            |12/3/2020 0:00:00| 30/3/2020 0:00:00|              PCR|                 6|                   |
|13/3/2020 0:00:00|     14| 10/3/2020 0:00:00|                          76|              VALLE|                    76520|         PALMIRA|  48|                 1|   M|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     7/3/2020 0:00:00|            |13/3/2020 0:00:00| 21/3/2020 0:00:00|              PCR|                 5|                   |
|13/3/2020 0:00:00|     15| 13/3/2020 0:00:00|                          50|               META|                    50001|   VILLAVICENCIO|  30|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     9/3/2020 0:00:00|            |13/3/2020 0:00:00| 23/3/2020 0:00:00|              PCR|                 6|                   |
|13/3/2020 0:00:00|     16| 11/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  61|                 1|   F|    Importado|          Casa|  Leve|            380|              ITALIA|Recuperado|     8/3/2020 0:00:00|            |13/3/2020 0:00:00| 23/3/2020 0:00:00|              PCR|                 5|                   |
|14/3/2020 0:00:00|     17| 11/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  73|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|    28/2/2020 0:00:00|            |14/3/2020 0:00:00| 14/3/2020 0:00:00|              PCR|                 6|                   |
|14/3/2020 0:00:00|     18| 11/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  54|                 1|   M|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     7/3/2020 0:00:00|            |14/3/2020 0:00:00|  7/4/2020 0:00:00|           Tiempo|                 6|                   |
|14/3/2020 0:00:00|     19| 12/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  54|                 1|   F|  Relacionado|          Casa|  Leve|               |                    |Recuperado|     9/3/2020 0:00:00|            |14/3/2020 0:00:00| 24/3/2020 0:00:00|              PCR|                 6|                   |
|14/3/2020 0:00:00|     20| 11/3/2020 0:00:00|                           5|          ANTIOQUIA|                     5001|        MEDELLIN|  26|                 1|   F|  Relacionado|          Casa|  Leve|               |                    |Recuperado|     9/3/2020 0:00:00|            |14/3/2020 0:00:00| 24/3/2020 0:00:00|              PCR|                 6|                   |
+-----------------+-------+------------------+----------------------------+-------------------+-------------------------+----------------+----+------------------+----+-------------+--------------+------+---------------+--------------------+----------+---------------------+------------+-----------------+------------------+-----------------+------------------+-------------------+
only showing top 20 rows

24/11/24 09:11:26 INFO BlockManagerInfo: Removed broadcast_0_piece0 on ip-172-31-80-66.ec2.internal:34243 in memory (size: 7.2 KiB, free: 4.8 GiB)
24/11/24 09:11:26 INFO BlockManagerInfo: Removed broadcast_0_piece0 on ip-172-31-84-208.ec2.internal:42387 in memory (size: 7.2 KiB, free: 912.3 MiB)
24/11/24 09:11:26 INFO ClientConfigurationFactory: Set initial getObject socket timeout to 2000 ms.
24/11/24 09:11:28 INFO ParquetUtils: Using user defined output committer for Parquet: com.amazon.emr.committer.EmrOptimizedSparkSqlParquetOutputCommitter
24/11/24 09:11:28 INFO SQLConfCommitterProvider: Getting user defined output committer class com.amazon.emr.committer.EmrOptimizedSparkSqlParquetOutputCommitter
24/11/24 09:11:28 INFO EmrOptimizedParquetOutputCommitter: EMR Optimized Committer: ENABLED
24/11/24 09:11:28 INFO EmrOptimizedParquetOutputCommitter: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileSystemOptimizedCommitter
24/11/24 09:11:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
24/11/24 09:11:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: true
24/11/24 09:11:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
24/11/24 09:11:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: true
24/11/24 09:11:28 INFO SQLConfCommitterProvider: Using output committer class com.amazon.emr.committer.EmrOptimizedSparkSqlParquetOutputCommitter
24/11/24 09:11:28 INFO FileSystemOptimizedCommitter: Nothing to setup as successful task attempt outputs are written directly
24/11/24 09:11:28 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
24/11/24 09:11:28 INFO DAGScheduler: Got job 1 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/11/24 09:11:28 INFO DAGScheduler: Final stage: ResultStage 1 (parquet at NativeMethodAccessorImpl.java:0)
24/11/24 09:11:28 INFO DAGScheduler: Parents of final stage: List()
24/11/24 09:11:28 INFO DAGScheduler: Missing parents: List()
24/11/24 09:11:28 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
24/11/24 09:11:28 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 261.7 KiB, free 912.0 MiB)
24/11/24 09:11:28 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 97.1 KiB, free 911.9 MiB)
24/11/24 09:11:28 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on ip-172-31-84-208.ec2.internal:42387 (size: 97.1 KiB, free: 912.2 MiB)
24/11/24 09:11:28 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1592
24/11/24 09:11:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/11/24 09:11:28 INFO YarnScheduler: Adding task set 1.0 with 1 tasks resource profile 0
24/11/24 09:11:28 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (ip-172-31-80-66.ec2.internal, executor 2, partition 0, PROCESS_LOCAL, 7220 bytes) 
24/11/24 09:11:28 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on ip-172-31-80-66.ec2.internal:34243 (size: 97.1 KiB, free: 4.8 GiB)
24/11/24 09:11:30 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2048 ms on ip-172-31-80-66.ec2.internal (executor 2) (1/1)
24/11/24 09:11:30 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
24/11/24 09:11:30 INFO DAGScheduler: ResultStage 1 (parquet at NativeMethodAccessorImpl.java:0) finished in 2.118 s
24/11/24 09:11:30 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
24/11/24 09:11:30 INFO YarnScheduler: Killing all running tasks in stage 1: Stage finished
24/11/24 09:11:30 INFO DAGScheduler: Job 1 finished: parquet at NativeMethodAccessorImpl.java:0, took 2.124697 s
24/11/24 09:11:30 INFO FileFormatWriter: Start to commit write Job dfbdcaa7-15f6-49a9-a782-3c23206c7078.
24/11/24 09:11:30 INFO MultipartUploadOutputStream: close closed:false s3://p3-tet/raw/_SUCCESS
24/11/24 09:11:30 INFO FileFormatWriter: Write Job dfbdcaa7-15f6-49a9-a782-3c23206c7078 committed. Elapsed time: 94 ms.
24/11/24 09:11:30 INFO FileFormatWriter: Finished processing stats for write job dfbdcaa7-15f6-49a9-a782-3c23206c7078.
Datos extrados y guardados en S3 en la zona raw.
24/11/24 09:11:30 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/11/24 09:11:30 INFO SparkUI: Stopped Spark web UI at http://ip-172-31-84-208.ec2.internal:4040
24/11/24 09:11:30 INFO YarnClientSchedulerBackend: Interrupting monitor thread
24/11/24 09:11:30 INFO YarnClientSchedulerBackend: Shutting down all executors
24/11/24 09:11:30 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
24/11/24 09:11:30 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
24/11/24 09:11:30 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/11/24 09:11:30 INFO MemoryStore: MemoryStore cleared
24/11/24 09:11:30 INFO BlockManager: BlockManager stopped
24/11/24 09:11:30 INFO BlockManagerMaster: BlockManagerMaster stopped
24/11/24 09:11:30 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/11/24 09:11:30 INFO SparkContext: Successfully stopped SparkContext
24/11/24 09:11:31 INFO ShutdownHookManager: Shutdown hook called
24/11/24 09:11:31 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-5caa0a21-4c5f-4a42-be89-01ea8662df8a
24/11/24 09:11:31 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-5caa0a21-4c5f-4a42-be89-01ea8662df8a/pyspark-d44ac7a6-1e04-4778-9c7d-417f732f1f90
24/11/24 09:11:31 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-8746ee53-127a-44bd-9ae9-f042d1645581
24/11/24 09:12:04 INFO SparkContext: Running Spark version 3.4.1-amzn-1
24/11/24 09:12:04 INFO ResourceUtils: ==============================================================
24/11/24 09:12:04 INFO ResourceUtils: No custom resources configured for spark.driver.
24/11/24 09:12:04 INFO ResourceUtils: ==============================================================
24/11/24 09:12:04 INFO SparkContext: Submitted application: Extract_DB_to_S3
24/11/24 09:12:04 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 4, script: , vendor: , memory -> name: memory, amount: 9486, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/11/24 09:12:04 INFO ResourceProfile: Limiting resource is cpus at 4 tasks per executor
24/11/24 09:12:04 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/11/24 09:12:04 INFO SecurityManager: Changing view acls to: hadoop
24/11/24 09:12:04 INFO SecurityManager: Changing modify acls to: hadoop
24/11/24 09:12:04 INFO SecurityManager: Changing view acls groups to: 
24/11/24 09:12:04 INFO SecurityManager: Changing modify acls groups to: 
24/11/24 09:12:04 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: hadoop; groups with view permissions: EMPTY; users with modify permissions: hadoop; groups with modify permissions: EMPTY
24/11/24 09:12:05 INFO Utils: Successfully started service 'sparkDriver' on port 35819.
24/11/24 09:12:05 INFO SparkEnv: Registering MapOutputTracker
24/11/24 09:12:05 INFO SparkEnv: Registering BlockManagerMaster
24/11/24 09:12:05 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/11/24 09:12:05 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/11/24 09:12:05 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/11/24 09:12:05 INFO DiskBlockManager: Created local directory at /mnt/tmp/blockmgr-26106179-44df-4436-8809-f37d850f5247
24/11/24 09:12:05 INFO MemoryStore: MemoryStore started with capacity 912.3 MiB
24/11/24 09:12:05 INFO SparkEnv: Registering OutputCommitCoordinator
24/11/24 09:12:05 INFO SubResultCacheManager: Sub-result caches are disabled.
24/11/24 09:12:05 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/11/24 09:12:05 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/11/24 09:12:05 INFO SparkContext: Added JAR /home/hadoop/mysql-connector-j-9.1.0.jar at spark://ip-172-31-84-208.ec2.internal:35819/jars/mysql-connector-j-9.1.0.jar with timestamp 1732439524595
24/11/24 09:12:05 INFO Utils: Using 50 preallocated executors (minExecutors: 0). Set spark.dynamicAllocation.preallocateExecutors to `false` disable executor preallocation.
24/11/24 09:12:05 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at ip-172-31-84-208.ec2.internal/172.31.84.208:8032
24/11/24 09:12:06 INFO Configuration: resource-types.xml not found
24/11/24 09:12:06 INFO ResourceUtils: Unable to find 'resource-types.xml'.
24/11/24 09:12:06 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (12288 MB per container)
24/11/24 09:12:06 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
24/11/24 09:12:06 INFO Client: Setting up container launch context for our AM
24/11/24 09:12:06 INFO Client: Setting up the launch environment for our AM container
24/11/24 09:12:06 INFO Client: Preparing resources for our AM container
24/11/24 09:12:06 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
24/11/24 09:12:09 INFO Client: Uploading resource file:/mnt/tmp/spark-1df5acf0-6d62-4b65-b4ca-8982724d6df9/__spark_libs__2813544026009501030.zip -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0022/__spark_libs__2813544026009501030.zip
24/11/24 09:12:10 INFO Client: Uploading resource file:/home/hadoop/mysql-connector-j-9.1.0.jar -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0022/mysql-connector-j-9.1.0.jar
24/11/24 09:12:10 INFO Client: Uploading resource file:/etc/spark/conf.dist/hive-site.xml -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0022/hive-site.xml
24/11/24 09:12:10 INFO Client: Uploading resource file:/etc/hudi/conf.dist/hudi-defaults.conf -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0022/hudi-defaults.conf
24/11/24 09:12:10 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0022/pyspark.zip
24/11/24 09:12:10 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0022/py4j-0.10.9.7-src.zip
24/11/24 09:12:10 INFO Client: Uploading resource file:/mnt/tmp/spark-1df5acf0-6d62-4b65-b4ca-8982724d6df9/__spark_conf__2711367819749798673.zip -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0022/__spark_conf__.zip
24/11/24 09:12:10 INFO SecurityManager: Changing view acls to: hadoop
24/11/24 09:12:10 INFO SecurityManager: Changing modify acls to: hadoop
24/11/24 09:12:10 INFO SecurityManager: Changing view acls groups to: 
24/11/24 09:12:10 INFO SecurityManager: Changing modify acls groups to: 
24/11/24 09:12:10 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: hadoop; groups with view permissions: EMPTY; users with modify permissions: hadoop; groups with modify permissions: EMPTY
24/11/24 09:12:10 INFO Client: Submitting application application_1732429252608_0022 to ResourceManager
24/11/24 09:12:10 INFO YarnClientImpl: Submitted application application_1732429252608_0022
24/11/24 09:12:11 INFO Client: Application report for application_1732429252608_0022 (state: ACCEPTED)
24/11/24 09:12:11 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1732439530610
	 final status: UNDEFINED
	 tracking URL: http://ip-172-31-84-208.ec2.internal:20888/proxy/application_1732429252608_0022/
	 user: hadoop
24/11/24 09:12:12 INFO Client: Application report for application_1732429252608_0022 (state: ACCEPTED)
24/11/24 09:12:13 INFO Client: Application report for application_1732429252608_0022 (state: ACCEPTED)
24/11/24 09:12:14 INFO Client: Application report for application_1732429252608_0022 (state: ACCEPTED)
24/11/24 09:12:15 INFO Client: Application report for application_1732429252608_0022 (state: ACCEPTED)
24/11/24 09:12:15 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> ip-172-31-84-208.ec2.internal, PROXY_URI_BASES -> http://ip-172-31-84-208.ec2.internal:20888/proxy/application_1732429252608_0022), /proxy/application_1732429252608_0022
24/11/24 09:12:16 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
24/11/24 09:12:16 INFO Client: Application report for application_1732429252608_0022 (state: RUNNING)
24/11/24 09:12:16 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 172.31.80.66
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1732439530610
	 final status: UNDEFINED
	 tracking URL: http://ip-172-31-84-208.ec2.internal:20888/proxy/application_1732429252608_0022/
	 user: hadoop
24/11/24 09:12:16 INFO YarnClientSchedulerBackend: Application application_1732429252608_0022 has started running.
24/11/24 09:12:16 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40453.
24/11/24 09:12:16 INFO NettyBlockTransferService: Server created on ip-172-31-84-208.ec2.internal:40453
24/11/24 09:12:16 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/11/24 09:12:16 INFO BlockManager: external shuffle service port = 7337
24/11/24 09:12:16 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ip-172-31-84-208.ec2.internal, 40453, None)
24/11/24 09:12:16 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-84-208.ec2.internal:40453 with 912.3 MiB RAM, BlockManagerId(driver, ip-172-31-84-208.ec2.internal, 40453, None)
24/11/24 09:12:16 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ip-172-31-84-208.ec2.internal, 40453, None)
24/11/24 09:12:16 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ip-172-31-84-208.ec2.internal, 40453, None)
24/11/24 09:12:16 INFO SingleEventLogFileWriter: Logging events to hdfs:/var/log/spark/apps/application_1732429252608_0022.inprogress
24/11/24 09:12:17 INFO Utils: Using 50 preallocated executors (minExecutors: 0). Set spark.dynamicAllocation.preallocateExecutors to `false` disable executor preallocation.
24/11/24 09:12:17 INFO ServerInfo: Adding filter to /jobs: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:12:17 INFO ServerInfo: Adding filter to /jobs/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:12:17 INFO ServerInfo: Adding filter to /jobs/job: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:12:17 INFO ServerInfo: Adding filter to /jobs/job/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:12:17 INFO ServerInfo: Adding filter to /stages: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:12:17 INFO ServerInfo: Adding filter to /stages/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:12:17 INFO ServerInfo: Adding filter to /stages/stage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:12:17 INFO ServerInfo: Adding filter to /stages/stage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:12:17 INFO ServerInfo: Adding filter to /stages/pool: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:12:17 INFO ServerInfo: Adding filter to /stages/pool/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:12:17 INFO ServerInfo: Adding filter to /storage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:12:17 INFO ServerInfo: Adding filter to /storage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:12:17 INFO ServerInfo: Adding filter to /storage/rdd: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:12:17 INFO ServerInfo: Adding filter to /storage/rdd/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:12:17 INFO ServerInfo: Adding filter to /environment: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:12:17 INFO ServerInfo: Adding filter to /environment/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:12:17 INFO ServerInfo: Adding filter to /executors: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:12:17 INFO ServerInfo: Adding filter to /executors/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:12:17 INFO ServerInfo: Adding filter to /executors/threadDump: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:12:17 INFO ServerInfo: Adding filter to /executors/threadDump/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:12:17 INFO ServerInfo: Adding filter to /static: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:12:17 INFO ServerInfo: Adding filter to /: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:12:17 INFO ServerInfo: Adding filter to /api: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:12:17 INFO ServerInfo: Adding filter to /jobs/job/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:12:17 INFO ServerInfo: Adding filter to /stages/stage/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:12:17 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:12:17 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
24/11/24 09:12:17 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
24/11/24 09:12:17 INFO SharedState: Warehouse path is 'hdfs://ip-172-31-84-208.ec2.internal:8020/user/spark/warehouse'.
24/11/24 09:12:17 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:12:17 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:12:17 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:12:17 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:12:17 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:12:19 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.31.80.66:47448) with ID 1,  ResourceProfileId 0
24/11/24 09:12:19 INFO ExecutorMonitor: New executor 1 has registered (new total is 1)
24/11/24 09:12:20 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-80-66.ec2.internal:45145 with 4.8 GiB RAM, BlockManagerId(1, ip-172-31-80-66.ec2.internal, 45145, None)
Datos extrados de la base de datos:
24/11/24 09:12:21 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.31.90.72:41840) with ID 2,  ResourceProfileId 0
24/11/24 09:12:21 INFO ExecutorMonitor: New executor 2 has registered (new total is 2)
24/11/24 09:12:21 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-90-72.ec2.internal:38803 with 4.8 GiB RAM, BlockManagerId(2, ip-172-31-90-72.ec2.internal, 38803, None)
24/11/24 09:12:22 INFO CodeGenerator: Code generated in 243.56511 ms
24/11/24 09:12:22 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
24/11/24 09:12:22 INFO DAGScheduler: Got job 0 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/11/24 09:12:22 INFO DAGScheduler: Final stage: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0)
24/11/24 09:12:22 INFO DAGScheduler: Parents of final stage: List()
24/11/24 09:12:22 INFO DAGScheduler: Missing parents: List()
24/11/24 09:12:22 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
24/11/24 09:12:23 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 17.9 KiB, free 912.3 MiB)
24/11/24 09:12:23 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 912.3 MiB)
24/11/24 09:12:23 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ip-172-31-84-208.ec2.internal:40453 (size: 7.2 KiB, free: 912.3 MiB)
24/11/24 09:12:23 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1592
24/11/24 09:12:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/11/24 09:12:23 INFO YarnScheduler: Adding task set 0.0 with 1 tasks resource profile 0
24/11/24 09:12:23 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (ip-172-31-90-72.ec2.internal, executor 2, partition 0, PROCESS_LOCAL, 7220 bytes) 
24/11/24 09:12:23 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ip-172-31-90-72.ec2.internal:38803 (size: 7.2 KiB, free: 4.8 GiB)
24/11/24 09:12:25 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1955 ms on ip-172-31-90-72.ec2.internal (executor 2) (1/1)
24/11/24 09:12:25 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/11/24 09:12:25 INFO DAGScheduler: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0) finished in 2.171 s
24/11/24 09:12:25 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
24/11/24 09:12:25 INFO YarnScheduler: Killing all running tasks in stage 0: Stage finished
24/11/24 09:12:25 INFO DAGScheduler: Job 0 finished: showString at NativeMethodAccessorImpl.java:0, took 2.235024 s
24/11/24 09:12:25 INFO CodeGenerator: Code generated in 60.81833 ms
+-----------------+-------+------------------+----------------------------+-------------------+-------------------------+----------------+----+------------------+----+-------------+--------------+------+---------------+--------------------+----------+---------------------+------------+-----------------+------------------+-----------------+------------------+-------------------+
|fecha_reporte_web|id_caso|fecha_notificacion|codigo_divipola_departamento|nombre_departamento|codigo_divipola_municipio|nombre_municipio|edad|unidad_medida_edad|sexo|tipo_contagio|ubicacion_caso|estado|codigo_iso_pais|         nombre_pais|recuperado|fecha_inicio_sintomas|fecha_muerte|fecha_diagnostico|fecha_recuperacion|tipo_recuperacion|pertenencia_etnica|nombre_grupo_etnico|
+-----------------+-------+------------------+----------------------------+-------------------+-------------------------+----------------+----+------------------+----+-------------+--------------+------+---------------+--------------------+----------+---------------------+------------+-----------------+------------------+-----------------+------------------+-------------------+
| 6/3/2020 0:00:00|      1|  2/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  19|                 1|   F|    Importado|          Casa|  Leve|            380|              ITALIA|Recuperado|    27/2/2020 0:00:00|            | 6/3/2020 0:00:00| 13/3/2020 0:00:00|              PCR|                 6|                   |
| 9/3/2020 0:00:00|      2|  6/3/2020 0:00:00|                          76|              VALLE|                    76111|            BUGA|  34|                 1|   M|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     4/3/2020 0:00:00|            | 9/3/2020 0:00:00| 19/3/2020 0:00:00|              PCR|                 5|                   |
| 9/3/2020 0:00:00|      3|  7/3/2020 0:00:00|                           5|          ANTIOQUIA|                     5001|        MEDELLIN|  50|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|    29/2/2020 0:00:00|            | 9/3/2020 0:00:00| 15/3/2020 0:00:00|              PCR|                 6|                   |
|11/3/2020 0:00:00|      4|  9/3/2020 0:00:00|                           5|          ANTIOQUIA|                     5001|        MEDELLIN|  55|                 1|   M|  Relacionado|          Casa|  Leve|               |                    |Recuperado|     6/3/2020 0:00:00|            |11/3/2020 0:00:00| 26/3/2020 0:00:00|              PCR|                 6|                   |
|11/3/2020 0:00:00|      5|  9/3/2020 0:00:00|                           5|          ANTIOQUIA|                     5001|        MEDELLIN|  25|                 1|   M|  Relacionado|          Casa|  Leve|               |                    |Recuperado|     8/3/2020 0:00:00|            |11/3/2020 0:00:00| 23/3/2020 0:00:00|              PCR|                 6|                   |
|11/3/2020 0:00:00|      6| 10/3/2020 0:00:00|                           5|          ANTIOQUIA|                     5360|          ITAGUI|  27|                 1|   F|  Relacionado|          Casa|  Leve|               |                    |Recuperado|     6/3/2020 0:00:00|            |11/3/2020 0:00:00| 26/3/2020 0:00:00|              PCR|                 6|                   |
|11/3/2020 0:00:00|      7|  8/3/2020 0:00:00|                       13001|          CARTAGENA|                    13001|       CARTAGENA|  85|                 1|   F|    Importado|          Casa|  Leve|            840|ESTADOS UNIDOS DE...|Recuperado|     2/3/2020 0:00:00|            |11/3/2020 0:00:00| 17/3/2020 0:00:00|              PCR|                 6|                   |
|11/3/2020 0:00:00|      8|  9/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  22|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     6/3/2020 0:00:00|            |11/3/2020 0:00:00| 21/3/2020 0:00:00|              PCR|                 6|                   |
|11/3/2020 0:00:00|      9|  8/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  28|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     7/3/2020 0:00:00|            |11/3/2020 0:00:00| 23/3/2020 0:00:00|              PCR|                 6|                   |
|12/3/2020 0:00:00|     10| 12/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  36|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     6/3/2020 0:00:00|            |12/3/2020 0:00:00| 21/3/2020 0:00:00|              PCR|                 6|                   |
|12/3/2020 0:00:00|     11| 11/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  42|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     6/3/2020 0:00:00|            |12/3/2020 0:00:00| 31/3/2020 0:00:00|              PCR|                 6|                   |
|12/3/2020 0:00:00|     12| 10/3/2020 0:00:00|                          41|              HUILA|                    41001|           NEIVA|  74|                 1|   F|    Importado|          Casa|  Leve|            380|              ITALIA|Recuperado|     6/3/2020 0:00:00|            |12/3/2020 0:00:00|  9/4/2020 0:00:00|              PCR|                 6|                   |
|12/3/2020 0:00:00|     13| 10/3/2020 0:00:00|                          41|              HUILA|                    41001|           NEIVA|  68|                 1|   F|  Relacionado|          Casa|  Leve|               |                    |Recuperado|     6/3/2020 0:00:00|            |12/3/2020 0:00:00| 30/3/2020 0:00:00|              PCR|                 6|                   |
|13/3/2020 0:00:00|     14| 10/3/2020 0:00:00|                          76|              VALLE|                    76520|         PALMIRA|  48|                 1|   M|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     7/3/2020 0:00:00|            |13/3/2020 0:00:00| 21/3/2020 0:00:00|              PCR|                 5|                   |
|13/3/2020 0:00:00|     15| 13/3/2020 0:00:00|                          50|               META|                    50001|   VILLAVICENCIO|  30|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     9/3/2020 0:00:00|            |13/3/2020 0:00:00| 23/3/2020 0:00:00|              PCR|                 6|                   |
|13/3/2020 0:00:00|     16| 11/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  61|                 1|   F|    Importado|          Casa|  Leve|            380|              ITALIA|Recuperado|     8/3/2020 0:00:00|            |13/3/2020 0:00:00| 23/3/2020 0:00:00|              PCR|                 5|                   |
|14/3/2020 0:00:00|     17| 11/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  73|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|    28/2/2020 0:00:00|            |14/3/2020 0:00:00| 14/3/2020 0:00:00|              PCR|                 6|                   |
|14/3/2020 0:00:00|     18| 11/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  54|                 1|   M|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     7/3/2020 0:00:00|            |14/3/2020 0:00:00|  7/4/2020 0:00:00|           Tiempo|                 6|                   |
|14/3/2020 0:00:00|     19| 12/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  54|                 1|   F|  Relacionado|          Casa|  Leve|               |                    |Recuperado|     9/3/2020 0:00:00|            |14/3/2020 0:00:00| 24/3/2020 0:00:00|              PCR|                 6|                   |
|14/3/2020 0:00:00|     20| 11/3/2020 0:00:00|                           5|          ANTIOQUIA|                     5001|        MEDELLIN|  26|                 1|   F|  Relacionado|          Casa|  Leve|               |                    |Recuperado|     9/3/2020 0:00:00|            |14/3/2020 0:00:00| 24/3/2020 0:00:00|              PCR|                 6|                   |
+-----------------+-------+------------------+----------------------------+-------------------+-------------------------+----------------+----+------------------+----+-------------+--------------+------+---------------+--------------------+----------+---------------------+------------+-----------------+------------------+-----------------+------------------+-------------------+
only showing top 20 rows

24/11/24 09:12:25 INFO BlockManagerInfo: Removed broadcast_0_piece0 on ip-172-31-84-208.ec2.internal:40453 in memory (size: 7.2 KiB, free: 912.3 MiB)
24/11/24 09:12:25 INFO BlockManagerInfo: Removed broadcast_0_piece0 on ip-172-31-90-72.ec2.internal:38803 in memory (size: 7.2 KiB, free: 4.8 GiB)
24/11/24 09:12:26 INFO ClientConfigurationFactory: Set initial getObject socket timeout to 2000 ms.
24/11/24 09:12:27 INFO ParquetUtils: Using user defined output committer for Parquet: com.amazon.emr.committer.EmrOptimizedSparkSqlParquetOutputCommitter
24/11/24 09:12:27 INFO SQLConfCommitterProvider: Getting user defined output committer class com.amazon.emr.committer.EmrOptimizedSparkSqlParquetOutputCommitter
24/11/24 09:12:27 INFO EmrOptimizedParquetOutputCommitter: EMR Optimized Committer: ENABLED
24/11/24 09:12:27 INFO EmrOptimizedParquetOutputCommitter: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileSystemOptimizedCommitter
24/11/24 09:12:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
24/11/24 09:12:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: true
24/11/24 09:12:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
24/11/24 09:12:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: true
24/11/24 09:12:27 INFO SQLConfCommitterProvider: Using output committer class com.amazon.emr.committer.EmrOptimizedSparkSqlParquetOutputCommitter
24/11/24 09:12:27 INFO FileSystemOptimizedCommitter: Nothing to setup as successful task attempt outputs are written directly
24/11/24 09:12:27 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
24/11/24 09:12:27 INFO DAGScheduler: Got job 1 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/11/24 09:12:27 INFO DAGScheduler: Final stage: ResultStage 1 (parquet at NativeMethodAccessorImpl.java:0)
24/11/24 09:12:27 INFO DAGScheduler: Parents of final stage: List()
24/11/24 09:12:27 INFO DAGScheduler: Missing parents: List()
24/11/24 09:12:27 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
24/11/24 09:12:27 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 261.7 KiB, free 912.0 MiB)
24/11/24 09:12:27 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 97.1 KiB, free 911.9 MiB)
24/11/24 09:12:27 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on ip-172-31-84-208.ec2.internal:40453 (size: 97.1 KiB, free: 912.2 MiB)
24/11/24 09:12:27 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1592
24/11/24 09:12:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/11/24 09:12:27 INFO YarnScheduler: Adding task set 1.0 with 1 tasks resource profile 0
24/11/24 09:12:27 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (ip-172-31-80-66.ec2.internal, executor 1, partition 0, PROCESS_LOCAL, 7220 bytes) 
24/11/24 09:12:27 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on ip-172-31-80-66.ec2.internal:45145 (size: 97.1 KiB, free: 4.8 GiB)
24/11/24 09:12:31 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 3844 ms on ip-172-31-80-66.ec2.internal (executor 1) (1/1)
24/11/24 09:12:31 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
24/11/24 09:12:31 INFO DAGScheduler: ResultStage 1 (parquet at NativeMethodAccessorImpl.java:0) finished in 3.924 s
24/11/24 09:12:31 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
24/11/24 09:12:31 INFO YarnScheduler: Killing all running tasks in stage 1: Stage finished
24/11/24 09:12:31 INFO DAGScheduler: Job 1 finished: parquet at NativeMethodAccessorImpl.java:0, took 3.929453 s
24/11/24 09:12:31 INFO FileFormatWriter: Start to commit write Job be88b8bb-091e-4ecd-a753-574ed1f2ba2c.
24/11/24 09:12:31 INFO MultipartUploadOutputStream: close closed:false s3://p3-tet/raw/_SUCCESS
24/11/24 09:12:31 INFO FileFormatWriter: Write Job be88b8bb-091e-4ecd-a753-574ed1f2ba2c committed. Elapsed time: 87 ms.
24/11/24 09:12:31 INFO FileFormatWriter: Finished processing stats for write job be88b8bb-091e-4ecd-a753-574ed1f2ba2c.
Datos extrados y guardados en S3 en la zona raw.
24/11/24 09:12:31 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/11/24 09:12:31 INFO SparkUI: Stopped Spark web UI at http://ip-172-31-84-208.ec2.internal:4040
24/11/24 09:12:31 INFO YarnClientSchedulerBackend: Interrupting monitor thread
24/11/24 09:12:31 INFO YarnClientSchedulerBackend: Shutting down all executors
24/11/24 09:12:31 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
24/11/24 09:12:31 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
24/11/24 09:12:31 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/11/24 09:12:31 INFO MemoryStore: MemoryStore cleared
24/11/24 09:12:31 INFO BlockManager: BlockManager stopped
24/11/24 09:12:31 INFO BlockManagerMaster: BlockManagerMaster stopped
24/11/24 09:12:31 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/11/24 09:12:31 INFO SparkContext: Successfully stopped SparkContext
24/11/24 09:12:32 INFO ShutdownHookManager: Shutdown hook called
24/11/24 09:12:32 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-68cc12ee-c23e-4d89-b394-b7c08bc44b87
24/11/24 09:12:32 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-1df5acf0-6d62-4b65-b4ca-8982724d6df9/pyspark-a17c03b9-d2b1-4e10-bd49-bf83bb99702c
24/11/24 09:12:32 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-1df5acf0-6d62-4b65-b4ca-8982724d6df9
24/11/24 09:13:04 INFO SparkContext: Running Spark version 3.4.1-amzn-1
24/11/24 09:13:04 INFO ResourceUtils: ==============================================================
24/11/24 09:13:04 INFO ResourceUtils: No custom resources configured for spark.driver.
24/11/24 09:13:04 INFO ResourceUtils: ==============================================================
24/11/24 09:13:04 INFO SparkContext: Submitted application: Extract_DB_to_S3
24/11/24 09:13:04 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 4, script: , vendor: , memory -> name: memory, amount: 9486, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/11/24 09:13:04 INFO ResourceProfile: Limiting resource is cpus at 4 tasks per executor
24/11/24 09:13:04 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/11/24 09:13:04 INFO SecurityManager: Changing view acls to: hadoop
24/11/24 09:13:04 INFO SecurityManager: Changing modify acls to: hadoop
24/11/24 09:13:04 INFO SecurityManager: Changing view acls groups to: 
24/11/24 09:13:04 INFO SecurityManager: Changing modify acls groups to: 
24/11/24 09:13:04 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: hadoop; groups with view permissions: EMPTY; users with modify permissions: hadoop; groups with modify permissions: EMPTY
24/11/24 09:13:05 INFO Utils: Successfully started service 'sparkDriver' on port 44607.
24/11/24 09:13:05 INFO SparkEnv: Registering MapOutputTracker
24/11/24 09:13:05 INFO SparkEnv: Registering BlockManagerMaster
24/11/24 09:13:05 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/11/24 09:13:05 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/11/24 09:13:05 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/11/24 09:13:05 INFO DiskBlockManager: Created local directory at /mnt/tmp/blockmgr-d689d098-44c6-403e-9063-47f49c8447a6
24/11/24 09:13:05 INFO MemoryStore: MemoryStore started with capacity 912.3 MiB
24/11/24 09:13:05 INFO SparkEnv: Registering OutputCommitCoordinator
24/11/24 09:13:05 INFO SubResultCacheManager: Sub-result caches are disabled.
24/11/24 09:13:05 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/11/24 09:13:05 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/11/24 09:13:05 INFO SparkContext: Added JAR /home/hadoop/mysql-connector-j-9.1.0.jar at spark://ip-172-31-84-208.ec2.internal:44607/jars/mysql-connector-j-9.1.0.jar with timestamp 1732439584734
24/11/24 09:13:05 INFO Utils: Using 50 preallocated executors (minExecutors: 0). Set spark.dynamicAllocation.preallocateExecutors to `false` disable executor preallocation.
24/11/24 09:13:05 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at ip-172-31-84-208.ec2.internal/172.31.84.208:8032
24/11/24 09:13:06 INFO Configuration: resource-types.xml not found
24/11/24 09:13:06 INFO ResourceUtils: Unable to find 'resource-types.xml'.
24/11/24 09:13:06 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (12288 MB per container)
24/11/24 09:13:06 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
24/11/24 09:13:06 INFO Client: Setting up container launch context for our AM
24/11/24 09:13:06 INFO Client: Setting up the launch environment for our AM container
24/11/24 09:13:06 INFO Client: Preparing resources for our AM container
24/11/24 09:13:06 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
24/11/24 09:13:09 INFO Client: Uploading resource file:/mnt/tmp/spark-7042da50-aa59-4faf-8e67-b1b8d7344796/__spark_libs__5207228847608277462.zip -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0023/__spark_libs__5207228847608277462.zip
24/11/24 09:13:10 INFO Client: Uploading resource file:/home/hadoop/mysql-connector-j-9.1.0.jar -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0023/mysql-connector-j-9.1.0.jar
24/11/24 09:13:10 INFO Client: Uploading resource file:/etc/spark/conf.dist/hive-site.xml -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0023/hive-site.xml
24/11/24 09:13:10 INFO Client: Uploading resource file:/etc/hudi/conf.dist/hudi-defaults.conf -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0023/hudi-defaults.conf
24/11/24 09:13:10 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0023/pyspark.zip
24/11/24 09:13:10 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0023/py4j-0.10.9.7-src.zip
24/11/24 09:13:10 INFO Client: Uploading resource file:/mnt/tmp/spark-7042da50-aa59-4faf-8e67-b1b8d7344796/__spark_conf__8943232535800906072.zip -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0023/__spark_conf__.zip
24/11/24 09:13:10 INFO SecurityManager: Changing view acls to: hadoop
24/11/24 09:13:10 INFO SecurityManager: Changing modify acls to: hadoop
24/11/24 09:13:10 INFO SecurityManager: Changing view acls groups to: 
24/11/24 09:13:10 INFO SecurityManager: Changing modify acls groups to: 
24/11/24 09:13:10 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: hadoop; groups with view permissions: EMPTY; users with modify permissions: hadoop; groups with modify permissions: EMPTY
24/11/24 09:13:10 INFO Client: Submitting application application_1732429252608_0023 to ResourceManager
24/11/24 09:13:10 INFO YarnClientImpl: Submitted application application_1732429252608_0023
24/11/24 09:13:11 INFO Client: Application report for application_1732429252608_0023 (state: ACCEPTED)
24/11/24 09:13:11 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1732439590677
	 final status: UNDEFINED
	 tracking URL: http://ip-172-31-84-208.ec2.internal:20888/proxy/application_1732429252608_0023/
	 user: hadoop
24/11/24 09:13:12 INFO Client: Application report for application_1732429252608_0023 (state: ACCEPTED)
24/11/24 09:13:13 INFO Client: Application report for application_1732429252608_0023 (state: ACCEPTED)
24/11/24 09:13:14 INFO Client: Application report for application_1732429252608_0023 (state: ACCEPTED)
24/11/24 09:13:15 INFO Client: Application report for application_1732429252608_0023 (state: RUNNING)
24/11/24 09:13:15 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 172.31.90.72
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1732439590677
	 final status: UNDEFINED
	 tracking URL: http://ip-172-31-84-208.ec2.internal:20888/proxy/application_1732429252608_0023/
	 user: hadoop
24/11/24 09:13:15 INFO YarnClientSchedulerBackend: Application application_1732429252608_0023 has started running.
24/11/24 09:13:15 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40009.
24/11/24 09:13:15 INFO NettyBlockTransferService: Server created on ip-172-31-84-208.ec2.internal:40009
24/11/24 09:13:15 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/11/24 09:13:15 INFO BlockManager: external shuffle service port = 7337
24/11/24 09:13:15 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ip-172-31-84-208.ec2.internal, 40009, None)
24/11/24 09:13:15 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-84-208.ec2.internal:40009 with 912.3 MiB RAM, BlockManagerId(driver, ip-172-31-84-208.ec2.internal, 40009, None)
24/11/24 09:13:15 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ip-172-31-84-208.ec2.internal, 40009, None)
24/11/24 09:13:15 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ip-172-31-84-208.ec2.internal, 40009, None)
24/11/24 09:13:15 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> ip-172-31-84-208.ec2.internal, PROXY_URI_BASES -> http://ip-172-31-84-208.ec2.internal:20888/proxy/application_1732429252608_0023), /proxy/application_1732429252608_0023
24/11/24 09:13:15 INFO SingleEventLogFileWriter: Logging events to hdfs:/var/log/spark/apps/application_1732429252608_0023.inprogress
24/11/24 09:13:16 INFO Utils: Using 50 preallocated executors (minExecutors: 0). Set spark.dynamicAllocation.preallocateExecutors to `false` disable executor preallocation.
24/11/24 09:13:16 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to request executors before the AM has registered!
24/11/24 09:13:16 INFO ServerInfo: Adding filter to /jobs: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:13:16 INFO ServerInfo: Adding filter to /jobs/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:13:16 INFO ServerInfo: Adding filter to /jobs/job: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:13:16 INFO ServerInfo: Adding filter to /jobs/job/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:13:16 INFO ServerInfo: Adding filter to /stages: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:13:16 INFO ServerInfo: Adding filter to /stages/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:13:16 INFO ServerInfo: Adding filter to /stages/stage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:13:16 INFO ServerInfo: Adding filter to /stages/stage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:13:16 INFO ServerInfo: Adding filter to /stages/pool: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:13:16 INFO ServerInfo: Adding filter to /stages/pool/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:13:16 INFO ServerInfo: Adding filter to /storage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:13:16 INFO ServerInfo: Adding filter to /storage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:13:16 INFO ServerInfo: Adding filter to /storage/rdd: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:13:16 INFO ServerInfo: Adding filter to /storage/rdd/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:13:16 INFO ServerInfo: Adding filter to /environment: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:13:16 INFO ServerInfo: Adding filter to /environment/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:13:16 INFO ServerInfo: Adding filter to /executors: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:13:16 INFO ServerInfo: Adding filter to /executors/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:13:16 INFO ServerInfo: Adding filter to /executors/threadDump: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:13:16 INFO ServerInfo: Adding filter to /executors/threadDump/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:13:16 INFO ServerInfo: Adding filter to /static: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:13:16 INFO ServerInfo: Adding filter to /: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:13:16 INFO ServerInfo: Adding filter to /api: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:13:16 INFO ServerInfo: Adding filter to /jobs/job/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:13:16 INFO ServerInfo: Adding filter to /stages/stage/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:13:16 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
24/11/24 09:13:16 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:13:16 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
24/11/24 09:13:16 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
24/11/24 09:13:16 INFO SharedState: Warehouse path is 'hdfs://ip-172-31-84-208.ec2.internal:8020/user/spark/warehouse'.
24/11/24 09:13:16 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:13:16 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:13:16 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:13:16 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:13:16 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:13:20 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.31.90.72:45898) with ID 2,  ResourceProfileId 0
24/11/24 09:13:20 INFO ExecutorMonitor: New executor 2 has registered (new total is 1)
24/11/24 09:13:20 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-90-72.ec2.internal:33201 with 4.8 GiB RAM, BlockManagerId(2, ip-172-31-90-72.ec2.internal, 33201, None)
Datos extrados de la base de datos:
24/11/24 09:13:21 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.31.80.66:54380) with ID 1,  ResourceProfileId 0
24/11/24 09:13:21 INFO ExecutorMonitor: New executor 1 has registered (new total is 2)
24/11/24 09:13:21 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-80-66.ec2.internal:41123 with 4.8 GiB RAM, BlockManagerId(1, ip-172-31-80-66.ec2.internal, 41123, None)
24/11/24 09:13:21 INFO CodeGenerator: Code generated in 276.40389 ms
24/11/24 09:13:21 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
24/11/24 09:13:21 INFO DAGScheduler: Got job 0 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/11/24 09:13:21 INFO DAGScheduler: Final stage: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0)
24/11/24 09:13:21 INFO DAGScheduler: Parents of final stage: List()
24/11/24 09:13:21 INFO DAGScheduler: Missing parents: List()
24/11/24 09:13:21 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
24/11/24 09:13:22 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 17.9 KiB, free 912.3 MiB)
24/11/24 09:13:22 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 912.3 MiB)
24/11/24 09:13:22 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ip-172-31-84-208.ec2.internal:40009 (size: 7.2 KiB, free: 912.3 MiB)
24/11/24 09:13:22 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1592
24/11/24 09:13:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/11/24 09:13:22 INFO YarnScheduler: Adding task set 0.0 with 1 tasks resource profile 0
24/11/24 09:13:22 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (ip-172-31-90-72.ec2.internal, executor 2, partition 0, PROCESS_LOCAL, 7220 bytes) 
24/11/24 09:13:22 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ip-172-31-90-72.ec2.internal:33201 (size: 7.2 KiB, free: 4.8 GiB)
24/11/24 09:13:24 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1988 ms on ip-172-31-90-72.ec2.internal (executor 2) (1/1)
24/11/24 09:13:24 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/11/24 09:13:24 INFO DAGScheduler: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0) finished in 2.196 s
24/11/24 09:13:24 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
24/11/24 09:13:24 INFO YarnScheduler: Killing all running tasks in stage 0: Stage finished
24/11/24 09:13:24 INFO DAGScheduler: Job 0 finished: showString at NativeMethodAccessorImpl.java:0, took 2.246075 s
24/11/24 09:13:24 INFO CodeGenerator: Code generated in 62.665397 ms
+-----------------+-------+------------------+----------------------------+-------------------+-------------------------+----------------+----+------------------+----+-------------+--------------+------+---------------+--------------------+----------+---------------------+------------+-----------------+------------------+-----------------+------------------+-------------------+
|fecha_reporte_web|id_caso|fecha_notificacion|codigo_divipola_departamento|nombre_departamento|codigo_divipola_municipio|nombre_municipio|edad|unidad_medida_edad|sexo|tipo_contagio|ubicacion_caso|estado|codigo_iso_pais|         nombre_pais|recuperado|fecha_inicio_sintomas|fecha_muerte|fecha_diagnostico|fecha_recuperacion|tipo_recuperacion|pertenencia_etnica|nombre_grupo_etnico|
+-----------------+-------+------------------+----------------------------+-------------------+-------------------------+----------------+----+------------------+----+-------------+--------------+------+---------------+--------------------+----------+---------------------+------------+-----------------+------------------+-----------------+------------------+-------------------+
| 6/3/2020 0:00:00|      1|  2/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  19|                 1|   F|    Importado|          Casa|  Leve|            380|              ITALIA|Recuperado|    27/2/2020 0:00:00|            | 6/3/2020 0:00:00| 13/3/2020 0:00:00|              PCR|                 6|                   |
| 9/3/2020 0:00:00|      2|  6/3/2020 0:00:00|                          76|              VALLE|                    76111|            BUGA|  34|                 1|   M|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     4/3/2020 0:00:00|            | 9/3/2020 0:00:00| 19/3/2020 0:00:00|              PCR|                 5|                   |
| 9/3/2020 0:00:00|      3|  7/3/2020 0:00:00|                           5|          ANTIOQUIA|                     5001|        MEDELLIN|  50|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|    29/2/2020 0:00:00|            | 9/3/2020 0:00:00| 15/3/2020 0:00:00|              PCR|                 6|                   |
|11/3/2020 0:00:00|      4|  9/3/2020 0:00:00|                           5|          ANTIOQUIA|                     5001|        MEDELLIN|  55|                 1|   M|  Relacionado|          Casa|  Leve|               |                    |Recuperado|     6/3/2020 0:00:00|            |11/3/2020 0:00:00| 26/3/2020 0:00:00|              PCR|                 6|                   |
|11/3/2020 0:00:00|      5|  9/3/2020 0:00:00|                           5|          ANTIOQUIA|                     5001|        MEDELLIN|  25|                 1|   M|  Relacionado|          Casa|  Leve|               |                    |Recuperado|     8/3/2020 0:00:00|            |11/3/2020 0:00:00| 23/3/2020 0:00:00|              PCR|                 6|                   |
|11/3/2020 0:00:00|      6| 10/3/2020 0:00:00|                           5|          ANTIOQUIA|                     5360|          ITAGUI|  27|                 1|   F|  Relacionado|          Casa|  Leve|               |                    |Recuperado|     6/3/2020 0:00:00|            |11/3/2020 0:00:00| 26/3/2020 0:00:00|              PCR|                 6|                   |
|11/3/2020 0:00:00|      7|  8/3/2020 0:00:00|                       13001|          CARTAGENA|                    13001|       CARTAGENA|  85|                 1|   F|    Importado|          Casa|  Leve|            840|ESTADOS UNIDOS DE...|Recuperado|     2/3/2020 0:00:00|            |11/3/2020 0:00:00| 17/3/2020 0:00:00|              PCR|                 6|                   |
|11/3/2020 0:00:00|      8|  9/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  22|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     6/3/2020 0:00:00|            |11/3/2020 0:00:00| 21/3/2020 0:00:00|              PCR|                 6|                   |
|11/3/2020 0:00:00|      9|  8/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  28|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     7/3/2020 0:00:00|            |11/3/2020 0:00:00| 23/3/2020 0:00:00|              PCR|                 6|                   |
|12/3/2020 0:00:00|     10| 12/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  36|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     6/3/2020 0:00:00|            |12/3/2020 0:00:00| 21/3/2020 0:00:00|              PCR|                 6|                   |
|12/3/2020 0:00:00|     11| 11/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  42|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     6/3/2020 0:00:00|            |12/3/2020 0:00:00| 31/3/2020 0:00:00|              PCR|                 6|                   |
|12/3/2020 0:00:00|     12| 10/3/2020 0:00:00|                          41|              HUILA|                    41001|           NEIVA|  74|                 1|   F|    Importado|          Casa|  Leve|            380|              ITALIA|Recuperado|     6/3/2020 0:00:00|            |12/3/2020 0:00:00|  9/4/2020 0:00:00|              PCR|                 6|                   |
|12/3/2020 0:00:00|     13| 10/3/2020 0:00:00|                          41|              HUILA|                    41001|           NEIVA|  68|                 1|   F|  Relacionado|          Casa|  Leve|               |                    |Recuperado|     6/3/2020 0:00:00|            |12/3/2020 0:00:00| 30/3/2020 0:00:00|              PCR|                 6|                   |
|13/3/2020 0:00:00|     14| 10/3/2020 0:00:00|                          76|              VALLE|                    76520|         PALMIRA|  48|                 1|   M|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     7/3/2020 0:00:00|            |13/3/2020 0:00:00| 21/3/2020 0:00:00|              PCR|                 5|                   |
|13/3/2020 0:00:00|     15| 13/3/2020 0:00:00|                          50|               META|                    50001|   VILLAVICENCIO|  30|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     9/3/2020 0:00:00|            |13/3/2020 0:00:00| 23/3/2020 0:00:00|              PCR|                 6|                   |
|13/3/2020 0:00:00|     16| 11/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  61|                 1|   F|    Importado|          Casa|  Leve|            380|              ITALIA|Recuperado|     8/3/2020 0:00:00|            |13/3/2020 0:00:00| 23/3/2020 0:00:00|              PCR|                 5|                   |
|14/3/2020 0:00:00|     17| 11/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  73|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|    28/2/2020 0:00:00|            |14/3/2020 0:00:00| 14/3/2020 0:00:00|              PCR|                 6|                   |
|14/3/2020 0:00:00|     18| 11/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  54|                 1|   M|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     7/3/2020 0:00:00|            |14/3/2020 0:00:00|  7/4/2020 0:00:00|           Tiempo|                 6|                   |
|14/3/2020 0:00:00|     19| 12/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  54|                 1|   F|  Relacionado|          Casa|  Leve|               |                    |Recuperado|     9/3/2020 0:00:00|            |14/3/2020 0:00:00| 24/3/2020 0:00:00|              PCR|                 6|                   |
|14/3/2020 0:00:00|     20| 11/3/2020 0:00:00|                           5|          ANTIOQUIA|                     5001|        MEDELLIN|  26|                 1|   F|  Relacionado|          Casa|  Leve|               |                    |Recuperado|     9/3/2020 0:00:00|            |14/3/2020 0:00:00| 24/3/2020 0:00:00|              PCR|                 6|                   |
+-----------------+-------+------------------+----------------------------+-------------------+-------------------------+----------------+----+------------------+----+-------------+--------------+------+---------------+--------------------+----------+---------------------+------------+-----------------+------------------+-----------------+------------------+-------------------+
only showing top 20 rows

24/11/24 09:13:25 INFO BlockManagerInfo: Removed broadcast_0_piece0 on ip-172-31-84-208.ec2.internal:40009 in memory (size: 7.2 KiB, free: 912.3 MiB)
24/11/24 09:13:25 INFO BlockManagerInfo: Removed broadcast_0_piece0 on ip-172-31-90-72.ec2.internal:33201 in memory (size: 7.2 KiB, free: 4.8 GiB)
24/11/24 09:13:25 INFO ClientConfigurationFactory: Set initial getObject socket timeout to 2000 ms.
24/11/24 09:13:26 INFO ParquetUtils: Using user defined output committer for Parquet: com.amazon.emr.committer.EmrOptimizedSparkSqlParquetOutputCommitter
24/11/24 09:13:26 INFO SQLConfCommitterProvider: Getting user defined output committer class com.amazon.emr.committer.EmrOptimizedSparkSqlParquetOutputCommitter
24/11/24 09:13:26 INFO EmrOptimizedParquetOutputCommitter: EMR Optimized Committer: ENABLED
24/11/24 09:13:26 INFO EmrOptimizedParquetOutputCommitter: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileSystemOptimizedCommitter
24/11/24 09:13:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
24/11/24 09:13:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: true
24/11/24 09:13:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
24/11/24 09:13:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: true
24/11/24 09:13:26 INFO SQLConfCommitterProvider: Using output committer class com.amazon.emr.committer.EmrOptimizedSparkSqlParquetOutputCommitter
24/11/24 09:13:26 INFO FileSystemOptimizedCommitter: Nothing to setup as successful task attempt outputs are written directly
24/11/24 09:13:26 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
24/11/24 09:13:26 INFO DAGScheduler: Got job 1 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/11/24 09:13:26 INFO DAGScheduler: Final stage: ResultStage 1 (parquet at NativeMethodAccessorImpl.java:0)
24/11/24 09:13:26 INFO DAGScheduler: Parents of final stage: List()
24/11/24 09:13:26 INFO DAGScheduler: Missing parents: List()
24/11/24 09:13:26 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
24/11/24 09:13:26 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 261.7 KiB, free 912.0 MiB)
24/11/24 09:13:26 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 97.1 KiB, free 911.9 MiB)
24/11/24 09:13:26 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on ip-172-31-84-208.ec2.internal:40009 (size: 97.1 KiB, free: 912.2 MiB)
24/11/24 09:13:26 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1592
24/11/24 09:13:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/11/24 09:13:26 INFO YarnScheduler: Adding task set 1.0 with 1 tasks resource profile 0
24/11/24 09:13:26 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (ip-172-31-80-66.ec2.internal, executor 1, partition 0, PROCESS_LOCAL, 7220 bytes) 
24/11/24 09:13:27 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on ip-172-31-80-66.ec2.internal:41123 (size: 97.1 KiB, free: 4.8 GiB)
24/11/24 09:13:30 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 3745 ms on ip-172-31-80-66.ec2.internal (executor 1) (1/1)
24/11/24 09:13:30 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
24/11/24 09:13:30 INFO DAGScheduler: ResultStage 1 (parquet at NativeMethodAccessorImpl.java:0) finished in 3.834 s
24/11/24 09:13:30 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
24/11/24 09:13:30 INFO YarnScheduler: Killing all running tasks in stage 1: Stage finished
24/11/24 09:13:30 INFO DAGScheduler: Job 1 finished: parquet at NativeMethodAccessorImpl.java:0, took 3.839549 s
24/11/24 09:13:30 INFO FileFormatWriter: Start to commit write Job 7557c7a0-ac25-49ae-ac68-1b3a8dfa026a.
24/11/24 09:13:30 INFO MultipartUploadOutputStream: close closed:false s3://p3-tet/raw/_SUCCESS
24/11/24 09:13:30 INFO FileFormatWriter: Write Job 7557c7a0-ac25-49ae-ac68-1b3a8dfa026a committed. Elapsed time: 84 ms.
24/11/24 09:13:30 INFO FileFormatWriter: Finished processing stats for write job 7557c7a0-ac25-49ae-ac68-1b3a8dfa026a.
Datos extrados y guardados en S3 en la zona raw.
24/11/24 09:13:30 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/11/24 09:13:30 INFO SparkUI: Stopped Spark web UI at http://ip-172-31-84-208.ec2.internal:4040
24/11/24 09:13:30 INFO YarnClientSchedulerBackend: Interrupting monitor thread
24/11/24 09:13:30 INFO YarnClientSchedulerBackend: Shutting down all executors
24/11/24 09:13:30 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
24/11/24 09:13:30 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
24/11/24 09:13:30 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/11/24 09:13:30 INFO MemoryStore: MemoryStore cleared
24/11/24 09:13:30 INFO BlockManager: BlockManager stopped
24/11/24 09:13:30 INFO BlockManagerMaster: BlockManagerMaster stopped
24/11/24 09:13:30 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/11/24 09:13:30 INFO SparkContext: Successfully stopped SparkContext
24/11/24 09:13:31 INFO ShutdownHookManager: Shutdown hook called
24/11/24 09:13:31 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-3109681a-9d87-44e4-973a-8b267ebf8433
24/11/24 09:13:31 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-7042da50-aa59-4faf-8e67-b1b8d7344796/pyspark-b9002311-91bc-466a-ada3-af46fafc3dfe
24/11/24 09:13:31 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-7042da50-aa59-4faf-8e67-b1b8d7344796
24/11/24 09:14:04 INFO SparkContext: Running Spark version 3.4.1-amzn-1
24/11/24 09:14:04 INFO ResourceUtils: ==============================================================
24/11/24 09:14:04 INFO ResourceUtils: No custom resources configured for spark.driver.
24/11/24 09:14:04 INFO ResourceUtils: ==============================================================
24/11/24 09:14:04 INFO SparkContext: Submitted application: Extract_DB_to_S3
24/11/24 09:14:04 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 4, script: , vendor: , memory -> name: memory, amount: 9486, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/11/24 09:14:04 INFO ResourceProfile: Limiting resource is cpus at 4 tasks per executor
24/11/24 09:14:04 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/11/24 09:14:04 INFO SecurityManager: Changing view acls to: hadoop
24/11/24 09:14:04 INFO SecurityManager: Changing modify acls to: hadoop
24/11/24 09:14:04 INFO SecurityManager: Changing view acls groups to: 
24/11/24 09:14:04 INFO SecurityManager: Changing modify acls groups to: 
24/11/24 09:14:04 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: hadoop; groups with view permissions: EMPTY; users with modify permissions: hadoop; groups with modify permissions: EMPTY
24/11/24 09:14:05 INFO Utils: Successfully started service 'sparkDriver' on port 42301.
24/11/24 09:14:05 INFO SparkEnv: Registering MapOutputTracker
24/11/24 09:14:05 INFO SparkEnv: Registering BlockManagerMaster
24/11/24 09:14:05 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/11/24 09:14:05 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/11/24 09:14:05 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/11/24 09:14:05 INFO DiskBlockManager: Created local directory at /mnt/tmp/blockmgr-1a182810-4e69-4b41-be38-77747b3c78bb
24/11/24 09:14:05 INFO MemoryStore: MemoryStore started with capacity 912.3 MiB
24/11/24 09:14:05 INFO SparkEnv: Registering OutputCommitCoordinator
24/11/24 09:14:05 INFO SubResultCacheManager: Sub-result caches are disabled.
24/11/24 09:14:05 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/11/24 09:14:05 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/11/24 09:14:05 INFO SparkContext: Added JAR /home/hadoop/mysql-connector-j-9.1.0.jar at spark://ip-172-31-84-208.ec2.internal:42301/jars/mysql-connector-j-9.1.0.jar with timestamp 1732439644695
24/11/24 09:14:05 INFO Utils: Using 50 preallocated executors (minExecutors: 0). Set spark.dynamicAllocation.preallocateExecutors to `false` disable executor preallocation.
24/11/24 09:14:05 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at ip-172-31-84-208.ec2.internal/172.31.84.208:8032
24/11/24 09:14:06 INFO Configuration: resource-types.xml not found
24/11/24 09:14:06 INFO ResourceUtils: Unable to find 'resource-types.xml'.
24/11/24 09:14:06 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (12288 MB per container)
24/11/24 09:14:06 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
24/11/24 09:14:06 INFO Client: Setting up container launch context for our AM
24/11/24 09:14:06 INFO Client: Setting up the launch environment for our AM container
24/11/24 09:14:06 INFO Client: Preparing resources for our AM container
24/11/24 09:14:06 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
24/11/24 09:14:09 INFO Client: Uploading resource file:/mnt/tmp/spark-c80a3da2-4f48-42b7-b737-cba62417afe2/__spark_libs__3986357716779736779.zip -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0024/__spark_libs__3986357716779736779.zip
24/11/24 09:14:10 INFO Client: Uploading resource file:/home/hadoop/mysql-connector-j-9.1.0.jar -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0024/mysql-connector-j-9.1.0.jar
24/11/24 09:14:10 INFO Client: Uploading resource file:/etc/spark/conf.dist/hive-site.xml -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0024/hive-site.xml
24/11/24 09:14:10 INFO Client: Uploading resource file:/etc/hudi/conf.dist/hudi-defaults.conf -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0024/hudi-defaults.conf
24/11/24 09:14:10 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0024/pyspark.zip
24/11/24 09:14:10 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0024/py4j-0.10.9.7-src.zip
24/11/24 09:14:10 INFO Client: Uploading resource file:/mnt/tmp/spark-c80a3da2-4f48-42b7-b737-cba62417afe2/__spark_conf__5736320539457324156.zip -> hdfs://ip-172-31-84-208.ec2.internal:8020/user/hadoop/.sparkStaging/application_1732429252608_0024/__spark_conf__.zip
24/11/24 09:14:10 INFO SecurityManager: Changing view acls to: hadoop
24/11/24 09:14:10 INFO SecurityManager: Changing modify acls to: hadoop
24/11/24 09:14:10 INFO SecurityManager: Changing view acls groups to: 
24/11/24 09:14:10 INFO SecurityManager: Changing modify acls groups to: 
24/11/24 09:14:10 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: hadoop; groups with view permissions: EMPTY; users with modify permissions: hadoop; groups with modify permissions: EMPTY
24/11/24 09:14:10 INFO Client: Submitting application application_1732429252608_0024 to ResourceManager
24/11/24 09:14:10 INFO YarnClientImpl: Submitted application application_1732429252608_0024
24/11/24 09:14:11 INFO Client: Application report for application_1732429252608_0024 (state: ACCEPTED)
24/11/24 09:14:11 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1732439650489
	 final status: UNDEFINED
	 tracking URL: http://ip-172-31-84-208.ec2.internal:20888/proxy/application_1732429252608_0024/
	 user: hadoop
24/11/24 09:14:12 INFO Client: Application report for application_1732429252608_0024 (state: ACCEPTED)
24/11/24 09:14:13 INFO Client: Application report for application_1732429252608_0024 (state: ACCEPTED)
24/11/24 09:14:14 INFO Client: Application report for application_1732429252608_0024 (state: ACCEPTED)
24/11/24 09:14:15 INFO Client: Application report for application_1732429252608_0024 (state: ACCEPTED)
24/11/24 09:14:15 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> ip-172-31-84-208.ec2.internal, PROXY_URI_BASES -> http://ip-172-31-84-208.ec2.internal:20888/proxy/application_1732429252608_0024), /proxy/application_1732429252608_0024
24/11/24 09:14:16 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
24/11/24 09:14:16 INFO Client: Application report for application_1732429252608_0024 (state: RUNNING)
24/11/24 09:14:16 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 172.31.90.72
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1732439650489
	 final status: UNDEFINED
	 tracking URL: http://ip-172-31-84-208.ec2.internal:20888/proxy/application_1732429252608_0024/
	 user: hadoop
24/11/24 09:14:16 INFO YarnClientSchedulerBackend: Application application_1732429252608_0024 has started running.
24/11/24 09:14:16 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42773.
24/11/24 09:14:16 INFO NettyBlockTransferService: Server created on ip-172-31-84-208.ec2.internal:42773
24/11/24 09:14:16 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/11/24 09:14:16 INFO BlockManager: external shuffle service port = 7337
24/11/24 09:14:16 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ip-172-31-84-208.ec2.internal, 42773, None)
24/11/24 09:14:16 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-84-208.ec2.internal:42773 with 912.3 MiB RAM, BlockManagerId(driver, ip-172-31-84-208.ec2.internal, 42773, None)
24/11/24 09:14:16 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ip-172-31-84-208.ec2.internal, 42773, None)
24/11/24 09:14:16 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ip-172-31-84-208.ec2.internal, 42773, None)
24/11/24 09:14:16 INFO SingleEventLogFileWriter: Logging events to hdfs:/var/log/spark/apps/application_1732429252608_0024.inprogress
24/11/24 09:14:16 INFO Utils: Using 50 preallocated executors (minExecutors: 0). Set spark.dynamicAllocation.preallocateExecutors to `false` disable executor preallocation.
24/11/24 09:14:16 INFO ServerInfo: Adding filter to /jobs: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:14:16 INFO ServerInfo: Adding filter to /jobs/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:14:16 INFO ServerInfo: Adding filter to /jobs/job: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:14:16 INFO ServerInfo: Adding filter to /jobs/job/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:14:16 INFO ServerInfo: Adding filter to /stages: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:14:16 INFO ServerInfo: Adding filter to /stages/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:14:16 INFO ServerInfo: Adding filter to /stages/stage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:14:16 INFO ServerInfo: Adding filter to /stages/stage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:14:16 INFO ServerInfo: Adding filter to /stages/pool: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:14:16 INFO ServerInfo: Adding filter to /stages/pool/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:14:16 INFO ServerInfo: Adding filter to /storage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:14:16 INFO ServerInfo: Adding filter to /storage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:14:16 INFO ServerInfo: Adding filter to /storage/rdd: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:14:16 INFO ServerInfo: Adding filter to /storage/rdd/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:14:16 INFO ServerInfo: Adding filter to /environment: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:14:16 INFO ServerInfo: Adding filter to /environment/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:14:16 INFO ServerInfo: Adding filter to /executors: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:14:16 INFO ServerInfo: Adding filter to /executors/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:14:16 INFO ServerInfo: Adding filter to /executors/threadDump: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:14:16 INFO ServerInfo: Adding filter to /executors/threadDump/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:14:16 INFO ServerInfo: Adding filter to /static: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:14:17 INFO ServerInfo: Adding filter to /: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:14:17 INFO ServerInfo: Adding filter to /api: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:14:17 INFO ServerInfo: Adding filter to /jobs/job/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:14:17 INFO ServerInfo: Adding filter to /stages/stage/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:14:17 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:14:17 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
24/11/24 09:14:17 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
24/11/24 09:14:17 INFO SharedState: Warehouse path is 'hdfs://ip-172-31-84-208.ec2.internal:8020/user/spark/warehouse'.
24/11/24 09:14:17 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:14:17 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:14:17 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:14:17 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:14:17 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
24/11/24 09:14:20 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.31.90.72:43546) with ID 1,  ResourceProfileId 0
24/11/24 09:14:20 INFO ExecutorMonitor: New executor 1 has registered (new total is 1)
24/11/24 09:14:20 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-90-72.ec2.internal:38949 with 4.8 GiB RAM, BlockManagerId(1, ip-172-31-90-72.ec2.internal, 38949, None)
Datos extrados de la base de datos:
24/11/24 09:14:21 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.31.80.66:56940) with ID 2,  ResourceProfileId 0
24/11/24 09:14:21 INFO ExecutorMonitor: New executor 2 has registered (new total is 2)
24/11/24 09:14:21 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-80-66.ec2.internal:40615 with 4.8 GiB RAM, BlockManagerId(2, ip-172-31-80-66.ec2.internal, 40615, None)
24/11/24 09:14:22 INFO CodeGenerator: Code generated in 252.442155 ms
24/11/24 09:14:22 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
24/11/24 09:14:22 INFO DAGScheduler: Got job 0 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/11/24 09:14:22 INFO DAGScheduler: Final stage: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0)
24/11/24 09:14:22 INFO DAGScheduler: Parents of final stage: List()
24/11/24 09:14:22 INFO DAGScheduler: Missing parents: List()
24/11/24 09:14:22 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
24/11/24 09:14:22 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 17.9 KiB, free 912.3 MiB)
24/11/24 09:14:22 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 912.3 MiB)
24/11/24 09:14:22 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ip-172-31-84-208.ec2.internal:42773 (size: 7.2 KiB, free: 912.3 MiB)
24/11/24 09:14:22 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1592
24/11/24 09:14:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/11/24 09:14:23 INFO YarnScheduler: Adding task set 0.0 with 1 tasks resource profile 0
24/11/24 09:14:23 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (ip-172-31-90-72.ec2.internal, executor 1, partition 0, PROCESS_LOCAL, 7220 bytes) 
24/11/24 09:14:23 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ip-172-31-90-72.ec2.internal:38949 (size: 7.2 KiB, free: 4.8 GiB)
24/11/24 09:14:25 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2076 ms on ip-172-31-90-72.ec2.internal (executor 1) (1/1)
24/11/24 09:14:25 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/11/24 09:14:25 INFO DAGScheduler: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0) finished in 2.333 s
24/11/24 09:14:25 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
24/11/24 09:14:25 INFO YarnScheduler: Killing all running tasks in stage 0: Stage finished
24/11/24 09:14:25 INFO DAGScheduler: Job 0 finished: showString at NativeMethodAccessorImpl.java:0, took 2.409088 s
24/11/24 09:14:25 INFO CodeGenerator: Code generated in 56.2093 ms
+-----------------+-------+------------------+----------------------------+-------------------+-------------------------+----------------+----+------------------+----+-------------+--------------+------+---------------+--------------------+----------+---------------------+------------+-----------------+------------------+-----------------+------------------+-------------------+
|fecha_reporte_web|id_caso|fecha_notificacion|codigo_divipola_departamento|nombre_departamento|codigo_divipola_municipio|nombre_municipio|edad|unidad_medida_edad|sexo|tipo_contagio|ubicacion_caso|estado|codigo_iso_pais|         nombre_pais|recuperado|fecha_inicio_sintomas|fecha_muerte|fecha_diagnostico|fecha_recuperacion|tipo_recuperacion|pertenencia_etnica|nombre_grupo_etnico|
+-----------------+-------+------------------+----------------------------+-------------------+-------------------------+----------------+----+------------------+----+-------------+--------------+------+---------------+--------------------+----------+---------------------+------------+-----------------+------------------+-----------------+------------------+-------------------+
| 6/3/2020 0:00:00|      1|  2/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  19|                 1|   F|    Importado|          Casa|  Leve|            380|              ITALIA|Recuperado|    27/2/2020 0:00:00|            | 6/3/2020 0:00:00| 13/3/2020 0:00:00|              PCR|                 6|                   |
| 9/3/2020 0:00:00|      2|  6/3/2020 0:00:00|                          76|              VALLE|                    76111|            BUGA|  34|                 1|   M|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     4/3/2020 0:00:00|            | 9/3/2020 0:00:00| 19/3/2020 0:00:00|              PCR|                 5|                   |
| 9/3/2020 0:00:00|      3|  7/3/2020 0:00:00|                           5|          ANTIOQUIA|                     5001|        MEDELLIN|  50|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|    29/2/2020 0:00:00|            | 9/3/2020 0:00:00| 15/3/2020 0:00:00|              PCR|                 6|                   |
|11/3/2020 0:00:00|      4|  9/3/2020 0:00:00|                           5|          ANTIOQUIA|                     5001|        MEDELLIN|  55|                 1|   M|  Relacionado|          Casa|  Leve|               |                    |Recuperado|     6/3/2020 0:00:00|            |11/3/2020 0:00:00| 26/3/2020 0:00:00|              PCR|                 6|                   |
|11/3/2020 0:00:00|      5|  9/3/2020 0:00:00|                           5|          ANTIOQUIA|                     5001|        MEDELLIN|  25|                 1|   M|  Relacionado|          Casa|  Leve|               |                    |Recuperado|     8/3/2020 0:00:00|            |11/3/2020 0:00:00| 23/3/2020 0:00:00|              PCR|                 6|                   |
|11/3/2020 0:00:00|      6| 10/3/2020 0:00:00|                           5|          ANTIOQUIA|                     5360|          ITAGUI|  27|                 1|   F|  Relacionado|          Casa|  Leve|               |                    |Recuperado|     6/3/2020 0:00:00|            |11/3/2020 0:00:00| 26/3/2020 0:00:00|              PCR|                 6|                   |
|11/3/2020 0:00:00|      7|  8/3/2020 0:00:00|                       13001|          CARTAGENA|                    13001|       CARTAGENA|  85|                 1|   F|    Importado|          Casa|  Leve|            840|ESTADOS UNIDOS DE...|Recuperado|     2/3/2020 0:00:00|            |11/3/2020 0:00:00| 17/3/2020 0:00:00|              PCR|                 6|                   |
|11/3/2020 0:00:00|      8|  9/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  22|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     6/3/2020 0:00:00|            |11/3/2020 0:00:00| 21/3/2020 0:00:00|              PCR|                 6|                   |
|11/3/2020 0:00:00|      9|  8/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  28|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     7/3/2020 0:00:00|            |11/3/2020 0:00:00| 23/3/2020 0:00:00|              PCR|                 6|                   |
|12/3/2020 0:00:00|     10| 12/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  36|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     6/3/2020 0:00:00|            |12/3/2020 0:00:00| 21/3/2020 0:00:00|              PCR|                 6|                   |
|12/3/2020 0:00:00|     11| 11/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  42|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     6/3/2020 0:00:00|            |12/3/2020 0:00:00| 31/3/2020 0:00:00|              PCR|                 6|                   |
|12/3/2020 0:00:00|     12| 10/3/2020 0:00:00|                          41|              HUILA|                    41001|           NEIVA|  74|                 1|   F|    Importado|          Casa|  Leve|            380|              ITALIA|Recuperado|     6/3/2020 0:00:00|            |12/3/2020 0:00:00|  9/4/2020 0:00:00|              PCR|                 6|                   |
|12/3/2020 0:00:00|     13| 10/3/2020 0:00:00|                          41|              HUILA|                    41001|           NEIVA|  68|                 1|   F|  Relacionado|          Casa|  Leve|               |                    |Recuperado|     6/3/2020 0:00:00|            |12/3/2020 0:00:00| 30/3/2020 0:00:00|              PCR|                 6|                   |
|13/3/2020 0:00:00|     14| 10/3/2020 0:00:00|                          76|              VALLE|                    76520|         PALMIRA|  48|                 1|   M|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     7/3/2020 0:00:00|            |13/3/2020 0:00:00| 21/3/2020 0:00:00|              PCR|                 5|                   |
|13/3/2020 0:00:00|     15| 13/3/2020 0:00:00|                          50|               META|                    50001|   VILLAVICENCIO|  30|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     9/3/2020 0:00:00|            |13/3/2020 0:00:00| 23/3/2020 0:00:00|              PCR|                 6|                   |
|13/3/2020 0:00:00|     16| 11/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  61|                 1|   F|    Importado|          Casa|  Leve|            380|              ITALIA|Recuperado|     8/3/2020 0:00:00|            |13/3/2020 0:00:00| 23/3/2020 0:00:00|              PCR|                 5|                   |
|14/3/2020 0:00:00|     17| 11/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  73|                 1|   F|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|    28/2/2020 0:00:00|            |14/3/2020 0:00:00| 14/3/2020 0:00:00|              PCR|                 6|                   |
|14/3/2020 0:00:00|     18| 11/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  54|                 1|   M|    Importado|          Casa|  Leve|            724|              ESPAA|Recuperado|     7/3/2020 0:00:00|            |14/3/2020 0:00:00|  7/4/2020 0:00:00|           Tiempo|                 6|                   |
|14/3/2020 0:00:00|     19| 12/3/2020 0:00:00|                          11|             BOGOTA|                    11001|          BOGOTA|  54|                 1|   F|  Relacionado|          Casa|  Leve|               |                    |Recuperado|     9/3/2020 0:00:00|            |14/3/2020 0:00:00| 24/3/2020 0:00:00|              PCR|                 6|                   |
|14/3/2020 0:00:00|     20| 11/3/2020 0:00:00|                           5|          ANTIOQUIA|                     5001|        MEDELLIN|  26|                 1|   F|  Relacionado|          Casa|  Leve|               |                    |Recuperado|     9/3/2020 0:00:00|            |14/3/2020 0:00:00| 24/3/2020 0:00:00|              PCR|                 6|                   |
+-----------------+-------+------------------+----------------------------+-------------------+-------------------------+----------------+----+------------------+----+-------------+--------------+------+---------------+--------------------+----------+---------------------+------------+-----------------+------------------+-----------------+------------------+-------------------+
only showing top 20 rows

24/11/24 09:14:26 INFO ClientConfigurationFactory: Set initial getObject socket timeout to 2000 ms.
24/11/24 09:14:26 INFO BlockManagerInfo: Removed broadcast_0_piece0 on ip-172-31-90-72.ec2.internal:38949 in memory (size: 7.2 KiB, free: 4.8 GiB)
24/11/24 09:14:26 INFO BlockManagerInfo: Removed broadcast_0_piece0 on ip-172-31-84-208.ec2.internal:42773 in memory (size: 7.2 KiB, free: 912.3 MiB)
24/11/24 09:14:27 INFO ParquetUtils: Using user defined output committer for Parquet: com.amazon.emr.committer.EmrOptimizedSparkSqlParquetOutputCommitter
24/11/24 09:14:27 INFO SQLConfCommitterProvider: Getting user defined output committer class com.amazon.emr.committer.EmrOptimizedSparkSqlParquetOutputCommitter
24/11/24 09:14:27 INFO EmrOptimizedParquetOutputCommitter: EMR Optimized Committer: ENABLED
24/11/24 09:14:27 INFO EmrOptimizedParquetOutputCommitter: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileSystemOptimizedCommitter
24/11/24 09:14:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
24/11/24 09:14:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: true
24/11/24 09:14:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
24/11/24 09:14:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: true
24/11/24 09:14:27 INFO SQLConfCommitterProvider: Using output committer class com.amazon.emr.committer.EmrOptimizedSparkSqlParquetOutputCommitter
24/11/24 09:14:27 INFO FileSystemOptimizedCommitter: Nothing to setup as successful task attempt outputs are written directly
24/11/24 09:14:27 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
24/11/24 09:14:27 INFO DAGScheduler: Got job 1 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/11/24 09:14:27 INFO DAGScheduler: Final stage: ResultStage 1 (parquet at NativeMethodAccessorImpl.java:0)
24/11/24 09:14:27 INFO DAGScheduler: Parents of final stage: List()
24/11/24 09:14:27 INFO DAGScheduler: Missing parents: List()
24/11/24 09:14:27 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
24/11/24 09:14:27 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 261.7 KiB, free 912.0 MiB)
24/11/24 09:14:27 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 97.1 KiB, free 911.9 MiB)
24/11/24 09:14:27 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on ip-172-31-84-208.ec2.internal:42773 (size: 97.1 KiB, free: 912.2 MiB)
24/11/24 09:14:27 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1592
24/11/24 09:14:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/11/24 09:14:27 INFO YarnScheduler: Adding task set 1.0 with 1 tasks resource profile 0
24/11/24 09:14:27 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (ip-172-31-90-72.ec2.internal, executor 1, partition 0, PROCESS_LOCAL, 7220 bytes) 
24/11/24 09:14:27 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on ip-172-31-90-72.ec2.internal:38949 (size: 97.1 KiB, free: 4.8 GiB)
24/11/24 09:14:29 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2163 ms on ip-172-31-90-72.ec2.internal (executor 1) (1/1)
24/11/24 09:14:29 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
24/11/24 09:14:29 INFO DAGScheduler: ResultStage 1 (parquet at NativeMethodAccessorImpl.java:0) finished in 2.233 s
24/11/24 09:14:29 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
24/11/24 09:14:29 INFO YarnScheduler: Killing all running tasks in stage 1: Stage finished
24/11/24 09:14:29 INFO DAGScheduler: Job 1 finished: parquet at NativeMethodAccessorImpl.java:0, took 2.238566 s
24/11/24 09:14:29 INFO FileFormatWriter: Start to commit write Job dad6523c-eeaf-4a5c-8a34-d5bdf77cdcae.
24/11/24 09:14:29 INFO MultipartUploadOutputStream: close closed:false s3://p3-tet/raw/_SUCCESS
24/11/24 09:14:30 INFO FileFormatWriter: Write Job dad6523c-eeaf-4a5c-8a34-d5bdf77cdcae committed. Elapsed time: 117 ms.
24/11/24 09:14:30 INFO FileFormatWriter: Finished processing stats for write job dad6523c-eeaf-4a5c-8a34-d5bdf77cdcae.
24/11/24 09:14:30 INFO SparkContext: SparkContext is stopping with exitCode 0.
Datos extrados y guardados en S3 en la zona raw.
24/11/24 09:14:30 INFO SparkUI: Stopped Spark web UI at http://ip-172-31-84-208.ec2.internal:4040
24/11/24 09:14:30 INFO YarnClientSchedulerBackend: Interrupting monitor thread
24/11/24 09:14:30 INFO YarnClientSchedulerBackend: Shutting down all executors
24/11/24 09:14:30 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
24/11/24 09:14:30 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
24/11/24 09:14:30 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/11/24 09:14:30 INFO MemoryStore: MemoryStore cleared
24/11/24 09:14:30 INFO BlockManager: BlockManager stopped
24/11/24 09:14:30 INFO BlockManagerMaster: BlockManagerMaster stopped
24/11/24 09:14:30 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/11/24 09:14:30 INFO SparkContext: Successfully stopped SparkContext
24/11/24 09:14:30 INFO ShutdownHookManager: Shutdown hook called
24/11/24 09:14:30 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-8a243653-5eab-4feb-9fc7-f99ca3b94dd1
24/11/24 09:14:30 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-c80a3da2-4f48-42b7-b737-cba62417afe2
24/11/24 09:14:30 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-c80a3da2-4f48-42b7-b737-cba62417afe2/pyspark-730bf761-a3a1-48df-bffc-3a9fcb14e845
